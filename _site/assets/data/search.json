[
  
  {
    "title"    : "Jekyll Mermaid js 사용하기",
    "category" : "",
    "tags"     : " Blog, Jekyll, Mermaid, Liquid, Markdown, Template",
    "url"      : "/2021/11/29/jekyll-mermaid.html",
    "date"     : "November 29, 2021",
    "excerpt"  : "Jekyll Mermaid js 사용하기\n\n\n\nMarkdown 작성시 다이어그램 툴로 많이 사용하는 Mermaid js Jekyll 적용하기\n\n\n\nMermaid\n\nMermaid?\n\n\n\nmermaid-js\n\nMarkdown에서 Mermaid syntax를 이용하여 여러 다이어그램과 차트들을 텍스트 기반으로 작성하여 렌더링할 수 있다.\n\n예를 들면 아래와 같은 다이어그램들을 외부 draw tool 없이 쉽게 작성이 가능하다.\n\n관련 내용을 작...",
  "content"  : "Jekyll Mermaid js 사용하기\n\n\n\nMarkdown 작성시 다이어그램 툴로 많이 사용하는 Mermaid js Jekyll 적용하기\n\n\n\nMermaid\n\nMermaid?\n\n\n\nmermaid-js\n\nMarkdown에서 Mermaid syntax를 이용하여 여러 다이어그램과 차트들을 텍스트 기반으로 작성하여 렌더링할 수 있다.\n\n예를 들면 아래와 같은 다이어그램들을 외부 draw tool 없이 쉽게 작성이 가능하다.\n\n관련 내용을 작성할때 매우 유용하므로 익혀두면 좋다.\n\n\n\nFlowchart\n\nMermaid rendered\n\n\n  flowchart LR\n    A[Hard edge] --&amp;gt;|Link text| B(Round edge)\n    B --&amp;gt; C{Decision}\n    C --&amp;gt;|One| D[Result one]\n    C --&amp;gt;|Two| E[Result two]\n\n\nIn Mermaid syntax\n\n  flowchart LR\n    A[Hard edge] --&amp;gt;|Link text| B(Round edge)\n    B --&amp;gt; C{Decision}\n    C --&amp;gt;|One| D[Result one]\n    C --&amp;gt;|Two| E[Result two]\n\n\n\n\nSequence Diagram\n\nMermaid rendered\n\n\n  sequenceDiagram\n    autonumber\n    Alice-&amp;gt;&amp;gt;John: Hello John, how are you?\n    loop Healthcheck\n        John-&amp;gt;&amp;gt;John: Fight against hypochondria\n    end\n    Note right of John: Rational thoughts!\n    John--&amp;gt;&amp;gt;Alice: Great!\n    John-&amp;gt;&amp;gt;Bob: How about you?\n    Bob--&amp;gt;&amp;gt;John: Jolly good!\n\n\nIn Mermaid syntax\n\n  sequenceDiagram\n    autonumber\n    Alice-&amp;gt;&amp;gt;John: Hello John, how are you?\n    loop Healthcheck\n        John-&amp;gt;&amp;gt;John: Fight against hypochondria\n    end\n    Note right of John: Rational thoughts!\n    John--&amp;gt;&amp;gt;Alice: Great!\n    John-&amp;gt;&amp;gt;Bob: How about you?\n    Bob--&amp;gt;&amp;gt;John: Jolly good!\n\n\n\n\nClass Diagram\n\nMermaid rendered\n\n\nclassDiagram\n  direction RL\n  class Student {\n    -idCard : IdCard\n  }\n  class IdCard{\n    -id : int\n    -name : string\n  }\n  class Bike{\n    -id : int\n    -name : string\n  }\n  Student &quot;1&quot; --o &quot;1&quot; IdCard : carries\n  Student &quot;1&quot; --o &quot;1&quot; Bike : rides\n\n\nIn Mermaid syntax\n\nclassDiagram\n  direction RL\n  class Student {\n    -idCard : IdCard\n  }\n  class IdCard{\n    -id : int\n    -name : string\n  }\n  class Bike{\n    -id : int\n    -name : string\n  }\n  Student &quot;1&quot; --o &quot;1&quot; IdCard : carries\n  Student &quot;1&quot; --o &quot;1&quot; Bike : rides\n\n\n\n\nState Diagram\n\nMermaid rendered\n\n\nstateDiagram-v2\n    [*] --&amp;gt; Active\n  state Active {\n    [*] --&amp;gt; NumLockOff\n    NumLockOff --&amp;gt; NumLockOn : EvNumLockPressed\n    NumLockOn --&amp;gt; NumLockOff : EvNumLockPressed\n    --\n    [*] --&amp;gt; CapsLockOff\n    CapsLockOff --&amp;gt; CapsLockOn : EvCapsLockPressed\n    CapsLockOn --&amp;gt; CapsLockOff : EvCapsLockPressed\n    --\n    [*] --&amp;gt; ScrollLockOff\n    ScrollLockOff --&amp;gt; ScrollLockOn : EvScrollLockPressed\n    ScrollLockOn --&amp;gt; ScrollLockOff : EvScrollLockPressed\n}\n\n\nIn Mermaid syntax\n\nstateDiagram-v2\n    [*] --&amp;gt; Active\n\n    state Active {\n        [*] --&amp;gt; NumLockOff\n        NumLockOff --&amp;gt; NumLockOn : EvNumLockPressed\n        NumLockOn --&amp;gt; NumLockOff : EvNumLockPressed\n        --\n        [*] --&amp;gt; CapsLockOff\n        CapsLockOff --&amp;gt; CapsLockOn : EvCapsLockPressed\n        CapsLockOn --&amp;gt; CapsLockOff : EvCapsLockPressed\n        --\n        [*] --&amp;gt; ScrollLockOff\n        ScrollLockOff --&amp;gt; ScrollLockOn : EvScrollLockPressed\n        ScrollLockOn --&amp;gt; ScrollLockOff : EvScrollLockPressed\n    }\n\n\n\n\nEntity Relationship Diagram\n\nMermaid rendered\n\n\n  erDiagram\n    CAR ||--o{ NAMED-DRIVER : allows\n    CAR {\n        string registrationNumber\n        string make\n        string model\n    }\n    PERSON ||--o{ NAMED-DRIVER : is\n    PERSON {\n        string firstName\n        string lastName\n        int age\n    }\n\n\nIn Mermaid syntax\n\nerDiagram\n    CAR ||--o{ NAMED-DRIVER : allows\n    CAR {\n        string registrationNumber\n        string make\n        string model\n    }\n    PERSON ||--o{ NAMED-DRIVER : is\n    PERSON {\n        string firstName\n        string lastName\n        int age\n    }\n\n\n\n\nUser Journey\n\nMermaid rendered\n\n\n  journey\n    title My working day\n    section Go to work\n      Make tea: 5: Me\n      Go upstairs: 3: Me\n      Do work: 1: Me, Cat\n    section Go home\n      Go downstairs: 5: Me\n      Sit down: 5: Me\n\n\nIn Mermaid syntax\n\njourney\n    title My working day\n    section Go to work\n      Make tea: 5: Me\n      Go upstairs: 3: Me\n      Do work: 1: Me, Cat\n    section Go home\n      Go downstairs: 5: Me\n      Sit down: 5: Me\n\n\n\n\nGantt\n\nMermaid rendered\n\n\n  gantt\n    dateFormat  YYYY-MM-DD\n    title       Adding GANTT diagram functionality to mermaid\n    excludes    weekends\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (&quot;sunday&quot;) or &quot;weekends&quot;, but not the word &quot;weekdays&quot;.)\n  section A section\nCompleted task            :done,    des1, 2014-01-06,2014-01-08\nActive task               :active,  des2, 2014-01-09, 3d\nFuture task               :         des3, after des2, 5d\nFuture task2              :         des4, after des3, 5d\n  section Critical tasks\nCompleted task in the critical line :crit, done, 2014-01-06,24h\nImplement parser and jison          :crit, done, after des1, 2d\nCreate tests for parser             :crit, active, 3d\nFuture task in critical line        :crit, 5d\nCreate tests for renderer           :2d\nAdd to mermaid                      :1d\nFunctionality added                 :milestone, 2014-01-25, 0d\nsection Documentation\nDescribe gantt syntax               :active, a1, after des1, 3d\nAdd gantt diagram to demo page      :after a1  , 20h\nAdd another diagram to demo page    :doc1, after a1  , 48h\nsection Last section\nDescribe gantt syntax               :after doc1, 3d\nAdd gantt diagram to demo page      :20h\nAdd another diagram to demo page    :48h\n\n\nIn Mermaid syntax\n\ngantt\n    dateFormat  YYYY-MM-DD\n    title       Adding GANTT diagram functionality to mermaid\n    excludes    weekends\n    %% (`excludes` accepts specific dates in YYYY-MM-DD format, days of the week (&quot;sunday&quot;) or &quot;weekends&quot;, but not the word &quot;weekdays&quot;.)\n\n    section A section\n    Completed task            :done,    des1, 2014-01-06,2014-01-08\n    Active task               :active,  des2, 2014-01-09, 3d\n    Future task               :         des3, after des2, 5d\n    Future task2              :         des4, after des3, 5d\n\n    section Critical tasks\n    Completed task in the critical line :crit, done, 2014-01-06,24h\n    Implement parser and jison          :crit, done, after des1, 2d\n    Create tests for parser             :crit, active, 3d\n    Future task in critical line        :crit, 5d\n    Create tests for renderer           :2d\n    Add to mermaid                      :1d\n    Functionality added                 :milestone, 2014-01-25, 0d\n\n    section Documentation\n    Describe gantt syntax               :active, a1, after des1, 3d\n    Add gantt diagram to demo page      :after a1  , 20h\n    Add another diagram to demo page    :doc1, after a1  , 48h\n\n    section Last section\n    Describe gantt syntax               :after doc1, 3d\n    Add gantt diagram to demo page      :20h\n    Add another diagram to demo page    :48h\n\n\n\n\nPie Chart\n\nMermaid rendered\n\n\n  pie\n    title Key elements in Product X\n    &quot;Calcium&quot; : 42.96\n    &quot;Potassium&quot; : 50.05\n    &quot;Magnesium&quot; : 10.01\n    &quot;Iron&quot; :  5\n\n\nIn Mermaid syntax\n\npie\n    title Key elements in Product X\n    &quot;Calcium&quot; : 42.96\n    &quot;Potassium&quot; : 50.05\n    &quot;Magnesium&quot; : 10.01\n    &quot;Iron&quot; :  5\n\n\n\n\nJekyll with mermaid\n\nJekyll에서는 기본적인 Jekyll Template에서 Mermaid를 제공하지는 않는 것으로 보인다.\n\nMermaid를 사용하려면 jekyll-spaceship과 같은 별도의 플러그인을 설치해야하는 것으로 보임.\n\n현재 사용중인 Type on Strap 테마에서는 mermaid-js를 포함하고 있어 아래와 같이 HTML 문법으로 매핑하여 작성이 가능하다.\n\n\n  현재 사용중인 테마에서 지원 여부를 확인 후 plugin 도입을 검토해보면 좋을 것 같다.\n\n\nMermaid rendered\n\n\nsequenceDiagram\n    Alice-&amp;gt;&amp;gt;John: Hello John, how are you?\n    John--&amp;gt;&amp;gt;Alice: Great!\n\n\nIn Mermaid syntax wrapped in HTML\n\n&amp;lt;div class=&quot;mermaid&quot;&amp;gt;\nsequenceDiagram\n    Alice-&amp;gt;&amp;gt;John: Hello John, how are you?\n    John--&amp;gt;&amp;gt;Alice: Great!\n&amp;lt;/div&amp;gt;\n\n\n\n\nReference\n\n\n  Mermaid-js\n  Type-on-Strap\n  jekyll-spaceship\n\n\n"
} ,
  
  {
    "title"    : "Jekyll Liquid 문법 빠져나오기(중괄호 무시)",
    "category" : "",
    "tags"     : " Blog, Jekyll, Liquid, Markdown, Template",
    "url"      : "/2021/11/29/jekyll-escape-liquid.html",
    "date"     : "November 29, 2021",
    "excerpt"  : "Jekyll Liquid 문법 빠져나오기(중괄호 무시)\n\n\n\nJekyll Liquid\n\nJekyll 작성시 Liquid template {{ , }} 이중 중괄호 및 중괄호 빠져나오기 (무시하기)\n\n\n\nEscape Liquid\n\nJekyll은 내부 템플릿 언어로 Liquid를 사용한다.\n\nLiquid는 Ruby로 작성된 템플릿 언어로\n\n\n{{ variable }}\n{{ if sentence }}\n\n\n\n등의 중괄호를 이용한 문법을 통하여 템...",
  "content"  : "Jekyll Liquid 문법 빠져나오기(중괄호 무시)\n\n\n\nJekyll Liquid\n\nJekyll 작성시 Liquid template {{ , }} 이중 중괄호 및 중괄호 빠져나오기 (무시하기)\n\n\n\nEscape Liquid\n\nJekyll은 내부 템플릿 언어로 Liquid를 사용한다.\n\nLiquid는 Ruby로 작성된 템플릿 언어로\n\n\n{{ variable }}\n{{ if sentence }}\n\n\n\n등의 중괄호를 이용한 문법을 통하여 템플릿 변수나 로직을 실행한다.\n\n문서를 작성하다 보니 아래와 같은 에러를 발견하게 되었다.\n\n\n\n이는 kubernetes에서 사용하는 go-template이 똑같은 이중괄호를 사용해서 나타난 문제였다.\n\n이를 해결하기 위해서는 아래와 같이 {% raw %}, {% endraw %}  사이에 문법을 적어주면 된다.\n\nRaw\n\n{% raw %}\n$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/monitoring-user -o jsonpath=&quot;{.secrets[0].name}&quot;) -o go-template=&quot;{{.data.token | base64decode}}&quot;\neyJhbGciOiJSUzI1NiIsImtpZCI6IlVmM2...\n{% endraw %}\n\n\nTemplate processed\n\n\n$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/monitoring-user -o jsonpath=&quot;{.secrets[0].name}&quot;) -o go-template=&quot;{{.data.token | base64decode}}&quot;\neyJhbGciOiJSUzI1NiIsImtpZCI6IlVmM2...\n\n\n\n\n\nReference\n\n\n  Jekyll Liquid\n  Liquid Template\n\n\n"
} ,
  
  {
    "title"    : "Jekyll과 Github pages를 사용하여 블로그 만들기",
    "category" : "",
    "tags"     : " Blog, Github, Github pages, Jekyll",
    "url"      : "/2021/11/18/github-pages-with-jekyll.html",
    "date"     : "November 18, 2021",
    "excerpt"  : "Jekyll과 Github pages를 사용하여 간단하게 블로그를 만들어보자\n\n\n\nGitHub Pages\n\n본 문서는 현재 보고 있는 블로그를 구현하였던 내용을 담고 있다.\n\n\n\n시작하게된 계기\n\n문득 개발을 하고 있는데 왜 남는게 없지? 라는 생각이 들기 시작하였다.\n\n요즘엔 양질의 개발자료를 손가락 몇번 놀리면 쉽게 찾을 수 있다보니 정보를 검증도 하지 않고 여과없이 받아들일 수도 있겠다는 생각이 들었고 정보를 내것으로 만들어야겠다는 ...",
  "content"  : "Jekyll과 Github pages를 사용하여 간단하게 블로그를 만들어보자\n\n\n\nGitHub Pages\n\n본 문서는 현재 보고 있는 블로그를 구현하였던 내용을 담고 있다.\n\n\n\n시작하게된 계기\n\n문득 개발을 하고 있는데 왜 남는게 없지? 라는 생각이 들기 시작하였다.\n\n요즘엔 양질의 개발자료를 손가락 몇번 놀리면 쉽게 찾을 수 있다보니 정보를 검증도 하지 않고 여과없이 받아들일 수도 있겠다는 생각이 들었고 정보를 내것으로 만들어야겠다는 필요성이 느껴졌다.\n\n문서를 작성하면서 헷갈리는 부분도 정리할 수 있고 무엇보다 남들에게 설명하도록 나중에 내가 봐도 이해할 수 있도록 작성하려면 확실한 이해가 바탕이 되어야 하기 때문에 시작하게 되었다.\n\n\n\n어떤걸 써야할까\n\n웹개발이라는 것은 익숙하지만 결코 간단한 것이 아니다. (자세히 말하면 public하게 오픈하는 것)\n\n\n  JS, CSS 등의 Frontend 스택을 활용하여 프론트 웹을 개발한다.\n  웹에서 사용할 REST API를 정의 및 개발하고 경우에 따라 DB를 구축한다.\n  Cloud 환경이든 자체 구축 서버든 웹을 빌드하여 올릴 서버가 필요하다.\n  도메인, 인증서 구매 및 적용을 통해 외부에 오픈이 필요하다.\n  추가적으로) 최소한의 보안과 무중단 배포를 위한 proxy 서버가 필요\n\n\n선택기준\n\n\n  간단한 기술 블로그를 위하여 웹개발을 하기는 싫었다.\n  자체 서버를 구축하기 싫었다. (비용적으로든 노력으로든)\n  추가적인 비용을 들이기 싫었다.\n\n\n따라서 자체 호스팅을 하는 사이트를 이용하는 방법을 선택하였다.\n\n\n\n블로그 플랫폼\n\n조사 결과 여러가지 블로그 플랫폼 서비스들이 존재하였다.\n\n\n  https://velog.io\n    \n      \n        개발자를 위한 블로그 서비스(플랫폼)\n      \n      \n        마크다운 작성 가능\n      \n    \n  \n  \n    https://wordpress.org\n\n    \n      \n        전세계적으로 가장 유명한 블로그 서비스\n      \n      \n        커스터마이징 기능이 강력\n      \n      \n        방대한 생태계\n      \n    \n  \n  \n    https://www.tistory.com\n\n    \n      \n        유명한 블로그 플랫폼\n      \n      \n        WYSIWYG 방식의 에디터를 제공\n      \n      \n        특정 환경에서 접속이 매우 느림\n      \n    \n  \n  \n    https://blog.naver.com\n\n    \n      \n        유명한 블로그 플랫폼2\n      \n      \n        WYSIWYG 방식의 에디터를 제공\n      \n      \n        구글 검색이 안되는 치명적인 단점이 있음\n      \n      \n        개발 한정으로 매우 기능이 부실\n      \n    \n  \n  \n    https://pages.github.com\n\n    \n      \n        github과 연동됨\n      \n      \n        markdown 지원\n      \n      \n        퍼블릭 도메인 간편하게 설정 가능\n      \n    \n  \n\n\n결론은?\n\n웹 개발에 공을 들이기는 싫었고 현재 마크다운으로 작성되어 있는 문서들을 쉽게 올릴 수 있어야 하였고 github과 연동되는 Github pages를 사용하기로 하였다.\n\n\n\nJekyll\n\n\n\nhttps://jekyllrb.com/\n\nGitHub 공동 설립자 Tom Preston-Werner에 의해 개발된 Ruby 기반의 정적 사이트 생성기(Static site generator)이다.\n\n여기서 SSG(Static Site Generator)란 DB없이 static file 즉 html만으로 돌아가는 웹을 의미한다.\n\n마크다운 형태로 작성이 가능하여 개발자 freindly하며 구현도 매우 간편하게 할 수 있다.\n\nGithub pages와 궁합이 매우 좋으며 Github pages 공식 문서에서도 Jekyll을 이용하도록 안내하고 있다.\n\n\n\nGithub pages\n\n1. 저장소 생성\n\n\n\nGithub pages는 계정의 github repository를 기반으로 웹을 제공한다.\n\n{사용자명}.github.io 의 형태로 저장소를 생성하면 해당 저장소는 자동으로 github pages 저장소로 설정이 된다.\n\n(올바른 branch에 커밋시 자동으로 github pages를 빌드하도록 설정이 되어있다.)\n\n\n  사진은 이미 저장소를 만들었기 때문에 중복되었다고 나옴\n\n\n2. git 설정\n\n# 유저명은 csupreme19로 가정\n$ mkdir -p ~/git/csupreme19.github.io\n$ cd ~/git/csupreme19.github.io\n# git 사용자 설정\n$ git config --local user.name csupreme19\n$ git config --local user.email csupreme19@gmail.com\n\n# git 로컬 저장소 초기화\n$ git init\n\n# git remote 저장소 연결\n$ git remote add origin git@github.com:csupreme19/csupreme19.github.io.git\n$ git remote -v\norigin\tgit@github.com:csupreme19/csupreme19.github.io.git (fetch)\norigin\tgit@github.com:csupreme19/csupreme19.github.io.git (push)\n\n\n\n  나는 사내 gitlab의 계정이 global로 설정되어 있어 해당 git 저장소에만 config를 적용하도록 --local flag를 사용하였다.\n\n\n3. 테마 설정\n\nhttp://jekyllthemes.org/\n\n위 사이트에서 마음에 드는 테마를 선택 후 github에서 fork 또는 clone한다.\n\n\n  jekyll을 설치 하는 것이 원래 해야 할 일이지만 보통 테마에서 jekyll gemspec 명세를 제공하므로 jekyll 설치를 뒤로하고 테마 설정을 진행한다.\n\n\n\n\n본인은 Type on Strap 테마를 선택하였다.\n\n선택 이유?\n\n\n  현재까지도 유지보수가 되고 있는 점(선택 시점에 10시간 전에 릴리즈된 것을 확인)\n  컨텐츠 중심의 가독성 좋은 테마\n  반응형 웹\n  다크 테마 지원\n  mermaid, katex 등 다이어그램, 수식 툴 지원\n\n\n단점이라면 한글 폰트가 생각보다 크게 보인다는 것인데… 이후 폰트 및 사이즈 변경 예정이다.\n\n\n\nJekyll 설치 및 설정\n\n선행사항\n\n\n  Ruby 2.5.0 이상\n  RubyGems\n  GCC\n  Make\n\n\n1. Ruby 설치(macOS)\n\n# Homebrew 설치\n$ /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;\n\n# Ruby 설치\n$ brew install ruby\n# zsh 사용시\necho &#39;export PATH=&quot;/usr/local/opt/ruby/bin:/usr/local/lib/ruby/gems/3.0.0/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.zshrc\n# bash 사용시\necho &#39;export PATH=&quot;/usr/local/opt/ruby/bin:/usr/local/lib/ruby/gems/3.0.0/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile\n\n# 버전 확인\n$ ruby -v\n$ gem -v\n$ gcc -v\n\n\n다른 OS 설치 방법은 Jekyll 설치 문서 참조\n\n2. Jekyll 및 테마 설치\n\n$ cd ~/git/csupreme19.github.io\n$ git clone https://github.com/Sylhare/Type-on-Strap.git\n\n# install\n$ gem install jekyll\n$ bundle add webrick\n$ bundle install\n\n# jekyll 로컬 서버 실행\n$ bundle exec jekyll serve\nConfiguration file: /Users/csupreme19/git/csupreme19.github.io/_config.yml\n            Source: /Users/csupreme19/git/csupreme19.github.io\n       Destination: /Users/csupreme19/git/csupreme19.github.io/_site\n Incremental build: disabled. Enable with --incremental\n      Generating...\n       Jekyll Feed: Generating feed for posts\n                    done in 1.461 seconds.\n Auto-regeneration: enabled for &#39;/Users/csupreme19/git/csupreme19.github.io&#39;\n    Server address: http://127.0.0.1:4000/\n  Server running... press ctrl-c to stop.\n\n\n3. _config.yml 수정\n\n# SITE CONFIGURATION\nbaseurl: &quot;&quot;\t\t# 설정한 url이 subdomain이 된다. 즉 {url}/{baseurl} 도메인으로 접속해야함\nurl: &quot;https://csupreme19.github.io&quot;\t\t# 도메인 주소 설정\n\n# THEME-SPECIFIC CONFIGURATION\ntitle: 내 블로그                             # 타이틀\ndescription: &quot;한글테스트&quot;      # 구글 검색 엔진에서 사용하는 정보\navatar: assets/img/triangle.png                         # 상단 navbar 이미지\nfavicon: assets/favicon.ico                             # 웹 favicon\n\n# Header and footer text\nheader_text: 헤더 텍스트  # 블로그 헤더 텍스트\nheader_feature_image: assets/img/pexels/triangular.jpeg # 헤더 이미지\n# 푸터 텍스트\nfooter_text: &amp;gt;\n  Powered by &amp;lt;a href=&quot;https://jekyllrb.com/&quot;&amp;gt;Jekyll&amp;lt;/a&amp;gt; with &amp;lt;a href=&quot;https://github.com/sylhare/Type-on-Strap&quot;&amp;gt;Type on Strap&amp;lt;/a&amp;gt;\n  \n...\n# 나머지 설정은 github 문서 참조\n\n\n4. 로컬 접속 테스트\n\n\n\nhttp://localhost:4000 접속 확인\n\n5. git push 및 github pages 설정\n\n$ git add .\n$ git commit -m &quot;initial commit&quot;\n$ git push -u origin main\n\n\n\n\ngithub 저장소 &amp;gt; Settings &amp;gt; Pages\n\n빌드 소스 변경\n\nSource 부분 main 브랜치, / (root) 폴더로 변경\n\n변경 후 Your site is published at https://csupreme19.github.io/ 빌드 성공 메시지 확인\n\n6. 접속\n\nhttps://csupreme19.github.io/ 접속 확인\n\n\n\n댓글창 활성화\n\nType-on-Strap 테마는 3가지의 Comment 오픈소스를 지원한다.\n\n1. Disqus\n\ndisqus.com\n\n장점\n\n\n  로그인 안해도 댓글 달 수 있음\n\n\n단점\n\n\n  무겁다, 무료버전은 광고가 존재\n  개인적인 의견이지만 댓글창이 너저분해보인다.\n\n\n2. Cusdis\n\ncusdis.com\n\n장점\n\n\n  Disqus에 비해 매우 깔끔한 레이아웃\n\n\n단점\n\n\n  중국발이라 왠지 모를 거부감\n  disqus와 마찬가지로 무겁다.\n\n\n3. Utterance\n\nutterance.es\n\n장점\n\n\n  매우 깔끔한 레이아웃\n  성능이 위 2 오픈소스에 비해 좋다.\n  완전 무료 오픈소스로 광고 없음\n\n\n단점\n\n\n  댓글이 repo에 GitHub 이슈로 등록되는 구조라 GitHub 계정이 있어야만 댓글 가능\n\n\nUtterance를 사용하기로 하였다.\n\n일반적인 블로그가 아니라 GitHub Pages로 운영되는 GitHub 기반 블로그이며 주로 개발 내용을 다루기 때문에 GitHub 계정이 필요한 것은 큰 단점으로 다가오지 않았다.\n\n또한 GitHub Issue로 등록되므로 Webhook을 등록하여 Slack 알람을 받는등 Alert 기능도 활성화 가능하다고 생각하였다.\n\n\n\nUtterance 적용하기\n\n\n\n1. Public Repo 생성\n\nGithub pages repo가 public이므로 해당 repo 사용\n\n2. Utterance App 설치\n\nhttps://github.com/apps/utterances에서 설치\n\n3. _config.yml 설정\n\n# Comments\ncomments:\n  utterances:\n    repo: csupreme19/csupreme19.github.io\n    issue-term: comment\n\n\n위 두가지 설정만 하면 설정은 끝난다.\n\n4. 코멘트 적용 확인\n\n$ bundle exec jekyll serve\n\n\n\n\n\n\n폰트 변경\n\n맥 환경에서 봤을땐 폰트가 사이즈 말고는 괜찮았는데 윈도우 환경에서 보니 계단 현상이 존재하고 가독성이 떨어져 보이는 문제가 발생하였다.\n\n테마(템플릿)를 사용하는 이유가 UI 개발에 힘을 쏟기 싫었기 때문인데 어쩔 수 없이 입맛에 맞는 커스텀은 필요한 것 같다.\n\n1. 폰트 .scss 파일 생성\n\n일반 폰트는 케이티의 Y 너만을 비춤체, 소스 코드 폰트는 네이버의 D2 coding ligature를 사용하였다.\n\n$ cd _sass/external\n$ vim _y-spotlight.scss\n$ vim _d2-coding-ligature.scss\n\n\n폰트 배포시 별도의 @font-face 소스를 제공한다.\n\n없다면 기본 구성되어있는 _source-sans-pro.scss 파일을 복사하여 사용하자.\n\n// _y-spotlight.scss\n@font-face {\n    font-family: &#39;Y_Spotlight&#39;;\n    src: url(&#39;https://cdn.jsdelivr.net/gh/projectnoonnu/noonfonts-20-12@1.0/Y_Spotlight.woff&#39;) format(&#39;woff&#39;);\n    font-weight: normal;\n    font-style: normal;\n}\n\n// _d2-coding-ligature.scss\n@font-face {\n    font-family: &#39;D2 coding Ligature&#39;;\n    src: url(&#39;https://cdn.jsdelivr.net/gh/everydayminder/assets/subset-D2Codingligature.woff&#39;) format(&#39;woff&#39;);\n    font-weight: 400;\n    font-style: normal;\n}\n\n\n폰트 소스는 cdn을 사용하였는데 D2 coding 폰트의 경우 공식으로 CDN을 제공하고 있지 않는 것 같다.\n\n따라서 everyminder 에서 제공한 woff CDN을 사용하였다.\n\n2. _variables.scss 수정\n\n$ vim _sass/base/_variables.scss\n\n\n_variables.scss 파일에 테마 변수 정보들이 담겨있다.\n\n아래 부분처럼 위에서 추가했던 font-family를 추가한다.\n\n/* TYPOGRAPHY */\n$font-family-main: &#39;Y_Spotlight&#39;, &#39;Source Sans Pro&#39;, Helvetica, Arial, sans-serif;\n$font-family-headings: &#39;Y_Spotlight&#39;, &#39;Source Sans Pro&#39;, Helvetica, Arial, sans-serif;\n$font-family-logo: &#39;Y_Spotlight&#39;, &#39;Source Sans Pro&#39;, Helvetica, Arial, sans-serif;\n$font-size: 0.875em;\n\n$monospace: &#39;D2 coding ligature&#39;, Consolas, Menlo, Monaco, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace, sans-serif !default;\n$font-size-code: 0.85em !default;\n$font-height-code: 1.3em !default;\n$border-radius: 4px !default;\n\n\n하는 김에 글씨 크기가 커서 폰트 사이즈도 조정해 주었다.\n\n3. 확인\n\n\n\n폰트 적용이 된 것을 확인할 수 있다.\n\n\n\nReference\n\n\n  Jekyll Installation\n  GitHub Pages\n  Jekyll Themes\n  Type-on-Strap\n\n\n"
} ,
  
  {
    "title"    : "Kubernetes Pod 재시작 장애 확인하기",
    "category" : "",
    "tags"     : " Kubernetes, K8S, Pod, Fail",
    "url"      : "/2021/10/14/kubernetes-pod-fail-test.html",
    "date"     : "October 14, 2021",
    "excerpt"  : "Kubernetes Pod 재시작 장애 확인하기\n\nContainer restart policy\n\n강제로 죽는 pod를 배포 후 해당 파드가 어떻게 restart 되는지 살펴보았다.\n\n\n\n실험\n\n실험 절차\n\n1. Pod 배포\n\n# alias k=kubectl\n\n# Declarative way\n$ vim dummy-pod.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-pod\nspec:\n  c...",
  "content"  : "Kubernetes Pod 재시작 장애 확인하기\n\nContainer restart policy\n\n강제로 죽는 pod를 배포 후 해당 파드가 어떻게 restart 되는지 살펴보았다.\n\n\n\n실험\n\n실험 절차\n\n1. Pod 배포\n\n# alias k=kubectl\n\n# Declarative way\n$ vim dummy-pod.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-pod\nspec:\n  containers:\n    - name: dummy-pod\n      image: ubuntu\n  restartPolicy: Always\n  \n$ k create -f dummy-pod.yaml\npod/dummy-pod created\n\n# Imperative way\n$ k run dummy-pod --image ubuntu\npod/dummy-pod created\n\n\n2. Pod 상태 확인\n\n$ k get po\nNAME                       READY   STATUS              RESTARTS   AGE\ndummy-pod                  0/1     ContainerCreating   0          2s\n\n$ k get po\nNAME                       READY   STATUS      RESTARTS   AGE\ndummy-pod                  0/1     Completed   1          10s\n\n$ k get po\nNAME                       READY   STATUS      RESTARTS   AGE\ndummy-pod                  0/1     Completed   2          32s\n\n$ k get po\nNAME                       READY   STATUS      RESTARTS   AGE\ndummy-pod                  0/1     Completed   3          61s\n\n$ k get po\nNAME                       READY   STATUS      RESTARTS   AGE\ndummy-pod                  0/1     Completed   4          118s\n\n$ k get po\nNAME                       READY   STATUS             RESTARTS   AGE\ndummy-pod                  0/1     CrashLoopBackOff   4          2m34s\n\n$ k get po\nNAME                       READY   STATUS      RESTARTS   AGE\ndummy-pod                  0/1     Completed   5          3m33s\n\n\n10, 30, 60, 110, 210초에 각각 restart되는 것으로 확인\n\n공식 문서에 따르면 지수 백오프 지연(10초, 20초, 40초, …)로 10초로 시작하여 2배씩 재시작 간격이 증가한다고 함(최대 300초(5분))\n\n지수배는 아니지만 비슷하게 재시작 된 것을 확인할 수 있었다.\n\n3. 결론\n\n아래와 같이 다양한 기준을 적용하여 파드가 계속 재시작중인지 restart 횟수 만으로 판별 가능\n\n  파드 배포 후 1분 내 2번 이상(10, 30, 60초)\n  파드 배포 후 3분 내 4번 이상(10, 30, 60, 110, 210초)\n\n\n\n\nReference\n\n\n  Container restart policy\n\n"
} ,
  
  {
    "title"    : "Kubernetes RBAC Authorization 개요 및 적용",
    "category" : "",
    "tags"     : " Kubernetes, K8S, Authorization, Authentication, RBAC, ABAC, Security",
    "url"      : "/2021/10/01/kubernetes-RBAC-auth.html",
    "date"     : "October 1, 2021",
    "excerpt"  : "Kubernetes RBAC Authorization 개요 및 적용\n\n\n\nUsing RBAC Authorization\n\n본 문서에서는 쿠버네티스 API 서버에 접근하기 위한 4가지 인증 방식을 살펴보고 그 중 RBAC 인증 적용방법에 대하여 정리하였다.\n\n\n\n인가(Authentication) vs 인증(Authorization)\n\n\n\n인가(Authentication)\n\n해당 사용자가 누구인지 확인하는 것(회원가입, 로그인)\n\n인증(Auth...",
  "content"  : "Kubernetes RBAC Authorization 개요 및 적용\n\n\n\nUsing RBAC Authorization\n\n본 문서에서는 쿠버네티스 API 서버에 접근하기 위한 4가지 인증 방식을 살펴보고 그 중 RBAC 인증 적용방법에 대하여 정리하였다.\n\n\n\n인가(Authentication) vs 인증(Authorization)\n\n\n\n인가(Authentication)\n\n해당 사용자가 누구인지 확인하는 것(회원가입, 로그인)\n\n인증(Authorization)\n\n해당 사용자에 대한 권한을 허락하는 것(호, 자원 접근)\n\n\n\nKubernetes 인증 방식\n\nKubernetes API 서버에 접근하기 위해서는 인증 단계가 필요하다.\n\n쿠버네티스에서는 인증 방식이 크게 4가지가 존재한다.\n\n\n  Node Authorization\n  ABAC Authorization\n  RBAC Authorization\n  Webhook Authorization\n\n\nAPI Server의 --authorization-mode  flag를 확인하여 현재 활성화된 인증 모드를 확인할 수 있다.\n\n$ cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i authorization\n    - --authorization-mode=Node,RBAC\n\n\n기본적으로 Node, RBAC 인증 방식이 활성화되어 있는 것을 확인할 수 있다.\n\n\n\nNode Authorization\n\n\nflowchart LR\n  A[User]\n  B[Kube API]\n  C[&quot;KubeletNode&quot;]\n  subgraph Kubernetes Cluster\n    C--Node Authorization--&amp;gt;B\n  end\n    A--&amp;gt;B\n\n\nKubernetes Clsuter에 속하는 Node들은 Kubelet에서 API 서버에 요청할 때 TLS 인증을 이용한다.\n\n이 때 Kubelet의 Group은 system:node에 속해 있으며 해당 그룹에 속해있는 인증 요청은 Node Authorizer에 의하여 인증된다.\n\n이 방식은 보통 Kubernetes TLS 부트스트랩 과정에서 자동으로 설정되므로 더 자세한 내용은 공식 문서 참고\n\n\n\nABAC(Attribute Based Access Control)\n\nJSON 형식의 Policy 정의를 사용하여 해당 사용자를 인증하는 방식\n\nExamples\n\n\nflowchart LR\n  A[User: admin]\n  B[Group: system:authenticated]\n  C[Kube API]\n  subgraph Kuberntes Cluster\n  A--ABAC--&amp;gt;C\n  B--ABAC--&amp;gt;C\n  end\n\n\n{&quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;, &quot;kind&quot;: &quot;Policy&quot;, &quot;spec&quot;: {&quot;user&quot;:&quot;admin&quot;,     &quot;namespace&quot;: &quot;*&quot;,              &quot;resource&quot;: &quot;*&quot;,         &quot;apiGroup&quot;: &quot;*&quot;                   }}\n{&quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;, &quot;kind&quot;: &quot;Policy&quot;, &quot;spec&quot;: {&quot;group&quot;:&quot;system:authenticated&quot;,  &quot;nonResourcePath&quot;: &quot;*&quot;, &quot;readonly&quot;: true}}\n\n\n기본적으로 비활성화 되어 있으며 kube-apiserver에 --authorization-mode=ABAC, --authorization-policy-file=파일명 설정을 추가하여야 한다.\n\nPoilicy 정의 후 API 서버를 재시작해야하고 접근권한을 파일로 정의하기 때문에 후술할 RBAC에 비하여 관리하기가 어렵다는 단점이 있다.\n\n\n\nRBAC(Role Based Access Control)\n\n사용자, 서비스의 접근 권한(인증)을 Role(ClusterRole)과 RoleBinding(ClusterRoleBinding) 자원에 기반하여 처리하는 방식으로 일반적으로 가장 많이 사용하고 관리하기 쉬운 인증 방식이다.\n\n해당 User, SA(Service Account), Group등이 RoleBinding에 의하여 어떤 접근권한을 가지고 있는지 인증된다.\n\n\nflowchart LR\n  A[User]\n  B[Group]\n    subgraph Kubernetes Cluster\n    C[Service Account]\n  D[Role: Developer]\n  E[Role: Production]\n  end\n  A--RBAC--&amp;gt;D &amp;amp; E\n  B--RBAC--&amp;gt;D\n  C--RBAC--&amp;gt;E\n\n\n\nWebhook Authorization\n\nKubernetes 내부에서 제공하는 인증이 아닌 외부의 인증 정책을 사용하기 위한 방식\n\nOpen Policy Agent와 같은 외부 오픈소스, Admission Controller를 사용할 때 사용한다.\n\n\nflowchart LR\n  A[User]\n    subgraph Kubernetes Cluster\n  B[Kube API]\n    end\n  C[Open Policy Agent]\n  A--&amp;gt;B\n  B--Authorization--&amp;gt;C\n  C-.Authorization.-&amp;gt;B\n\n\n\n\nRBAC 리소스\n\n1. Role / ClusterRole\n\n\nflowchart LR\n  A[RoleCan view PodsCan watch PodsCan list Pods]\n  A\n\n\n어떤 리소스에 어떤 호출이 가능한지 권한/역할을 정의한 리소스이다.\n\nRole과 ClusterRole이 있으며 ClusterRole은 Role과 달리 클러스터 레벨로 가지고 있어 네임스페이스가 존재하지 않는다.\n\nRole 예제\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group\n  resources: [&quot;pods&quot;]\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  # &quot;namespace&quot; omitted since ClusterRoles are not namespaced\n  name: secret-reader\nrules:\n- apiGroups: [&quot;&quot;]\n  resources: [&quot;secrets&quot;]\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]\n\n\nResources에 해당하는 자원의 verb에 해당하는 요청이 가능하다.\n\npod-reader를 예로 들면 pods를 get, watch, list 요청이 가능하다.\n\nkubernetes의 자원 종류는 아래 명령어로 확인 가능하다.\n\n$ kubectl api-resources\nNAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND\nbindings                                       v1                                     true         Binding\ncomponentstatuses                 cs           v1                                     false        ComponentStatus\nconfigmaps                        cm           v1                                     true         ConfigMap\nendpoints                         ep           v1                                     true         Endpoints\nevents                            ev           v1                                     true         Event\nlimitranges                       limits       v1                                     true         LimitRange\nnamespaces                        ns           v1                                     false        Namespace\nnodes                             no           v1                                     false        Node\npersistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim\npersistentvolumes                 pv           v1                                     false        PersistentVolume\n...\n\n\nRole 확인\n\n$ kubectl get role -n kube-system\n$ kubectl get clusterrole -n\n$ kubectl describe role/kube-proxy -n kube-system\n$ kubectl describe clusterrole/cluster-admin\nName:         cluster-admin\nLabels:       kubernetes.io/bootstrapping=rbac-defaults\nAnnotations:  rbac.authorization.kubernetes.io/autoupdate: true\nPolicyRule:\n  Resources  Non-Resource URLs  Resource Names  Verbs\n  ---------  -----------------  --------------  -----\n  *.*        []                 []              [*]\n             [*]                []              [*]\n\n\n2. ServiceAccount\n\n파드로 올라가있는 서비스에서 kubernetes 클러스터에 접근하기 위한 계정을 나타내는 리소스다.\n\n\n\n\n  이미지: Certified Kubernetes Administrator(CKA) with Practice Tests 발췌\n\n\n쉽게 말해 사용자 계정이 아닌 서비스의 계정이다.\n\n후술할 추상적인 User, Group과 달리 Kubernetes 자원 형태로 실존한다.\n\n\nflowchart LR\n  A[Service Account]\n  B[Secret]\n  A-.token.-B\n\n\nSA 예제\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: monitoring-user\n  namespace: kubernetes-dashboard\n---\n\nSA 확인\n\n$ kubectl get sa -n kube-system\n\n\nservice account 생성시 해당 서비스 어카운트의 토큰을 가지고 있는 secret이 자동 생성된다.\n\n서비스 어카운트 사용방법\n\n$ kubectl describe sa/monitoring-user -n kubernetes-dashboard\nName:                monitoring-user\nNamespace:           kubernetes-dashboard\nLabels:              &amp;lt;none&amp;gt;\nAnnotations:         &amp;lt;none&amp;gt;\nImage pull secrets:  &amp;lt;none&amp;gt;\nMountable secrets:   monitoring-user-token-r6nss\nTokens:              monitoring-user-token-r6nss\nEvents:              &amp;lt;none&amp;gt;\n\n\nservice account 생성시 해당 서비스 어카운트의 토큰을 가지고 있는 secret이 자동 생성된다.\n\n토큰 확인\n\n$ kubectl describe secret monitoring-user-token-r6nss -n kubernetes-dashboard\nName:         monitoring-user-token-r6nss\nNamespace:    kubernetes-dashboard\nLabels:       &amp;lt;none&amp;gt;\nAnnotations:  kubernetes.io/service-account.name: monitoring-user\n              kubernetes.io/service-account.uid: cca1bad1-fe4f-430c-978b-52d7f7ea53c5\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nca.crt:     1066 bytes\nnamespace:  20 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IlVmM2...\n\n# 참고) 한번에 토큰 조회\n\n$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/monitoring-user -o jsonpath=&quot;{.secrets[0].name}&quot;) -o go-template=&quot;{{.data.token | base64decode}}&quot;\neyJhbGciOiJSUzI1NiIsImtpZCI6IlVmM2...\n\n\n\nAPI 호출시 Authorization 헤더에 해당 토큰을 넣어서 요청하면 인증을 할 수 있다.\n\n자원에 적용\n\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      containers:\n        - name: dashboard-metrics-scraper\n          image: kubernetesui/metrics-scraper:v1.0.6\n          ...\n      serviceAccountName: kubernetes-dashboard\t# 해당 부분 추가\n      ...\n\n\nPod 명세 serviceAccountName 에 위에서 생성한 SA 추가하면 해당 Pod는 API Server에 정의된 권한을 가지고 접근할 수 있다.\n\n요청 예시\n\n# 직접 호출시 토큰 명시\n$ curl -H &quot;Authorization: Bearer eyJhbGciOiJSUzI1...&quot; -ivk https://10.213.196.211:6443 \n\n\n3. RoleBinding\n\n위에서 생성한 ServiceAccount가 실질적으로 권한을 가지려면 해당 어카운트가 어느 자원에 접근할 수 있는지 허락하는 단계가 필요하다.\n\n이것을 인증(Authorization)이라고 한다.\n\nkubernetes에서는 RBAC, 즉 Role 역할을 기반으로하여 인증을 하기 때문에 Role을 ServiceAccount / User / Group에 바인딩하는 방법을 사용하며 이를 나타내기 위한 리소스가 RoleBinding이다.\n\nRoleBinding과 ClusterRoleBinding이 있으며 ClusterRole의 경우 ClusterRoleBinding을 이용한다.\n\n각각의 Role은 ServiceAccount, User, Group에 바인딩 될 수 있다.\n\nUser / Group\n\nServiceAccount와 달리 Kubernetes API server의 User와 Group은 별도로 정의된 Kubernetes Resource가 아니다.\n\nAPI server에 접근하기 위한 아래와 같은 보안 설정 파일에 정의되어 있는 User와 Group이라는 추상적인 개념이다.\n\n\n  Static Password file\n  Static Token file\n  Certificates\n\n\n보통은 인증서 설정에 User와 Group 정보가 정의되어 있으며 system:  으로 시작하는 그룹은 미리 정의된 그룹이다.\n\nUser 정보는 kubeconfig에서 확인할 수 있다.\n\n$ kubectl config view\n\n\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://10.213.196.211:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: kubernetes-admin\n  user:\n    client-certificate-data: REDACTED\n    client-key-data: REDACTED\n\n\n\nflowchart LR\n  A[User]\n  B[Group]\n  C[Service Account]\n  D[Role]\n  E[Secret]\n  subgraph RoleBinding\n  A &amp;amp; B &amp;amp; C---D\n  end\n  C -.token.- E\n\n\nRoleBinding 예제\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: monitoring-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-monitoring\nsubjects:\n- kind: ServiceAccount\n  name: monitoring-user\n  namespace: kubernetes-dashboard\n---\n\n\n  ClusterRoleBinding 역시 클러스터 레벨이므로 네임스페이스가 따로 없음\n\n\nRoleBinding 확인\n\n$ kubectl get rolebinding\n$ kubectl get clusterrolebinding\n$ kubectl get pods --as jane\t# User 권한 확인\n\n\n\nRBAC 사용 예시\n\n\n  시나리오: k8s dashboard에 접근하기 위한 모니터링 인증을 생성한다.\n\n\n1. kubernetes-dashboard-rbac.yaml 작성\n\nServiceAccount, ClusterRole, ClusterRoleBinding을 설정한다.\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: monitoring-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-monitoring\nrules:\n- apiGroups: [&quot;*&quot;]\n  resources: [&quot;namespaces&quot;, &quot;deployments&quot;, &quot;replicasets&quot;, &quot;pods&quot;, &quot;pods/log&quot;]\n  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: monitoring-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-monitoring\nsubjects:\n- kind: ServiceAccount\n  name: monitoring-user\n  namespace: kubernetes-dashboard\n\n\nCluster의 자원이 아닌 헬스체크등 URI로 API를 호출해야하는 경우 아래와 같이 resources 대신 nonResourceURLs를 사용한다.\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-monitoring\nrules:\n- nonResourceURLs: [&quot;/healthz*&quot;, &quot;/livez*&quot;, &quot;/readyz*&quot;, &quot;/version*&quot;]\n  verbs: [&quot;get&quot;]\n\n\n해당 ClusterRole을 가진 SA의 시크릿 토큰을 이용하여 위 URL에 해당하는 API를 호출할 수 있다.\n\n2. Token 확인\n\n\n$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/monitoring-user -o jsonpath=&quot;{.secrets[0].name}&quot;) -o go-template=&quot;{{.data.token | base64decode}}&quot;\n\n\n\n3. Kubernetes 클러스터 접근 테스트\n\n$ curl -H &quot;Authorization: Bearer eyJhbGciOiJSUzI1...&quot; -k https://10.213.196.211:6443/livez\\?verbose\n[+]ping ok\n[+]log ok\n[+]etcd ok\n[+]poststarthook/start-kube-apiserver-admission-initializer ok\n[+]poststarthook/generic-apiserver-start-informers ok\n[+]poststarthook/priority-and-fairness-config-consumer ok\n[+]poststarthook/priority-and-fairness-filter ok\n[+]poststarthook/start-apiextensions-informers ok\n[+]poststarthook/start-apiextensions-controllers ok\n[+]poststarthook/crd-informer-synced ok\n[+]poststarthook/bootstrap-controller ok\n[+]poststarthook/rbac/bootstrap-roles ok\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\n[+]poststarthook/priority-and-fairness-config-producer ok\n[+]poststarthook/start-cluster-authentication-info-controller ok\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\n[+]poststarthook/start-kube-aggregator-informers ok\n[+]poststarthook/apiservice-registration-controller ok\n[+]poststarthook/apiservice-status-available-controller ok\n[+]poststarthook/kube-apiserver-autoregistration ok\n[+]autoregister-completion ok\n[+]poststarthook/apiservice-openapi-controller ok\nlivez check passed\n\n# 참고) 실패시\n{\n  &quot;kind&quot;: &quot;Status&quot;,\n  &quot;apiVersion&quot;: &quot;v1&quot;,\n  &quot;metadata&quot;: {\n  },\n  &quot;status&quot;: &quot;Failure&quot;,\n  &quot;message&quot;: &quot;Unauthorized&quot;,\n  &quot;reason&quot;: &quot;Unauthorized&quot;,\n  &quot;code&quot;: 401\n}\n\n\n\n\n요약\n\nKubernetes API 서버에 접근하기 위한 인증방식은 크게 4가지가 있다.\n\n\n  Node\n  ABAC\n  RBAC\n  Webhook\n\n\n일반적으로 사용하는 인증 방식으로는 RBAC가 있으며 Role에 기반하여 인증하는 방식이다.\n\n전체적인 구조를 보자면 아래와 같다. (ClusterRole도 동일)\n\n\nflowchart LR\n  A[User]\n  B[Group]\n  C[Service Account]\n  D[Role]\n  subgraph Kubernetes Cluster\n  E[Secret]\n  J[Pod]\n  F[Kube API]\n  RoleBinding\n  end\n  subgraph RoleBinding\n  A &amp;amp; B &amp;amp; C---D\n  end\n  C -.token.- E\n  C --&amp;gt; J\n  J --Request--&amp;gt; F\n  F -.Response.-&amp;gt; J\n\n\n\n\nReference\n\n\n  Using RBAC Authorization\n  Using Node Authorization\n  Using ABAC Authorization\n  Certified Kubernetes Administrator(CKA) with Practice Tests\n\n"
} ,
  
  {
    "title"    : "Apt 자동 업그레이드 비활성화(Unattended upgrade)",
    "category" : "",
    "tags"     : " Linux, OS, Ubuntu, Package Manager, APT, Upgrade",
    "url"      : "/2021/09/09/unattended-upgrade.html",
    "date"     : "September 9, 2021",
    "excerpt"  : "Apt 자동 업그레이드 비활성화(Unattended upgrade)\n\n\n\n\n개요\n\nUbuntu 16, 18, 20에서는 기본적으로 Apt 패키지의 Unattended upgrade라는 자동 업데이트 타이머가 활성화 되어 있다. (크론잡 형태)\n\n리눅스 커널, SSH, OpenSSL 등의 보안 패치를 위하여 자동으로  업데이트를 진행하도록 기본 설정되어 있다.\n\n보안 취약점을 패치한다는 취지에서는 좋지만 Install, Upgrade된 버전...",
  "content"  : "Apt 자동 업그레이드 비활성화(Unattended upgrade)\n\n\n\n\n개요\n\nUbuntu 16, 18, 20에서는 기본적으로 Apt 패키지의 Unattended upgrade라는 자동 업데이트 타이머가 활성화 되어 있다. (크론잡 형태)\n\n리눅스 커널, SSH, OpenSSL 등의 보안 패치를 위하여 자동으로  업데이트를 진행하도록 기본 설정되어 있다.\n\n보안 취약점을 패치한다는 취지에서는 좋지만 Install, Upgrade된 버전을 사용하려면 서버 재시작이 필수이며\n\n커널 업데이트로 인하여 OS레벨에서의 문제점이나 장애가 발생할 수 있다.\n\n이런 장애는 파악하기 매우 어려우므로 운영 관점에서는 해당 기능을 비활성화 하는 것이 좋다.\n\napt 로그 확인\n$ cd /var/log/apt\n$ tail history.log\n\n\napt-daily 비활성화\n$ systemctl stop apt-daily.timer\n$ systemctl stop apt-daily-upgrade.timer\n$ systemctl disable apt-daily.timer\n$ systemctl disable apt-daily.service\n$ systemctl disable apt-daily-upgrade.timer\n$ systemctl disable apt-daily-upgrade.service\n\n\nunattended-upgrade 비활성화\n$ vim /etc/apt/apt.conf.d/20auto-upgrades\nAPT::Periodic::Update-Package-Lists &quot;0&quot;;\nAPT::Periodic::Download-Upgradeable-Packages &quot;0&quot;;\nAPT::Periodic::AutocleanInterval &quot;0&quot;;\nAPT::Periodic::Unattended-Upgrade &quot;0&quot;;\n\n\n비활성화 확인\n$ systemctl status apt-daily.timer\n$ systemctl status apt-daily.service\n$ systemctl status apt-daily-upgrade.timer\n$ systemctl status apt-daily-upgrade.service\n$ systemctl list-unit-files | grep apt-daily\n\n$ apt-config dump | grep Update-Package-Lists\n\n\n"
} ,
  
  {
    "title"    : "Elasticsearch ILM 로그 주기 관리",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Index, ILM",
    "url"      : "/2021/08/11/elastic-ilm.html",
    "date"     : "August 11, 2021",
    "excerpt"  : "Elasticsearch ILM 로그 주기 관리 정책\n\n\n\nElasticsearch의 인덱스 주기 정책 개요 및 설정\n\n\n\n개요\n\nILM: Manage the index lifecycle\n\nILM(Index Lifecycle Management) 정책을 이용하여 로그 수집 주기를 관리한다.\n\nex)\n\n\n  3달치 로그를 저장하고 나머지는 archive\n  인덱스 개수가 일정치를 넘어가면 archive\n  인덱스 사이즈가 일정치를 넘어가면 ...",
  "content"  : "Elasticsearch ILM 로그 주기 관리 정책\n\n\n\nElasticsearch의 인덱스 주기 정책 개요 및 설정\n\n\n\n개요\n\nILM: Manage the index lifecycle\n\nILM(Index Lifecycle Management) 정책을 이용하여 로그 수집 주기를 관리한다.\n\nex)\n\n\n  3달치 로그를 저장하고 나머지는 archive\n  인덱스 개수가 일정치를 넘어가면 archive\n  인덱스 사이즈가 일정치를 넘어가면 archive\n\n\n위 임계값을 넘으면 아래와 같은 액션 수행이 가능\n\n\n  Rollover: 새로운 쓰기 인덱스를 생성한다.\n  Shrink: 인덱스의 primary shard 수를 줄인다.\n  Force merge: 인덱스 shard의 segment 수를 줄인다.\n  Freeze: 인덱스를 read-only로 만든다.\n  Delete: data, metadata를 포함한 인덱스를 지운다.\n\n\nIndex Lifecycle\n\n인덱스의 생명주기는 5가지 페이즈가 있다.\n\n  Hot: 활발하게 업데이트되고 쿼리되는 인덱스\n  Warm: 업데이트는 되지 않지만 쿼리되는 인덱스\n  Cold: 업데이트 되지 않으며 자주 쿼리되지 않는 인덱스(검색이 되어야하지만 느려도 상관 없는 인덱스)\n  Frozen: 업데이트 되지 않으며 드물게 쿼리되는 인덱스(검색이 되어야하지만 심하게 느려도 상관 없는 인덱스)\n  Delete: 더 이상 필요하지 않으며 지워도 상관없는 인덱스\n\n\n각 phase의 minimum age를 설정할 수 있음, 기본값은 0으로 현재 페이즈의 모든 액션이 끝나면 바로 다음 페이즈로 넘어감\n\nILM 기본 정책\n\nTutorial: Automate rollover with ILM 참고\n\nKibana UI 이용\n\nKibana 메뉴 - Stack Management - Data - Index Lifecycle Policies\n\n\n\n\n  Beats와 Logstash등에 ILM을 활성화하면 기본 정책이 설정된다.\n\n\n기본정책 확인 가능\n\n\n\nReference\n\n\n  \n    ILM: Manage the index lifecycle\n  \n  \n    Tutorial: Automate rollover with ILM\n  \n\n\n"
} ,
  
  {
    "title"    : "Elasticsearch Filebeat 모듈 설정",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Filebeat, Beat",
    "url"      : "/2021/08/06/elastic-filebeat-modules.html",
    "date"     : "August 6, 2021",
    "excerpt"  : "Elastic Filebeat 모듈 설정\n\n\n\nFilebeat Modules\n\n로그 정보를 수집하는 Filebeat의 모듈별 Output 설정방법을 정리하였다.\n\n\nModule 설정\n\n\nFilebeat 설치\n\nElastic Filebeat 설치 및 설정 참고\n\n\nModule 확인\n\n데이터를 수집하기 위한 filebeat의 module list 확인\n\n$ filebeat modules list\n\n\nSystem module 설정\n$ fileb...",
  "content"  : "Elastic Filebeat 모듈 설정\n\n\n\nFilebeat Modules\n\n로그 정보를 수집하는 Filebeat의 모듈별 Output 설정방법을 정리하였다.\n\n\nModule 설정\n\n\nFilebeat 설치\n\nElastic Filebeat 설치 및 설정 참고\n\n\nModule 확인\n\n데이터를 수집하기 위한 filebeat의 module list 확인\n\n$ filebeat modules list\n\n\nSystem module 설정\n$ filebeat modules enable system\n\n\n\nKafka module 설정\n\nKafka가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable postgresql\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/kafka.yml 수정\n\n- module: kafka\n  log:\n    enabled: true\n    \n    var.paths:\n      - &quot;/data/kafka-logs/controller.log*&quot;\n      - &quot;/data/kafka-logs/server.log*&quot;\n      - &quot;/data/kafka-logs/state-change.log*&quot;\n      - &quot;/data/kafka-logs/kafka-*.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-kafka.html\n\n\nkafka dashboards\n\n\n\nNginx module 설정\n\nNginx가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable nginx\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/nginx.yml 수정\n\n- module: nginx\n  # Access logs\n  access:\n    enabled: true\n    var.paths: \n      - &quot;/var/log/nginx/access.log*&quot;\n\n  # Error logs\n  error:\n    enabled: true\n      - &quot;/var/log/nginx/error.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-nginx.html\n\n\nnginx dashboards\n\n\n\n\n\n추가 사항\n\naccess.log에 관리자 페이지 관련하여 access_log가 많이 쌓이는 경우\n\n아래와 같이 access_log off; 추가\n\n$ cd /etc/nginx/conf.d\n$ vim management.conf\n        location / {\n                access_log off;\n        }\n\n\n\nPostgreSQL module 설정\n\nPostgreSQL이 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable postgresql\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/postgresql.yml 수정\n\n- module: postgresql\n  log:\n    enabled: true\n    var.paths:\n      - &quot;/data/postgres/pgdata/log/*.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-postgresql.html\n\n\npostgresql dashboards\n\n\n\n\nMySQL module 설정\n\nMySQL이 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable mysql\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/mysql.yml 수정\n\n- module: mysql\n  error:\n    enabled: true\n    var.paths:\n      - &quot;/data/mysql/logs/error.log*&quot;\n\n  slowlog:\n    enabled: true\n    var.paths:\n      - &quot;/data/mysql/logs/mysql-slow.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-mysql.html\n\n\nmysql dashboards\n\n\n\n\nMongoDB module 설정\n\nMongoDB가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable mongodb\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/mongodb.yml 수정\n\n- module: mongodb\n  log:\n    enabled: true\n    var.paths:\n      - &quot;/data/mongo/logs/*.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-mongodb.html\n\n\nmongodb dashboards\n\n\n\n\nRedis module 설정\n\nRedis가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable redis\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/redis.yml 수정\n\n- module: redis\n  log:\n    enabled: true\n    var.paths:\n      - &quot;/data/redis/redis-server.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-redis.html\n\n\nredis dashboards\n\n\n\n\nFilebeat setup\n\n# dashboard 생성을 위해 /usr/share/filebeat 이동하여 실행\n$ cd /usr/share/filebeat\n\n# -e 옵션으로 error 여부를 stdout 으로 출력. log에서 host에 정상 접속 했는지 확인 가능\n$ filebeat setup -e -c /etc/filebeat/filebeat.yml --dashboards\n\n\nsetup 완료 후 마지막 문장에 dashboard가 정상 로딩된 것을 확인\n\n\n\nFilebeat 구동\n\n$ systemctl start filebeat.service\n\n\nFilebeat log 확인\n\n$ journalctl -u filebeat\n\n\n\n\nReference\n\n\n  Filebeat Modules\n\n\n"
} ,
  
  {
    "title"    : "Elastic Filebeat 설치 및 설정",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Filebeat, Beat",
    "url"      : "/2021/08/05/elastic-filebeat-install.html",
    "date"     : "August 5, 2021",
    "excerpt"  : "Elastic Filebeat 설치 및 설정\n\n\n\nFilebeat Overview\n\n로그 정보를 수집하는 Filebeat를 각 VM(Ubuntu)에 설치하여 로그 파일을 Elasticsearch로 전송한다.\n\n\nFilebeat란\n\n\n\nElastic Stack에 포함되는 오픈소스로 파일 데이터와 로그 데이터를 경량화된 방식으로 수집하고 Logstash, Elasticsearch, Kibana 등으로 전달하는 수집기\n\n\nFilebeat 설치\n...",
  "content"  : "Elastic Filebeat 설치 및 설정\n\n\n\nFilebeat Overview\n\n로그 정보를 수집하는 Filebeat를 각 VM(Ubuntu)에 설치하여 로그 파일을 Elasticsearch로 전송한다.\n\n\nFilebeat란\n\n\n\nElastic Stack에 포함되는 오픈소스로 파일 데이터와 로그 데이터를 경량화된 방식으로 수집하고 Logstash, Elasticsearch, Kibana 등으로 전달하는 수집기\n\n\nFilebeat 설치\n\n# 설치\n$ curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.12.1-amd64.deb\n$ dpkg -i filebeat-7.12.1-amd64.deb\n\n# 설치 후 설치된 버전 upgrade 막기\n$ apt-mark hold filebeat\n\n# bin path를 .zshrc 파일에 추가 후 적용\nexport PATH=/usr/share/filebeat/bin:$PATH\n$ source .zshrc\n\n# filebeat 설정\n$ cd /etc/filebeat\n$ vim filebeat.yml\n\n# system 재부팅 시 자동 실행 설정\n$ systemctl enable filebeat\n\n\nyaml에 elasticsearch, kibana 호스트 정보 설정\n\n# =================================== Kibana ===================================\nsetup.kibana:\n  host: &quot;10.213.196.6:5601&quot;\n# ---------------------------- Elasticsearch Output ----------------------------\noutput.elasticsearch:\n  hosts: [&quot;10.213.196.68:9200&quot;,&quot;10.213.196.23:9200&quot;,&quot;10.213.196.44:9200&quot;]\n  protocol: &quot;http&quot;\n  username: &quot;elastic&quot;\n  password: &quot;elastic&quot;\n# ---------------------------- Filebeat inputs ----------------------------\nfilebeat.inputs:\n- type: log\n  enabled: false\n\n\nelasticsearch https 보안 설정 되어 있을 경우\n\n# ---------------------------- Elasticsearch Output ----------------------------\noutput.elasticsearch:\n  hosts: [&quot;10.213.196.68:9200&quot;,&quot;10.213.196.23:9200&quot;,&quot;10.213.196.44:9200&quot;]\n  protocol: &quot;https&quot;\n  username: &quot;elastic&quot;\n  password: &quot;elastic&quot;\n  \n  # ssl verification을 하지 않거나 인증서를 수동으로 등록하여야한다. 아래 택 1\n  \n  # ssl 검증하지 않기\n  ssl.verification_mode: &quot;none&quot;\n  \n  # ssl 인증서\n    ssl:\n    certificate_authorities: [&quot;/etc/elasticsearch/certs/ca.crt&quot;]\n    certificate: &quot;/etc/elasticsearch/certs/data1.crt&quot;\n    key: &quot;/etc/elasticsearch/certs/data1.key&quot;\n\n\n\nFilebeat 구동 및 확인\n\n$ cd /usr/share/filebeat\n$ filebeat setup -e -c /etc/filebeat/filebeat.yml --dashboards\n$ systemctl start filebeat\n$ systemctl status filebeat\n\n\n로그 확인\n\n$ journalctl -u filebeat\n\n\n\n\nFilebeat 모듈 설정\n\nElastic Filebeat 모듈 설정 참고\n\n\n\nReference\n\n\n  Filebeat Overview\n\n"
} ,
  
  {
    "title"    : "Elasticsearch Metricbeat 모듈 설정",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Metricbeat, Beat",
    "url"      : "/2021/08/02/elastic-metricbeat-modules.html",
    "date"     : "August 2, 2021",
    "excerpt"  : "Elastic Metricbeat 모듈 설정\n\n\n\nMetricbeat Modules\n\n메트릭 정보를 수집하는 Metricbeat의 모듈별 Output 설정방법을 정리하였다.\n\n\n\nModule 설정\n\n\nMetricbeat 설치\n\nMetricbeat 설치 및 설정 참고\n\n\nModule 확인\nmetric을 수집하기 위한 metricbeat의 module들 list 확인\n$ metricbeat modules list\n\n\nSystem module ...",
  "content"  : "Elastic Metricbeat 모듈 설정\n\n\n\nMetricbeat Modules\n\n메트릭 정보를 수집하는 Metricbeat의 모듈별 Output 설정방법을 정리하였다.\n\n\n\nModule 설정\n\n\nMetricbeat 설치\n\nMetricbeat 설치 및 설정 참고\n\n\nModule 확인\nmetric을 수집하기 위한 metricbeat의 module들 list 확인\n$ metricbeat modules list\n\n\nSystem module 설정\n# 필요 시 system 모듈 설정\n$ vi /etc/metricbeat/modules.d/system.yml\n\n# 추가 module 없이 활성화를 하면 system metric만 수집함.\n$ metricbeat modules enable\n\n\n\nPostgresql module 설정\n\nPostgres이 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable postgresql\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/postgresql.yml 수정\n\n- module: postgresql\n  enabled: true\n  metricsets:\n    - database\n    - bgwriter\n    - activity\n    - statement\n  period: 10s\n  hosts: [&quot;postgres://10.213.196.207:5432?sslmode=disable&quot;]\n  username: postgres\n  password: {암호}\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-module-postgresql.html\n\n\npostgresql dashboards\n\n\n\nRedis module 설정\n\nRedis가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable redis\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/redis.yml 수정\n\n- module: redis\n  metricsets:\n    - info\n    - keyspace\n  period: 10s\n\n  hosts: [&quot;127.0.0.1:6379&quot;]\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-redis.html\n\n\nredis dashboards\n\n\n\n\nMongodb module 설정\n\nMongodb가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable mongodb\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/mongodb.yml 수정\n\n- module: mongodb\n  metricsets:\n    - dbstats\n    - status\n    - collstats\n    - metrics\n  period: 10s\n\n  hosts: [&quot;localhost:27017&quot;]\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-mongodb.html\n\n\nmongodb dashboards\n\n\n\n\nMySQL module 설정\n\nMySQL이 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable mysql\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/mysql.yml 수정\n\n- module: mysql\n  metricsets:\n    - status\n    - performance\n    \n  period: 10s\n  \n  hosts: [&quot;metricbeat:password1234@tcp(127.0.0.1:13306)/&quot;]\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-mysql.html\n\n\n접속 에러시\n\nmysql의 root 계정이 localhost에서만 접속하도록 되어 있는지 확인\n\nex) docker의 경우 컨테이너 접속시 host ip로 접속\n# host ip 확인\n$ ifconfig\n\n\nmysql 접속하여 계정 생성 및 권한부여\n$ docer exec -it mysql bash\nmysql&amp;gt; select host, user from mysql.user;\n+-----------+------------------+\n| host      | user             |\n+-----------+------------------+\n| localhost | mysql.infoschema |\n| localhost | mysql.session    |\n| localhost | mysql.sys        |\n| localhost | root             |\n+-----------+------------------+\n7 rows in set (0.00 sec)\n\nmysql&amp;gt; create user &#39;metricbeat&#39;@&#39;172.18.0.1&#39; identified by &#39;password1234&#39;;\nQuery OK, 0 rows affected (0.02 sec)\n\nmysql &amp;gt; grant all privilieges on *.* to &#39;metricbeat&#39;@&#39;172.18.0.1&#39; identified by &#39;password1234&#39;;\nmysql &amp;gt; flush privileges;\n\n\n\n\n  자세한 내용은 MySQL 설치 및 사용자 생성 참조\n\n\nmysql dashboards\n\n\n\n\nNginx module 설정\n\nNginx가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable nginx\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/nginx.yml 수정\n\n- module: nginx\n  metricsets: [&quot;stubstatus&quot;]\n  enabled: true\n  period: 10s\n\n  # Nginx hosts\n  hosts: [&quot;http://127.0.0.1:9000&quot;]\n\n  # Path to server status. Default server-status\n  server_status_path: &quot;server-status&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-nginx.html\n\n\nnginx의 ngx_http_stub_status_module 모듈 활성화\n\n$ cd /etc/nginx/conf.d\n$ vim status.conf\n    server {\n        listen 9000;\n\n\t\tlocation /server-status {\n          stub_status;\t# stub_status 활성화\n          access_log off;\t# 접속 로그 비활성화\n          allow 127.0.0.1;\t# localhost에서만 접속 허용\n          deny all;\n\t\t}\n    }\n$ chmod 750 status.conf\n$ chown nginx:nginx status.conf\n$ nginx -s reload\n\n\n\n  metricbeat가 10초마다 localhost:9000/server-status를 호출하여 stub status 정보 가져감\n\n\nnginx dashboards\n\n\n\n\n\n\n\n\nKafka module 설정\n\nKafka가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable kafka\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/kafka.yml 수정\n\n- module: kafka\n  metricsets:\n    - partition\n    - consumergroup\n    - broker\n    - consumer\n    - producer\n  period: 10s\n  hosts: [&quot;localhost:9092&quot;]\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-kafka.html\n\n\nKafka with jolokia\n\nbroker, consumer, producer metricset을 가져오려면 jolokia를 이용하여 jmx 모니터링하여야한다.\n\nkafka에 jolokia가 javaagent로 붙어서 jvm 위에 실행\n\nhttps://dev.to/martinhynar/monitoring-kafka-brokers-using-jolokia-metricbeat-and-elasticsearch-5678 참조\n\nkafka dashboards\n\n\n\n\nZookeeper module 설정\n\nZookeeper가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable zookeeper\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/zookeeper.yml 수정\n\n- module: zookeeper\n  metricsets:\n    - connection\n    - mntr\n    - server\n  period: 10s\n  hosts: [&quot;localhost:2181&quot;]\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-zookeeper.html\n\n\nzookeeper dashboards\n\n\n\n\nMetricbeat setup\n# dashboard 생성을 위해 /usr/share/metricbeat 이동하여 실행\n$ cd /usr/share/metricbeat\n\n# -e 옵션으로 error 여부를 stdout 으로 출력. log에서 host에 정상 접속 했는지 확인 가능\n$ metricbeat setup -e -c /etc/metricbeat/metricbeat.yml --dashboards\n\n\nsetup 완료 후 마지막 문장에 dashboard가 정상 로딩된 것을 확인\n\n\n\nMetricbeat 구동\n\n$ systemctl start metricbeat.service\n\n\nMetricbeat log 확인\n\n$ journalctl -u metricbeat\n\n\n\n\nReference\n\n\n  Metricbeat Modules\n\n\n"
} ,
  
  {
    "title"    : "Elastic Stack Monitoring with Metricbeat",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Metricbeat, Metric, Monitoring, Observability",
    "url"      : "/2021/07/27/elastic-stack-monitoring.html",
    "date"     : "July 27, 2021",
    "excerpt"  : "Elastic Stack Monitoring with Metricbeat\n\n\n\nElastic Stack(ELK) 노드 자체의 메트릭 정보를 모니터링하기 위한 metricbeat 구축하기\n\n\n\n개요\n\nUse Metricbeat to send monitoring data\n\nMetricbeat를 사용하여 ELS(Elastic Stack) 모니터링 구축\n\n기존 Legacy 수집 방식과 Metricbeat 수집 방식이 있으나 7.x 버전에서는 Me...",
  "content"  : "Elastic Stack Monitoring with Metricbeat\n\n\n\nElastic Stack(ELK) 노드 자체의 메트릭 정보를 모니터링하기 위한 metricbeat 구축하기\n\n\n\n개요\n\nUse Metricbeat to send monitoring data\n\nMetricbeat를 사용하여 ELS(Elastic Stack) 모니터링 구축\n\n기존 Legacy 수집 방식과 Metricbeat 수집 방식이 있으나 7.x 버전에서는 Metricbeat를 사용하여 모니터링하도록 가이드하고 있다.\n\n아래 5가지 항목을 모니터링한다.\n\n\n  Elasticsearch\n  Logstash\n  Kibana\n  Beats\n  APM\n\n\n기존 Metricbeat로 수집되는 메트릭 정보들은 metricbeat-* 인덱스에 수집이 되지만 xpack.enabled 옵션으로 xpack 기능 활성화시 별도의 monitoring cluster로 수집하며 Kibana의 Stack Monitoring 메뉴에서 확인할 수 있다.\n\n\nMetricbeat 설치\n\nElastic Metricbeat 설치 및 설정 참고\n\n\nMetricbeat 설정\nmetricbeat.yml 수정\n\nsetup.kibana:\n  host: &quot;http://10.213.196.6:5601&quot;\n  \n  # ssl 인증서 생성 시 https 사용\n  host: &quot;https://172.25.0.166:5601&quot;\n\n  # ssl 인증서 적용 시 추가\n  ssl:\n    enabled: true\n    certificate_authorities: [&quot;/etc/logstash/certs/ca.crt&quot;]\n    certificate: &quot;/etc/logstash/certs/logstash-1.crt&quot;\n    key: &quot;/etc/logstash/certs/logstash-1.key&quot;\n\noutput.elasticsearch:\n  # elasticsearch 노드 IP 넣기\n  hosts: [&quot;{엘라스틱서치1}:9200&quot;,&quot;{엘라스틱서치2}:9200&quot;,&quot;{엘라스틱서치3}:9200&quot;]\n\n  # ssl 인증서 적용 시 https, 미 적용 시 http 사용\n  protocol: &quot;https&quot;\n\n  username: &quot;elastic&quot;\n  password: &quot;{암호}&quot;\n\n  # ssl 인증서 적용시\n  ssl:\n    certificate_authorities: [&quot;/etc/logstash/certs/ca.crt&quot;]\n    certificate: &quot;/etc/logstash/certs/logstash-1.crt&quot;\n    key: &quot;/etc/logstash/certs/logstash-1.key&quot;\n\n\nelastic stack monitoring 활성화시 시스템 정보를 수집해도 UI에 보이지 않으므로 비활성화한다.\n$ metricbeat modules disable system\n\n\n\nElasticsearch Monitoring\n\nElasticsearch 설정\nelasticsearch.yml 수정\n\nxpack.monitoring.collection.enabled: true\t# xpack 모니터링(metricbeat 사용)\nmonitoring.enabled: false\t\t\t\t\t\t\t\t\t# legacy 모니터링(metricbeat 미사용)\n\n기존 legacy 방식 모니터링은 false, xpack 모니터링은 true로 설정\n\nelasticsearch-xpack module 활성화\n\n$ metricbeat modules disable elasticsearch\n$ metricbeat modules enable elasticsearch-xpack\n\nxpack과 기본 모듈중 하나만 사용해야 하기 때문에 기존의 모듈은 disable\n\nelasticsearch-xpack module 설정\n\nmodules.d/elasticsearch-xpack.yml 수정\n- module: elasticsearch\n  xpack.enabled: true\n  period: 10s\n  hosts: [&quot;http://localhost:9200&quot;, &quot;http://localhost:9201&quot;, &quot;http://localhost:9202&quot;]\n  \n  # elasticsearch 모니터링용 user(Optional)\n  #username: &quot;user&quot;\n  #password: &quot;secret&quot;\n\n\nelasticsearch 모니터링용 사용자 생성(Optional)\nremote_monitoring_collector 롤을 가진 사용자를 생성. 생성한 사용자 id는 elasticsearch_monitoring\n\nMetricbeat 구동 및 kibana에서 확인\nmetricbeat 서비스 구동\n\n$ systemctl start metricbeat.service\n\n\nkibana Stack Monitoring 메뉴에서 Metricbeat monitoring 상태임을 확인\n\n\n\n\nLogstash Monitoring\n\nLogstash 설정\n\n기본 모니터링 설정 끄기\nkibana에서 stack monitoring 확인 시 Self monitoring으로 표기될 시 기본 모니터링 옵션을 끈다.\n\n\nlogstash.yml에 추가\n\nmonitoring.enabled: false\n\n\nlogstash-xpack 모듈 활성화\n$ metricbeat modules disable logstash\n$ metricbeat modules enable logstash-xpack\n\nxpack과 기본 모듈중 하나만 사용해야 하기 때문에 기존의 모듈은 disable\n\nlogstash-xpack 모듈 설정\nCollect Logstash monitorin data with Metricbeat\n\nmodules.d/logstash-xpack.yml 수정\n\n- module: logstash\n  metricsets: [&quot;node&quot;,&quot;node_stats&quot;]\n  xpack.enabled: true\n  # hosts 값에는 metric 수집 대상을 추가. \n  hosts: [&quot;http://172.25.0.66:9600&quot;]\n  period: 10s\n  # logstash 모니터링용 사용자(Optional)\n  username: &quot;logstash_monitoring&quot;\n  password: &quot;Infra1111&quot;\n\n\nlogstash 모니터링용 user 생성(Optional)\nkibana web ui 에서 아래 화면과 같이 모니터링용 user를 생성\n\n\nroles에는 remote_monitoring_collector 를 선택\n\nMetricbeat 구동 및 kibana에서 확인\n\nmetricbeat 서비스 구동\n\n$ systemctl start metricbeat.service\n\n\nkibana Stack Monitoring 메뉴에서 Metricbeat monitoring 상태임을 확인\n\n\n\n\nKibana Monitoring\n\nKibana 설정\n\n기본 모니터링 설정 끄기\n\nkibana에서 stack monitoring 확인 시 Self monitoring으로 표기될 시 기본 모니터링 옵션을 끈다.\n\n\nkibana.yml에 추가\n\nmonitoring.kibana.collection.enabled: false\n\n기존 legacy 방식으로 모니터링 데이터를 수집하는 monitoring.kibana.collection.enabled 설정 값을 false로 변경.\n\nkibana-xpack module 활성화\n$ metricbeat modules disable kibana\n$ metricbeat modules enable kibana-xpack\n\nxpack과 기본 모듈중 하나만 사용해야 하기 때문에 기존의 모듈은 disable\n\nkibana-xpack module 설정\nKibana module\n\nmodules.d/kibana-xpack.yml 수정\n\n- module: kibana\n  # 두 개의 metricset 설정 가능.\n  metricsets: [&quot;stats&quot;,&quot;status&quot;]\n  # metricbeat로 모니터링 시 아래 옵션을 true로 설정해야 함\n  xpack.enabled: true\n  period: 10s\n  # kibana 서버 url\n  hosts: [&quot;http://10.213.196.6:5601&quot;]\n  scopse: node\n  # kibana 모니터링용 user(Optional)\n  username: &quot;kibana_monitoring&quot;\n  password: &quot;Infra1111&quot;\n\n  # ssl 인증서 생성하여 적용 시 아래 옵션 적용\n  ssl.enabled: true\n  ssl.certificate_authorities: [&quot;/etc/kibana/certs/ca.crt&quot;]\n  ssl.certificate: &quot;/etc/kibana/certs/rmakers-kibana.crt&quot;\n  ssl.key: &quot;/etc/kibana/certs/rmakers-kibana.key&quot;\n  ssl.verification_mode: &quot;full&quot;\n\n\nkibana 모니터링용 사용자 생성(Optional)\nremote_monitoring_collector 롤을 가진 사용자를 생성. 생성한 사용자 id는 kibana_monitoring\n\nMetricbeat 구동 및 kibana에서 확인\nmetricbeat 서비스 구동\n\n$ systemctl start metricbeat.service\n\n\nkibana Stack Monitoring 메뉴에서 Metricbeat monitoring 상태임을 확인\n\n\n\n\n\nReference\n\n\n  \n    Use Metricbeat to send monitoring data\n  \n  Collect Logstash monitorin data with Metricbeat\n  Kibana module\n\n\n"
} ,
  
  {
    "title"    : "Elastic Stack Observability 개요",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Kibana, Logstash, Monitoring, Observability",
    "url"      : "/2021/07/27/elastic-observability.html",
    "date"     : "July 27, 2021",
    "excerpt"  : "Elastic Stack Observability 개요\n\n\n\nObservability with the elastic stack\n\nElastic Stack을 이용하여 MSA 서비스 모니터링시 모니터링 중앙 집중화를 통한 가관측성(Observability)에 대하여 알아본다.\n\n\n모니터링의 3요소\n\n\n    \n    \n        \n    \n    \n\n\n\n\n운영시 모니터링 요소는 Logs, Metrics, APM 크게 3가지로 분류할 수 있...",
  "content"  : "Elastic Stack Observability 개요\n\n\n\nObservability with the elastic stack\n\nElastic Stack을 이용하여 MSA 서비스 모니터링시 모니터링 중앙 집중화를 통한 가관측성(Observability)에 대하여 알아본다.\n\n\n모니터링의 3요소\n\n\n    \n    \n        \n    \n    \n\n\n\n\n운영시 모니터링 요소는 Logs, Metrics, APM 크게 3가지로 분류할 수 있다.\n\n\n  Logs\n  Metrics\n  APM(Application Perfromance Monitoring)\n\n\n추가로 Heartbeat, Packetbeat 등을 이용한 서버의 가용성을 체크하는 Uptime Data가 있음\n\nLogs Data\n\n\n\n어플리케이션 로그, 서비스 로그 등의 각 모듈의 실제 로그로 Filebeat, FunctionBeat 등을 이용하여 수집한다.\n\nDB 롱쿼리, 호출 내역, 이벤트 데이터 수발신, Kafka 행등 서비스 레벨에서 작성하는 로그를 통한 가시성을 챙길 수 있다.\n\nMetrics Data\n\n\n\n\n\n호스트, 시스템의 리소스 정보 및 사용량, 부하율, 네트워크 트래픽 등\n\n주기적인 전체 요약이나 통계적인 정보로 일정 기간에 걸쳐서 샘플링되는 경향을 지닌 데이터이다.\n\nVM의 부하율, 메모리 점유율 등을 파악하여 Scale Out을 하거나 DB의 사용량을 파악하여 Worker node를 늘린다던지 하는 방향으로 사용할 수 있다.\n\nAPM Data\n\n\n\n\n\n어플리케이션 중점의 성능 정보로 실제 어플리케이션 오류 정보나 작업 수행 시간, 서비스간 상호 연동, 트랜잭션 추적, 병목 지점등 많은 정보를 담아낸다.\n\n성능 병목 파악, 에러 검출, Long Query, 트랜잭션 추적 등 어플리케이션 레벨의 가시성을 챙겨준다.\n\nApplication에 Agent가 사이드카 형태로 올라가는 방식으로 많이 사용한다.\n\nUptime Data\n\nLiveness, Readiness, Healthz 등 Uptime 정보\n\n\n\nELK Stack(Elastic Stack)을 채택한 이유\n\n위의 모니터링을 통하여 가시성을 챙기는 방법은 여러가지가 있을 수 있으나 ELK Stack을 채택한 이유는 크게 다음과 같다.\n\n\n  루씬(Lucene) 기반의 뛰어난 분산처리를 통한 실시간 검색 기능을 제공한다.\n  메트릭, 로그, APM 등의 모니터링 정보를 중앙 집중화할 수 있다.\n  오픈소스 기반으로 다양한 서드파티 라이브러리를 제공하여 여러 오픈소스들을 쉽게 연동 가능\n  다양한 시각화 라이브러리를 제공하여 큰 어려움 없이 데이터 시각화 가능\n  시계열 데이터를 운영하는데 최적화\n\n\nELK 적용 이전\n\n\n\n\n\n각기 다른 정보를 모니터링하기 위해 특화된 여러 툴들을 띄워놓고 모니터링한다.\n\n따라서 관리 포인트가 많아지고 데이터의 중복이 발생할 수 있다.\n\nELK 적용 이후\n\n\n\n\n  단일 페이지에서 중앙 집중화된 로그를 쉽게 모니터링 할 수 있다.\n\n\n모든 metric, apm, logs 정보들은 Elasticsearch의 인덱스이므로 데이터를 시각화, 머신러닝, 필터링 하는데 아무런 제약이 없다.\n\n로그 중앙집중화 및 가시성 확보뿐만 아니라 Elasticsearch는 데이터 분석 엔진으로도 탁월한 성능을 자랑한다.\n\n\n\nReference\n\n\n  Observability with the elastic stack\n\n"
} ,
  
  {
    "title"    : "Elastic Metricbeat 설치 및 설정",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Metricbeat, Beat",
    "url"      : "/2021/07/27/elastic-metricbeat-install.html",
    "date"     : "July 27, 2021",
    "excerpt"  : "Elastic Metricbeat 설치 및 설정\n\n\n\nMetricbeat Overview\n\n로그 정보를 수집하는 Filebeat를 각 VM(Ubuntu)에 설치하여 로그 파일을 Elasticsearch로 전송한다.\n\n\nMetricbeat란\n\nElastic Stack에 포함되는 오픈소스로 시스템과 서비스의 규격화된 메트릭 정보를 경량화된 방식으로 수집하고 Logstash, Elasticsearch, Kibana 등으로 전달하는 수집기\n\n\nM...",
  "content"  : "Elastic Metricbeat 설치 및 설정\n\n\n\nMetricbeat Overview\n\n로그 정보를 수집하는 Filebeat를 각 VM(Ubuntu)에 설치하여 로그 파일을 Elasticsearch로 전송한다.\n\n\nMetricbeat란\n\nElastic Stack에 포함되는 오픈소스로 시스템과 서비스의 규격화된 메트릭 정보를 경량화된 방식으로 수집하고 Logstash, Elasticsearch, Kibana 등으로 전달하는 수집기\n\n\nMetricbeat 설치\n현재 설치된 elasticsearch 버전을 확인 후 호환되는 버전의 metricbeat를 설치한다.\n\n되도록 elaticsearch 버전과 동일한 버전의 metricbeat을 설치한다.\n\nelastic component들 과거버전 선택 download:\n\n  https://www.elastic.co/kr/downloads/past-releases\n\n\nmetricbeat 7.11.2 download url:\n\n  https://www.elastic.co/downloads/past-releases/metricbeat-7-11-2\n\n\nwget 으로 7.11.2 버전을 download 및 설치:\n# 다운로드\n$ wget https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.11.2-amd64.deb\n\n# CentOS 7 용(rpm)\n$ wget https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.12.1-x86_64.rpm\n\n# 설치\n$ dpkg -i metricbeat-7.11.2-amd64.deb\n# 7.12.1 버전\n$ dpkg -i metricbeat-7.12.1-amd64.deb\n#CentOS rpm\n$ rpm -ivh ./metricbeat-7.12.1-x86_64.rpm\n\n# 설치 후 설치된 버전 upgrade 막기\n$ apt-mark hold metricbeat\n\n# 설치파일 삭제\n$ rm metricbeat-7.11.2-amd64.deb\n\n# bin path를 .zshrc 파일에 추가 후 적용\nexport PATH=/usr/share/metricbeat/bin:$PATH\n\n$ source .zshrc\n\n\nsystem booting 시 자동 실행 등록.\n$ systemctl enable metricbeat.service\n$ systemctl daemon-reload\n\n\n\nMetricbeat 설정\nmetricbeat 설치 후 elasticsearch로의 접속 설정 및 보안 설정 진행\n\nconfig directory:\n\n  /etc/metricbeat\n\n\nconfig file:\n\n  metricbeat.yml\n\n\nMetricbeat 접속 설정 및 보안 설정\n미리 구축해둔 elasticsearch에 연결할 예정이므로 output.elasticsearch 섹션에 연결 설정을 진행한다.\n\n설정 진행 시 ssl 부분도 설정하며 인증서 파일은 data node용으로 만들어둔 인증서를 사용한다.\n\nelasticsearch 연결 설정:\n\n  output.elasticsearch 값을 설정\n  username, password 는 hard-coded 하지 말고 serets keystore 등으로 대체 필요\n\n\nmetricbeat.yml 설정 내용 - SSL 인증 키 없이 http 접속 시 아래 설정 사용\n# kibana 접속 정보 설정.\nsetup.kibana:\n  host: &quot;http://10.213.196.6:5601&quot;\n\noutput.elasticsearch:\n  # master node들 \n  hosts: [&quot;10.213.196.68:9200&quot;,&quot;10.213.196.23:9200&quot;,&quot;10.213.196.44:9200&quot;]\n\n  # 보안설정 후 https 접속\n  protocol: &quot;http&quot;\n\n  username: &quot;elastic&quot;\n  password: &quot;elastic&quot;\n\n\nmetricbeat.yml 설정 내용 - SSL 인증 키 생성하여 적용 시 아래 설정 사용:\n# kibana 접속 정보 설정.\nsetup.kibana:\n  host: &quot;https://172.25.0.166:5601&quot;\n\n  ssl:\n    enabled: true\n    certificate_authorities: [&quot;/etc/elasticsearch/certs/ca.crt&quot;]\n    certificate: &quot;/etc/elasticsearch/certs/data1.crt&quot;\n    key: &quot;/etc/elasticsearch/certs/data1.key&quot;\n    \noutput.elasticsearch:\n  # master node들 \n  hosts: [&quot;172.25.0.92:9200&quot;, &quot;172.25.0.159:9200&quot;, &quot;172.25.0.121:9200&quot;]\n\n  # 보안설정 후 https 접속\n  protocol: &quot;https&quot;\n\n  #api_key: &quot;id:api_key&quot;\n  username: &quot;elastic&quot;\n  password: &quot;{암호}&quot;\n\n  # tls 설정 값. logstash-1 서버에 적용중이기 때문에 해당 node의 인증서를 적용\n  ssl:\n    certificate_authorities: [&quot;/etc/elasticsearch/certs/ca.crt&quot;]\n    certificate: &quot;/etc/elasticsearch/certs/data1.crt&quot;\n    key: &quot;/etc/elasticsearch/certs/data1.key&quot;\n\n\n설정파일 테스트. 설정 파일이 위치한 directory 에서 테스트 가능하며 테스트 후 OK가 출력되면 OK:\n# 테스트 진행 후 Confg OK 출력 확인\n$ metricbeat test config -e\n\n\n\n\nSystem module 설정\n# 필요 시 system 모듈 설정\n$ vi /etc/metricbeat/modules.d/system.yml\n\n# 추가 module 없이 활성화를 하면 system metric만 수집함.\n$ metricbeat modules enable\n\n\nMetricbeat Setup\n# asset 로딩 시 dashboard 생성을 위해 /usr/share/metricbeat 으로 이동하여 실행\n$ cd /usr/share/metricbeat\n\n# -e 옵션으로 error 여부를 stdout 으로 출력. log에서 host에 정상 접속 했는지 확인 가능\n$ metricbeat setup -e -c /etc/metricbeat/metricbeat.yml --dashboards\n\n\nMetricbeat 구동 &amp;amp; metric 값 전송\n$ systemctl start metricbeat.service\n\n\n\n\nMetricbeat 모듈 설정\n\nElastic Metricbeat 모듈 설정 참고\n\n\n\nReference\n\n\n  Metricbeat Overview\n\n"
} ,
  
  {
    "title"    : "Elastic APM Java Agent 적용하기",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, APM, ArgoCD, GitOps, Helm, Java, JVM",
    "url"      : "/2021/07/27/elastic-apm-java-agent.html",
    "date"     : "July 27, 2021",
    "excerpt"  : "Elastic APM Java Agent 적용하기\n\n\n\nElastic APM Java Agent 적용기\n\n\n\nAPM Agent 설치\nAPM Agents\n\napm-agent를 사용하기 위해서는 apm-server가 필요하다.\n\napm-server가 구축되어 있지 않으면 Elastic APM Server 문서 참고\n\nAPM Java Agent 공식 지원 설치 방법은 3가지가 있다.\n\n1. Manual setup with -javaagent f...",
  "content"  : "Elastic APM Java Agent 적용하기\n\n\n\nElastic APM Java Agent 적용기\n\n\n\nAPM Agent 설치\nAPM Agents\n\napm-agent를 사용하기 위해서는 apm-server가 필요하다.\n\napm-server가 구축되어 있지 않으면 Elastic APM Server 문서 참고\n\nAPM Java Agent 공식 지원 설치 방법은 3가지가 있다.\n\n1. Manual setup with -javaagent flag\n\n2. Automatic setup with apm-agent-attach-cli.jar\n\n3. Programmatic API setup to self-attach\n\n본 문서에서는 최종적으로 아래 -javaagent와 k8s의 InitContainer를 사용하는 방법을 사용\n\n4. -javaagent flag with Kubernetes InitContainer\n\n\n\nManual setup with -javaagent flag\nAPM Java Agent Reference\n\n별도 코드 수정이 필요없는 -javaagent 플래그를 이용한 에이전트 설치 방법\n\n1. Dockerfile 수정\n\nFROM openjdk:8-jre-alpine\n\nADD your-project/build/libs/*.jar makers-web.jar\nADD https://search.maven.org/remotecontent?filepath=co/elastic/apm/elastic-apm-agent/1.25.0/elastic-apm-agent-1.25.0.jar elastic-apm-agent.jar\n\nENTRYPOINT [&quot;java&quot;, &quot;-javaagent:/elastic-apm-agent.jar&quot;, &quot;-Delastic.apm.service_name=your-project&quot;, &quot;-Delastic.apm.application_packages=com.company.app&quot;, &quot;-Delastic.apm.server_url=http://{엘라스틱 APM 서버 주소}:{포트}&quot;, &quot;-Delastic.apm.environment=local&quot;, &quot;-jar&quot;, &quot;your-project.jar&quot;]\n\n\n\n  maven repo에서 APM Agent jar를 받아서 -javaagent 플래그로 JVM 위에 같이 실행한다.\n\n\n플래그\n\n  -Delastic.apm.service_name=your-project\n    \n      APM 모니터링시 확인할 서비스명(필수)\n      APM 메뉴에 표출될 서비스명을 적기\n    \n  \n  -Delastic.apm.application_packages=com.company.app\n    \n      APM 모니터링할 root package(필수)\n      APM 모니터링할 Java root package명 적기\n    \n  \n  -Delastic.apm.server_url=http://{엘라스틱  APM 서버 주소}:{포트}\n    \n      APM Server 주소(필수)\n      APM Server 엔드포인트\n    \n  \n  -Delastic.apm.environment=local\n    \n      APM 모니터링 환경(옵션)\n    \n  \n\n\n그 외 자세한 configuration 값은 APM Java Agent Configuration 참조\n\n2. Jenkins 빌드\n\n3. 로그 확인\n\n2021-07-27 14:21:04,144 [main] INFO  co.elastic.apm.agent.util.JmxUtils - Found JVM-specific OperatingSystemMXBean interface: com.sun.management.OperatingSystemMXBean\n2021-07-27 14:21:04,535 [main] INFO  co.elastic.apm.agent.configuration.StartupInfo - Starting Elastic APM 1.25.0 as makers-web on Java 1.8.0_212 Runtime version: 1.8.0_212-b04 VM version: 25.212-b04 (IcedTea) Linux 4.15.0-144-generic\n2021-07-27 14:21:04,535 [main] INFO  co.elastic.apm.agent.configuration.StartupInfo - environment: &#39;local&#39; (source: Java System Properties)\n2021-07-27 14:21:04,535 [main] INFO  co.elastic.apm.agent.configuration.StartupInfo - server_url: &#39;http://{엘라스틱 APM 서버 주소}:{포트}&#39; (source: Java System Properties)\n2021-07-27 14:21:04,536 [main] INFO  co.elastic.apm.agent.configuration.StartupInfo - application_packages: &#39;com.company.app&#39; (source: Java System Properties)\n2021-07-27 14:21:11,952 [main] INFO  co.elastic.apm.agent.impl.ElasticApmTracer - Tracer switched to RUNNING state\n2021-07-27 14:21:12,340 [elastic-apm-server-healthcheck] INFO  co.elastic.apm.agent.report.ApmServerHealthChecker - Elastic APM server is available: {  &quot;build_date&quot;: &quot;2021-04-20T19:55:39Z&quot;,  &quot;build_sha&quot;: &quot;32f34ed4298d648bf9476790f2a8a54d72805bb6&quot;,  &quot;version&quot;: &quot;7.12.1&quot;}\n\n\nApplication 시작 전에 위와 같은 로그가 나오면 정상 동작 확인\n\n\n\nAutomatic setup with apm-agent-attach-cli.jar\n\nAutomatic setup with apm-agent-attach-cli.jar\n\n해당 방법은 현재 Host에서 실행되는 모든 JVM에 적용하므로 원하는 서비스만 선택 적용이 어렵다는 단점이 있음\n\n또한 서비스별 config를 적용할 수 있는 가이드가 제공되지 않는다.\n\nAPM 서버에서 수집하는 서비스명이 동일하게 나오는 문제가 있음\n\n아래는 모든 k8s worker 노드에 수행되어야함\n\n1. apm-agent-attach-cli.jar 다운로드\n\n$ wget -v -O apm-agent-attach-cli-1.25.0.jar https://search.maven.org/remotecontent?filepath=co/elastic/apm/apm-agent-attach-cli/1.25.0/apm-agent-attach-cli-1.25.0.jar\n\n2. attach.sh 작성\n\n#!/usr/bin/env bash\nset -ex\n\nattach () {\n    # only attempt attachment if this looks like a java container\n    if [[ $(docker inspect ${container_id} | jq --raw-output .[0].Config.Cmd[0]) == java ]]\n    then\n        echo attaching to $(docker ps --no-trunc | grep ${container_id})\n        docker cp ./apm-agent-attach-*-cli.jar ${container_id}:/apm-agent-attach-cli.jar\n        docker exec ${container_id} java -jar /apm-agent-attach-cli.jar --config\n    fi\n}\n\n# attach to running containers\nfor container_id in $(docker ps --quiet --no-trunc) ; do\n    attach\ndone\n\n# listen for starting containers and attach to those\n\ndocker events --filter &#39;event=start&#39; --format &#39;{{.ID}}&#39; |\nwhile IFS= read -r container_id\ndo\n    attach\ndone\n\n\n\n현재 구동중인 JVM 컨테이너와 앞으로 구동될 JVM 컨테이너에 java agent attach하는 방법\n\n3. jq 설치\n\n$ apt install jq\n$ jq --version\njq-1.5-1-a5b5cbe\n\n\n위 attach 스크립트에서 java를 검사하는 방법은 jq 사용하여 json 검사하는 방법이므로 jq 설치 필요\n\n4. 스크립트 실행\n\n$ chmod +x ./attach.sh\n$ ./attach.sh &amp;amp;\n\n\n\n\nProgrammatic API setup to self-attach\n\nProgrammatic API setup to self-attach\n\n소스코드 dependency 추가하여 직접 설정하는 방법\n\n어플리케이션 코드에 직접적인 코드 및 수정이 필요하므로 별로 추천하지는 않는다.\n\n마이크로서비스에서의 많은 서비스들 일일이 추가하기도 어렵고 중복되기 때문\n\n1. pom.xml\n\n&amp;lt;dependency&amp;gt;\n    &amp;lt;groupId&amp;gt;co.elastic.apm&amp;lt;/groupId&amp;gt;\n    &amp;lt;artifactId&amp;gt;apm-agent-attach&amp;lt;/artifactId&amp;gt;\n    &amp;lt;version&amp;gt;${elastic-apm.version}&amp;lt;/version&amp;gt;\n&amp;lt;/dependency&amp;gt;\n\n\n2. Application.java\n\nimport co.elastic.apm.attach.ElasticApmAttacher;\nimport org.springframework.boot.SpringApplication;\n\n@SpringBootApplication\npublic class MyApplication {\n    public static void main(String[] args) {\n        ElasticApmAttacher.attach();\n        SpringApplication.run(MyApplication.class, args);\n    }\n}\n\n\n각 어플리케이션의 java main class에 위 코드를 추가한다.\n\n\n\nKubernetes InitContainer(사용)\n\n참고: 엘라스틱서치 APM을 이용해서 쿠버네티스 환경의 자바 어플리케이션 모니터링하기(1)\n\n-javaagent 와 Kubernetes에서 제공하는 InitContainer 기능을 사용하는 방법\n\nInitContainer\n\n\nflowchart LR\n  subgraph Pod\n\t  A[InitContainer]\n    B[Container]\n    subgraph Volume\n    C[EmptyDir]\n  \tend\n  end\n  A--Create--&amp;gt;C\n  C-.Delete.-&amp;gt;A\n  C--Mount--&amp;gt;B\n\n\nInit Containers\n\nDeployment 파드 안에 컨테이너 앱 실행 이전에 실행된다.\n\nInitContainer는 항상 실행이 보장되어 있으므로 서비스와 모니터링을 별개의 레벨로 분리할 수 있음\n\n공유 볼륨을 이용하여 apm-agent jar를 공유 볼륨에 넣어놓고\n\n컨테이너 실행시 InitContainer에서 apm-agent jar를 복사하는 방식\n\nkubernetes.yaml 수정(추가된 부분만 작성)\n\napiVersion: apps/v1\nkind: Deployment\n...\nspec:\n  template:\n    metadata:\n    ...\n    spec:\n      initContainers:                        # apm-agent 적용을 위한 initContainer\n      - name: elastic-java-agent\n        image: docker.elastic.co/observability/apm-agent-java:1.25.0\n        volumeMounts:\n        - mountPath: /elastic/apm-agent\n          name: elastic-apm-agent\n        command: [&#39;cp&#39;, &#39;-v&#39;, &#39;/usr/agent/elastic-apm-agent.jar&#39;, &#39;/elastic/apm-agent&#39;]\n      containers:\n      - name: your-project\n        image: abcde\n        volumeMounts:\n        ...\n        - name: elastic-apm-agent\n          mountPath: /elastic/apm-agent\n        env:                                # apm-agent 필수 환경변수 추가\n        - name: ELASTIC_APM_SERVER_URL \n          value: &quot;http://{엘라스틱 APM 서버 주소}:{포트}&quot;\n        - name: ELASTIC_APM_SERVICE_NAME \n          value: &quot;your-project&quot;  \n        - name: ELASTIC_APM_ENVIRONMENT \n          value: local\n        - name: JAVA_TOOL_OPTIONS\n          value: -javaagent:/elastic/apm-agent/elastic-apm-agent.jar\n      ...\n      volumes:\n      ...\n      - name: elastic-apm-agent\n        emptyDir: {}\n...\n\n\n적용 이후 Jenkins 배포\n\npod 로그 확인\n\nPicked up JAVA_TOOL_OPTIONS: -javaagent:/elastic/apm-agent/elastic-apm-agent.jar\n2021-08-04 14:13:53,838 [main] INFO  co.elastic.apm.agent.util.JmxUtils - Found JVM-specific OperatingSystemMXBean interface: com.sun.management.OperatingSystemMXBean\n2021-08-04 14:13:54,174 [main] INFO  co.elastic.apm.agent.configuration.StartupInfo - Starting Elastic APM 1.25.0 as your-project on Java 1.8.0_212 Runtime version: 1.8.0_212-b04 VM version: 25.212-b04 (IcedTea) Linux 4.15.0-144-generic\n2021-08-04 14:13:54,226 [main] INFO  co.elastic.apm.agent.configuration.StartupInfo - service_name: &#39;your-project&#39; (source: Environment Variables)\n2021-08-04 14:13:54,227 [main] INFO  co.elastic.apm.agent.configuration.StartupInfo - environment: &#39;local&#39; (source: Environment Variables)\n2021-08-04 14:13:54,227 [main] INFO  co.elastic.apm.agent.configuration.StartupInfo - server_url: &#39;http://{엘라스틱 APM 서버 주소}:{포트}&#39; (source: Environment Variables)\n2021-08-04 14:13:54,227 [main] WARN  co.elastic.apm.agent.configuration.StartupInfo - To enable all features and decrease startup time, please configure application_packages\n2021-08-04 14:14:01,639 [main] INFO  co.elastic.apm.agent.impl.ElasticApmTracer - Tracer switched to RUNNING state\n2021-08-04 14:14:02,030 [elastic-apm-server-healthcheck] INFO  co.elastic.apm.agent.report.ApmServerHealthChecker - Elastic APM server is available: {  &quot;build_date&quot;: &quot;2021-04-20T19:55:39Z&quot;,  &quot;build_sha&quot;: &quot;32f34ed4298d648bf9476790f2a8a54d72805bb6&quot;,  &quot;version&quot;: &quot;7.12.1&quot;}\n\n\napm-server 접속 및 available 확인\n\nKibana 확인\n\n\n\n\n\n\n\n\n\n인덱스 정상 수집 확인\n\n\n\nReference\n\n\n  APM Agents\n  APM Java Agent Reference\n  APM Java Agent Configuration\n  Automatic setup with apm-agent-attach-cli.jar\n  Programmatic API setup to self-attach\n  엘라스틱서치 APM을 이용해서 쿠버네티스 환경의 자바 어플리케이션 모니터링하기(1)\n  Init Containers\n\n\n"
} ,
  
  {
    "title"    : "Elastic APM Server 구축하기",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, APM, ArgoCD, GitOps, Helm",
    "url"      : "/2021/07/22/elastic-apm-server.html",
    "date"     : "July 22, 2021",
    "excerpt"  : "Elastic APM Server 구축하기\n\n\n\nElastic APM Server 구축기\n\n\n\n개요\n\n\n\nAPM Server Overview\n\nAPM Server는 APM Agent로부터 메트릭, 로그 정보등을 수집하고 엘라스틱서치 Document로 변환하여 인덱스에 저장하는 역할을 한다.\n\nAPM?\n\nAPM(Application Performance Monitoring)\n\n어플리케이션 성능 지표 등을 모니터링하는 것을 말한다.\n\n어플리케...",
  "content"  : "Elastic APM Server 구축하기\n\n\n\nElastic APM Server 구축기\n\n\n\n개요\n\n\n\nAPM Server Overview\n\nAPM Server는 APM Agent로부터 메트릭, 로그 정보등을 수집하고 엘라스틱서치 Document로 변환하여 인덱스에 저장하는 역할을 한다.\n\nAPM?\n\nAPM(Application Performance Monitoring)\n\n어플리케이션 성능 지표 등을 모니터링하는 것을 말한다.\n\n어플리케이션 레벨의 메트릭 정보, 로그 정보, 트랜잭션 추적, API 레이턴시 등 인프라/컨테이너 레벨에서는 알기 힘든 정보들을 가시화해준다.\n\nJava APM을 예로들면 아래와 같은 정보들을 모니터링 가능하다.\n\n\n  HTTP Requests/Responses\n  Latency\n  Throughput\n  Transactions\n  Error rate\n  Dependencies\n  JVM Metrics\n  DB Queries\n\n\n\n\nElasitc APM 서버 설치\napm-server 설치(deb)\n1. 설치\n\n$ cd /root\n$ curl -L -O https://artifacts.elastic.co/downloads/apm-server/apm-server-7.12.1-amd64.deb\n$ dpkg -i apm-server-7.12.1-amd64.deb\n\n2. apm-server.yml 설정\n\n$ vim /etc/apm-server/apm-server.yml\n\n#-------------------------- Elasticsearch output --------------------------\noutput.elasticsearch:\n  hosts: [&quot;http://{엘라스틱서치1}:9200&quot;, &quot;http://{엘라스틱서치2}:9200&quot;, &quot;http://{엘라스틱서치3}:9200&quot;]\n  enabled: true\n  protocol: &quot;http&quot;\n  username: &quot;elastic&quot;\n  password: &quot;changeme&quot;\n\n3. 환경설정\n\n$ sudo -u apm-server apm-server setup\nIndex setup finished.\nLoaded Ingest pipelines\n\n\n  debian 패키지로 설치한 경우 공식문서에서는 non-root user로 실행권장(apm-server라는 유저가 자동 생성됨)\n  4. apm-server 실행\n\n\n$ sudo -u apm-server apm-server -e\n\n5. 접속 확인\n\n$ curl -ivs localhost:8200\n\n\napm-server 설치(k8s)\n\ncharts/apm-server\n\n헬름 차트를 사용하여 K8S에 배포\n\n1. gitlab 접속\n\n2. argocd git 저장소\n\n폴더 elastic/apm-server 생성\n\n\n  ArgoCD와 같은 GitOps 미적용시 일반 디렉토리로 대체\n\n\n3. 헬름 차트 다운로드\n\nhelm-charts/apm-server\n\ngit clone 또는 helm pull\n\n$ git clone git@github.com:elastic/helm-charts.git\n\n\n$ helm repo add elastic https://helm.elastic.co\n$ helm pull elastic/apm-server\n\n\n4. helm template에 secret 추가\n\n$ cd elastic/apm-server/templates\n$ vim secret.yaml\n\n\n{{- if .Values.secret }}\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: {{ .Values.secret.name }}\n  labels:\n    app: {{ .Chart.Name }}\n    release: {{ .Release.Name | quote }}\n    heritage: {{ .Release.Service }}\ntype: {{ .Values.secret.kind }}\ndata: \n  {{- range $key, $value := .Values.secret.data }}\n  {{ $key }}: {{ $value | quote }}\n  {{- end }}\n{{- end -}}\n\n\n\n  elastic apm-server 헬름 차트에서 secret 타입 템플릿을 제공하지 않으므로 직접 작성하는 과정이다.\n\n\n5. values.yaml 수정\n\n\n---\n# Allows you to add config files\napmConfig:\n  apm-server.yml: |\n    apm-server:\n      host: &quot;0.0.0.0:8200&quot;\n    queue: {}\n    output.elasticsearch:\n      hosts: [&quot;http://{엘라스틱서치1}:9200&quot;, &quot;http://{엘라스틱서치2}:9200&quot;, &quot;http://{엘라스틱서치3}:9200&quot;]\n      username: &quot;${ELASTICSEARCH_USERNAME}&quot;\n      password: &quot;${ELASTICSEARCH_PASSWORD}&quot;\n\n# Extra environment variables to append to the DaemonSet pod spec.\n# This will be appended to the current &#39;env:&#39; key. You can use any of the kubernetes env\n# syntax here\nextraEnvs:\n  - name: &#39;ELASTICSEARCH_USERNAME&#39;\n    valueFrom:\n      secretKeyRef:\n        name: elastic-credentials\n        key: username\n  - name: &#39;ELASTICSEARCH_PASSWORD&#39;\n    valueFrom:\n      secretKeyRef:\n        name: elastic-credentials\n        key: password\n\n# secret은 새로 만든 템플릿의 value이므로 추가\n# Allows kubernetes secret\nsecret:\n  name: elastic-credentials\n  type: Opaque\n  data:\n    username: ZWxhc3RpYw==\n    password: Szg2eWN5aEFERUtvV1Vid2dPMGo=\n\n# 노드 선택을 위한 셀렉터\nnodeSelector: \n  zone: private\n  role: worker\n  \n\n6. git commit &amp;amp; push\n\n7. ArgoCD new App\n\n\n  Application Name: apm-server\n  Project: default\n  Repository URL: ssh://git@{gitlab 주소}:30722/{group명}/{gitops 저장소 주소}.git\n  Path: elastic/apm-server\n  Cluster URL: https://kubernetes.default.svc\n  Namespace: elastic\n\n\n\n  \n    ArgoCD Sync\n\n    \n\n    \n      ArgoCD와 같은 GitOps 미사용시 해당 헬름 upgrade하여 직접 배포\n\n      $ helm install apm-server elastic/apm-server -f values.yaml\n      \n\n    \n  \n\n\n9. 배포 확인\n\n$ kubectl exec -it apm-server-apm-server-6ccd949d46-hfh29 bash -n elastic\n\nbash-4.2$ curl -i localhost:8200\nHTTP/1.1 200 OK\nContent-Type: application/json\nX-Content-Type-Options: nosniff\nDate: Thu, 22 Jul 2021 08:05:23 GMT\nContent-Length: 125\n\n{\n  &quot;build_date&quot;: &quot;2021-04-20T19:55:39Z&quot;,\n  &quot;build_sha&quot;: &quot;32f34ed4298d648bf9476790f2a8a54d72805bb6&quot;,\n  &quot;version&quot;: &quot;7.12.1&quot;\n}\n\n\n\n\nReference\n\n\n  APM Server Overview\n  charts/apm-server\n  helm-charts/apm-server\n\n\n"
} ,
  
  {
    "title"    : "Kubernetes Metricbeat 적용하기",
    "category" : "",
    "tags"     : " Kubernetes, Helm, K8S, Metricbeat, ArgoCD, GitOps, ELK, Elastic, Elasticsearch, Metric, Monitoring, Observability",
    "url"      : "/2021/06/30/kubernetes-metricbeat.html",
    "date"     : "June 30, 2021",
    "excerpt"  : "Kubernetes Metricbeat\n\n\n\nKubernetes 클러스터, 노드, 파드의 메트릭 정보 수집을 위한 kube-state-metrics와 metricbeat를 배포하여 메트릭 정보를 수집, 모니터링한다.\n\n\n\n개요\n\nKubernetes 메트릭 정보 수집을 위해 Metricbeat를 DaemonSet 형태로 띄워 메트릭 수집\n\n\nkube-state-metrics\nkube-system의 kube-state-metrics deplo...",
  "content"  : "Kubernetes Metricbeat\n\n\n\nKubernetes 클러스터, 노드, 파드의 메트릭 정보 수집을 위한 kube-state-metrics와 metricbeat를 배포하여 메트릭 정보를 수집, 모니터링한다.\n\n\n\n개요\n\nKubernetes 메트릭 정보 수집을 위해 Metricbeat를 DaemonSet 형태로 띄워 메트릭 수집\n\n\nkube-state-metrics\nkube-system의 kube-state-metrics deployment 필요\n\nhelm-chart를 이용하여 구성할 것임\n\nGitOps인 ArgoCD를 활용하여 구성한다고 가정\n\n아닌 경우 일반 디렉토리 생성하여 구성하여도 동일함\n\nkube-state-metrics using helm\n\nhelm-charts/kube-state-metrics\n\n1. GitLab ArgoCD repo에 kube-system 디렉토리 생성(새 네임스페이스이므로)\n\n2. kube-system/kube-state-metrics 디렉토리 생성\n\n3. 위 helm 링크에서 helm chart 다운받아 kube-state-metrics에 넣기\n\n4. values.yaml 수정\n\nworker 노드에 파드를 띄우기 위해 아래 nodeSelector 지정\n\n(zone과 role에 대한 label이 노드에 이미 지정되어 있다고 가정)\n\nnodeSelector:\n zone: private\n role: kong\n\n\n5. ArgoCD New App 생성(kube-system)\n\n6. ArgoCD sync 진행\n\n7. 배포 확인\n\n\nMetricbeat-kubernetes\n\nkubernetes 메트릭을 모니터링하기위해 metricbeat를 DaemonSet 형태로 k8s cluster에 띄워야함\n\nMetricbeat-kubernetes DaemonSet 배포\n\n1. GitLab ArgoCD repo kube-system/metricbeat-kubernetes 생성\n\n2. metricbeat-kubernetes.yaml 다운로드\n\n# 현재 ELK 버전 7.12.1이므로 버전을 맞춘다.\n$ curl -L -O    https://raw.githubusercontent.com/elastic/beats/7.12/deploy/kubernetes/metricbeat-kubernetes.yaml\n\n\n3. 위에서 받은 metricbeat-kubernetes.yaml gitlab repo에 파일 추가\n\n4. values.yaml 수정\n\nelastic search 호스트 지정\n - name: ELASTICSEARCH_HOST\n   value: elasticsearch\n - name: ELASTICSEARCH_PORT\n   value: &quot;9200&quot;\n - name: ELASTICSEARCH_USERNAME\n   value: elastic\n - name: ELASTICSEARCH_PASSWORD\n   value: changeme\n\n\n5. ArgoCD sync 진행\n\n6. 배포 확인\n\n\nKibana 대시보드 확인\n\n\n\n\n참고사항\n\n수집 영역\n\n  kubelet\n  kube-state-metrics\n  apiserver\n  controller-manager\n  scheduler\n  proxy\n\n\nMetricsets\n사용 가능한 Metricset들 리스트\n    apiserver\n    container\n    controllermanager\n    event\n    node\n    pod\n    proxy\n    scheduler\n    state_container\n    state_cronjob\n    state_daemonset\n    state_deployment\n    state_node\n    state_persistentvolumeclaim\n    state_pod\n    state_replicaset\n    state_resourcequota\n    state_service\n    state_statefulset\n    state_storageclass\n    system\n    volume\n\n\n\n\nReference\n\n\n  \n    Run Metricbeat on Kubernetes\n  \n  Kubernetes 통합 가시성 자습서: 메트릭 수집 및 분석\n  kube-state-metrics\n  helm-charts/kube-state-metrics\n\n"
} ,
  
  {
    "title"    : "MySQL dump &amp; restore",
    "category" : "",
    "tags"     : " MySQL, DB, RDBMS, SQL, Dump, Restore",
    "url"      : "/2021/06/15/mysql-dump.html",
    "date"     : "June 15, 2021",
    "excerpt"  : "MySQL dump &amp;amp; restore\n\n\n\nMySQL Dump 및 Restore 내용 정리\n\n\n\nDump &amp;amp; Restore\n\nDump\n\n현재 DB의 스키마와 데이터, 상태 등을 sql 쿼리 형태로 추출하는 것\n\nRestore\n\nDump된 결과물의 쿼리를 수행하는 것\n\n이론적으로는 dump 한 DB의 상태가 같아진다.\n\n1. MySQL 컨테이너 접속\n\n\n  \n    Using docker\n  \n\n\n# container id 확...",
  "content"  : "MySQL dump &amp;amp; restore\n\n\n\nMySQL Dump 및 Restore 내용 정리\n\n\n\nDump &amp;amp; Restore\n\nDump\n\n현재 DB의 스키마와 데이터, 상태 등을 sql 쿼리 형태로 추출하는 것\n\nRestore\n\nDump된 결과물의 쿼리를 수행하는 것\n\n이론적으로는 dump 한 DB의 상태가 같아진다.\n\n1. MySQL 컨테이너 접속\n\n\n  \n    Using docker\n  \n\n\n# container id 확인\n$ docker ps | grep -i mysql\ncd1ae64000ec   mysql/mysql-server:8.0   &quot;/entrypoint.sh --ch…&quot;   3 months ago   Up 3 months (healthy)   33060-33061/tcp, 0.0.0.0:13306-&amp;gt;3306/tcp   mysql\n\n# 컨테이너 접속\n$ docker exec -it cd1ae64000ec /bin/bash\n\n# 접속 확인\nbash-4.4#\nbash-4.4# whoami\nroot\n\n\n\n  \n    Using docker-compose\n  \n\n\n# 컨테이너 접속\n$ docker-compose exec mysql /bin/bash\n\n# 접속 확인\nbash-4.4#\nbash-4.4# whoami\nroot\n\n\n2. MySQL Dump\n# --databses: 복구할 db, --add-drop-database: database drop문 포함\n$ mysqldump --databases db1 db2 --add-drop-database -h localhost -u root -p{비밀번호} &amp;gt; {파일명}.sql\n$ mysqldump --databases mydb --add-drop-database -h localhost -u root -ppassword1234 &amp;gt; mysql_mydb_dump_20210615.sql\n\n\n3. MySQL Restore\n\n$ mysql -h localhost -u root -p{비밀번호} &amp;lt; {파일명}.sql\n# 예시\n$ mysql -h localhost -u root -ppassword1234 &amp;lt; mysql_mydb_dump_20210615.sql\n\n\n"
} ,
  
  {
    "title"    : "MySQL 설치 및 사용자 생성",
    "category" : "",
    "tags"     : " MySQL, DB, RDBMS, Docker, Docker-compose",
    "url"      : "/2021/05/31/mysql-install.html",
    "date"     : "May 31, 2021",
    "excerpt"  : "MySQL 설치 및 사용자 생성\n\n\n\nMySQL을 docker-compose로 설치 후 사용자 생성, 권한 설정을 진행한 내용이다.\n\n\n\nMySQL 설치\n\n1. 호스트 볼륨 생성\n$ mkdir -p /data/mysql\n\n\n2. docker-compose.yaml 작성\n접속 port는 13306(docker 외부) - 3306(docker 내부) 로 설정.\n\n$ vim docker-compose.yaml\n\n\nversion: &#39;3.1&#39;\nse...",
  "content"  : "MySQL 설치 및 사용자 생성\n\n\n\nMySQL을 docker-compose로 설치 후 사용자 생성, 권한 설정을 진행한 내용이다.\n\n\n\nMySQL 설치\n\n1. 호스트 볼륨 생성\n$ mkdir -p /data/mysql\n\n\n2. docker-compose.yaml 작성\n접속 port는 13306(docker 외부) - 3306(docker 내부) 로 설정.\n\n$ vim docker-compose.yaml\n\n\nversion: &#39;3.1&#39;\nservices:\n  mysql:\n    image: mysql/mysql-server:8.0\n    restart: always\n    container_name: mysql\n    ports:\n      - 13306:3306\t\t# 외부 port : 내부 port\n    environment:\n      TZ: Asia/Seoul\n      MYSQL_ROOT_PASSWORD: &quot;root&quot;\n    command: \n      - --character-set-server=utf8\n      - --collation-server=utf8_unicode_ci\n    volumes:\n      - /data/mysql:/var/lib/mysql\t# 호스트 볼륨 : 컨테이너 볼륨\n\n\n3. MySQL 구동\n\n$ docker-compose up -d mysql\n\n$ docker-compose ps mysql\nName               Command                  State                           Ports\n-----------------------------------------------------------------------------------------------------\nmysql   /entrypoint.sh --character ...   Up (healthy)   0.0.0.0:13306-&amp;gt;3306/tcp, 33060/tcp, 33061/tc\n\n$ docker-compose logs mysql\nAttaching to mysql\nmysql       | [Entrypoint] MySQL Docker Image 8.0.25-1.2.3-server\nmysql       | [Entrypoint] Starting MySQL 8.0.25-1.2.3-server\n\n\n\nMySQL 설정\n\n1. MySQL 컨테이너 접속\n\n\n  \n    Using docker\n  \n\n\n# container id 확인\n$ docker ps | grep -i mysql\ncd1ae64000ec   mysql/mysql-server:8.0   &quot;/entrypoint.sh --ch…&quot;   3 months ago   Up 3 months (healthy)   33060-33061/tcp, 0.0.0.0:13306-&amp;gt;3306/tcp   mysql\n\n# 컨테이너 접속\n$ docker exec -it cd1ae64000ec /bin/bash\n\n# 접속 확인\nbash-4.4#\nbash-4.4# whoami\nroot\n\n\n\n  \n    Using docker-compose\n  \n\n\n# 컨테이너 접속\n$ docker-compose exec mysql /bin/bash\n\n# 접속 확인\nbash-4.4#\nbash-4.4# whoami\nroot\n\n\n2. MySQL 사용자 생성\n\nMySQL 계정의 경우 계정명과 호스트가 같이 있다.\n\n위처럼 localhost로 host 생성하는 경우 localhost에서만 접속 가능(외부 접속 불가)하므로 외부 접속을 위한 계정을 생성해보자.\n\n1. MySQL 로그인\n\nbash-4.4# mysql -u root -p\nEnter password:\n\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 1600691\nServer version: 8.0.25 MySQL Community Server - GPL\n\nCopyright (c) 2000, 2021, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType &#39;help;&#39; or &#39;\\h&#39; for help. Type &#39;\\c&#39; to clear the current input statement.\n\nmysql&amp;gt;\n\n\n2. 사용자 생성\n\n# 사용자 확인\nmysql &amp;gt; select host, user from mysql.user;\n+-----------+------------------+\n| host      | user             |\n+-----------+------------------+\n| localhost | mysql.infoschema |\n| localhost | mysql.session    |\n| localhost | mysql.sys        |\n| localhost | root             |\n+-----------+------------------+\n\n# 전체 host에서 접속 가능한 사용자 생성\n# create user {사용자명}@{호스트} identified by {비밀번호};\nmysql &amp;gt; create user &#39;csupreme19&#39;@&#39;%&#39; identified by &#39;password1234&#39;;\n\n# 참고) 단일 IP host의 경우\nmysql &amp;gt; create user &#39;csupreme19&#39;@&#39;127.0.0.1&#39; identified by &#39;password1234&#39;;\n# 참고) IP 대역 host의 경우\nmysql &amp;gt; create user &#39;csupreme19&#39;@&#39;127.%&#39; identified by &#39;password1234&#39;;\n\n# 사용자 확인\nmysql &amp;gt; select host, user from mysql.user;\n+-----------+------------------+\n| host      | user             |\n+-----------+------------------+\n| %         | csupreme19       |\n| localhost | mysql.infoschema |\n| localhost | mysql.session    |\n| localhost | mysql.sys        |\n| localhost | root             |\n+-----------+------------------+\n\n# 접속 확인\n$ mysql -h 127.0.0.1 -u csupreme19 -p\n\n\n3. 권한 부여\n\n# 스키마 권한 추가\n# myschema 스키마의 모든 테이블\nmysql &amp;gt; grant all privileges on myschema.* to csupreme19@&#39;%&#39;;\n# 전체 스키마\nmysql &amp;gt; grant all privileges on *.* to csupreme19@&#39;%&#39;;\n\n# 권한 적용 필수\nmysql&amp;gt; flush privileges;\n\n\n"
} ,
  
  {
    "title"    : "Tomcat session clustering 적용기",
    "category" : "",
    "tags"     : " Tomcat, Session, Clustering, Multicast, Replication",
    "url"      : "/2021/05/24/tomcat-session-clustering.html",
    "date"     : "May 24, 2021",
    "excerpt"  : "Tomcat session clustering 적용기\n\n\n\nClustering/Session Replication How-To\n\n본 문서에서는 Kubernetes 환경에서의 Tomcat간 세션 공유를 위한 session clustering 적용기를 다룬다.\n\n\n\n문제점\n\n\n  flowchart LR\n  subgraph k8s\n  subgraph Node A\n  A[Tomcat A]\n  end\n  subgraph Node B\n  B[Tomc...",
  "content"  : "Tomcat session clustering 적용기\n\n\n\nClustering/Session Replication How-To\n\n본 문서에서는 Kubernetes 환경에서의 Tomcat간 세션 공유를 위한 session clustering 적용기를 다룬다.\n\n\n\n문제점\n\n\n  flowchart LR\n  subgraph k8s\n  subgraph Node A\n  A[Tomcat A]\n  end\n  subgraph Node B\n  B[Tomcat B]\n  end\n  C[Service]\n  end\n  D[User]\n  D--&amp;gt;C\n  C--&amp;gt;A &amp;amp; B\n  A x-.session.-x B\n\n\nSpring boot 기반 Tomcat  서비스는 클라이언트에서 요청시 세션을 Tomcat이 올라가 있는 노드에만 local 저장된다.\n\n이 경우 Pod replication, crash/shutdown 등으로 인하여 노드간 세션이 공유되지 않아 접속이 끊기는 문제가 발생할 수 있음\n\n\n\n요구사항\n\nKubernetes 클러스터 내에서 Pod 간 세션을 유지할 수 있는 방법 수립\n\n\n  유지보수가 쉬울 것(관리 포인트 적을 것)\n  어플리케이션(서비스) 코드에 영향이 없을 것\n  구성하는데 추가적인 리소스가 들어가지 않을 것\n\n\n\n\n방법\n\n1. Sticky session\n\n\n\n클라이언트가 세션을 맺은 서버랑만 통신하는 것\n\n사용자 입장에서는 세션이 끊기지 않고 유지되는 장점이 있으나\n\npod crash, shutdown 등의 문제로 pod 재시작, 중지시 세션이 끊기는 문제가 있다.\n\nkubernetes session affinity 설정으로 비교적 간단히 설정이 가능하나\n\n완벽한 session replication이 아니다.\n\n2. Tomcat session in-memory replication(Multicast)\n\nTomcat에서 기본적으로 제공하는 클러스터링\n\n서버 인메모리에 저장하고 SimpleTCPCluster와 같은 클래스를 이용하여 리플리케이션하는 방법\n\ntomcat xml(server.xml, pom.xml) 수정이 필요하여 spring boot에서 기본으로 제공하는 embedded tomcat에서 사용이 안된다고 하나 java config를 이용하여 embedded tomcat도 설정 가능한 것으로 보인다.\n\nmulticast 기능을 지원해야하지만 multicast는 대부분의 cloud 환경에서는 제공하지 않음\n\nTomcat 설정 파일을 바꾸려면 Spring boot의 경우 @Configuration 설정을 통하여 Bean을 주입해야하여 소스코드 추가가 필요하다.\n\n3. Tomcat session persistence replication\n\nredis, dynamoDB와 같은 DB에 세션 정보를 저장하고 각 서버로 클러스터링 하는 방법\n\nsession이 별도의 저장소에 저장되어 사용되는 장점이 있음\n\n\nTomcat session in-memory replication(Multicast) 테스트\n\nClustering/Session Replication How-To\n\nEPC Multicast 지원 테스트\n\n현재 사용중인 클라우드(EPC)에서 Multicast 기능을 지원하는지 테스트한다.\n\n멀티캐스트 IP 대역은 사설 224.0.1.0 ~ 238.255.255.255의 대역 IP가 예약되어 있으며\n\n멀티캐스트 사용을 위해서는 L2 장비가 IGMP 프로토콜을 지원해야한다고 한다.\n\n\n  AWS와 같은 메이저 클라우드에서는 지원을 안한다고 하니 확인 필요\n\n\niperf라는 네트워크 테스트 툴을 이용하여 EPC에서 멀티캐스트 IP 대역 사용이 가능한지 테스트한다.\n\nkubernetes worker 노드에서 진행\n\n# iperf 설치 (root 진행)\n$ wget -O /usr/bin/iperf https://iperf.fr/download/ubuntu/iperf_2.0.9\n$ chmod +x /usr/bin/iperf\n\n# Server side\n$ iperf -s -u -B 228.0.0.4 -i 1 -p 45564\n------------------------------------------------------------\nServer listening on UDP port 45564\nBinding to local address 228.0.0.4\nJoining multicast group  228.0.0.4\nReceiving 1470 byte datagrams\nUDP buffer size:  208 KByte (default)\n------------------------------------------------------------\n[  3] local 228.0.0.4 port 45564 connected with 10.213.196.14 port 49293\n[ ID] Interval       Transfer     Bandwidth        Jitter   Lost/Total Datagrams\n[  3]  0.0- 1.0 sec   129 KBytes  1.06 Mbits/sec   0.006 ms    0/   90 (0%)\n[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec   0.011 ms    0/   89 (0%)\n[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec   0.007 ms    0/   89 (0%)\n[  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec   0.007 ms    0/  269 (0%)\n\n# Client side (다른 노드에서 실행)\n$ iperf -c 228.0.0.4 -u -T 32 -t 3 -i 1 -p 45564\n------------------------------------------------------------\nClient connecting to 228.0.0.4, UDP port 45564\nSending 1470 byte datagrams, IPG target: 11215.21 us (kalman adjust)\nSetting multicast TTL to 32\nUDP buffer size:  208 KByte (default)\n------------------------------------------------------------\n[  3] local 10.213.196.14 port 49293 connected with 228.0.0.5 port 45564\n[ ID] Interval       Transfer     Bandwidth\n[  3]  0.0- 1.0 sec   131 KBytes  1.07 Mbits/sec\n[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec\n[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec\n[  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec\n[  3] Sent 269 datagrams\n\n\n\n  Tomcat의 경우 기본 multicast 주소는 228.0.0.4에 포트는 45564 사용\n\n\n서버에서 클라이언트 각각 실행하여 클라이언트에서 송신하는 메시지가 서버에 멀티캐스트 되는지 확인\n\n다른 노드에서 실행시 정상 송수신을 확인하였다. (Zone: Prd-private)\n\n\n\nTomcat session clustering 샘플 프로젝트 작성\n\nbuild.gradle 작성\n\nplugins {\n\tid &#39;org.springframework.boot&#39; version &#39;2.5.0&#39;\n\tid &#39;io.spring.dependency-management&#39; version &#39;1.0.11.RELEASE&#39;\n\tid &#39;java&#39;\n}\n\ngroup = &#39;com.example&#39;\nversion = &#39;0.0.1-SNAPSHOT&#39;\nsourceCompatibility = &#39;1.8&#39;\n\nrepositories {\n\tmavenCentral()\n}\n\ndependencies {\n\timplementation &#39;org.springframework.boot:spring-boot-starter-web&#39;\n\timplementation &#39;org.projectlombok:lombok:1.18.12&#39;\n\tannotationProcessor &#39;org.projectlombok:lombok:1.18.12&#39;\n\timplementation &#39;org.apache.tomcat:tomcat-catalina-ha:9.0.46&#39;\n\ttestImplementation &#39;org.springframework.boot:spring-boot-starter-test&#39;\n}\n\ntest {\n\tuseJUnitPlatform()\n}\n\n\n세션 정보 확인을 위한 간단한 Controller 작성\n\n@RestController\npublic class MainController {\n\t@GetMapping(&quot;/&quot;)\n\tpublic String getSession(HttpServletRequest request) {\n\t\tHttpSession session = request.getSession();\n\t\tString result = &quot;session ID: &quot; + session.getId() + &quot;\\n\\nsession created: &quot; + session.getCreationTime() + &quot;\\n\\nsession accessed: &quot; + session.getLastAccessedTime();\n\t\treturn result;\n\t}\n}\n\n\nTomcat config 설정\n\nTomcatClusterContextCustomizer\n\n@Component\npublic class TomcatClusterContextCustomizer implements TomcatContextCustomizer {\n\t@Value(&quot;${env.multicast.receiver.port}&quot;)\n\tprivate Integer receiverPort;\n\t\n\t@Override\n\tpublic void customize(final Context context) {\n\t\tcontext.setDistributable(true);\t// web.xml &amp;lt;distribute/&amp;gt;\n\t\t// Manager setting\n\t\tBackupManager manager = new BackupManager();\n\t\tmanager.setNotifyListenersOnReplication(true);\n\t\tcontext.setManager(manager);\n\t\tconfigureCluster((Engine)context.getParent().getParent());\n\t}\n\t\n\tprivate void configureCluster(Engine engine) {\n\t\t// Cluster setting\n\t\tSimpleTcpCluster cluster = new SimpleTcpCluster();\n\t\tcluster.setChannelSendOptions(8);\n\t\t// Channel setting\n\t\tGroupChannel channel = new GroupChannel();\n\t\t// Membership setting\n\t\tMcastService mcastService = new McastService();\n\t\tmcastService.setAddress(&quot;228.0.0.4&quot;);\n\t\tmcastService.setPort(45564);\n\t\tmcastService.setFrequency(500);\n\t\tmcastService.setDropTime(3000);\n\t\tchannel.setMembershipService(mcastService);\n\t\t// Receiver setting\n\t\tNioReceiver receiver = new NioReceiver();\n\t\treceiver.setAddress(&quot;auto&quot;);\t// tomcat의 LAN 주소를 가져온다. Ipv4로 설정해야함\n\t\treceiver.setPort(4000);\n//\t\treceiver.setPort(receiverPort);   톰캣이 같은 node에서 replication 구동시 리시버 포트는 서로 달라야함 이 경우 별도 포트 지정\n\t\treceiver.setAutoBind(100);\n\t\treceiver.setSelectorTimeout(5000);\n\t\treceiver.setMaxThreads(6);\n\t\tchannel.setChannelReceiver(receiver);\n\t\t// Sender setting\n\t\tReplicationTransmitter sender = new ReplicationTransmitter();\n\t\tsender.setTransport(new PooledParallelSender());\n\t\tchannel.setChannelSender(sender);\n\t\t// Intercepter setting\n\t\tchannel.addInterceptor(new TcpPingInterceptor());\n\t\tchannel.addInterceptor(new TcpFailureDetector());\n\t\tchannel.addInterceptor(new MessageDispatchInterceptor());\n\t\tcluster.addValve(new ReplicationValve());\n\t\tcluster.addValve(new JvmRouteBinderValve());\n\t\tcluster.setChannel(channel);\n\t\tcluster.addClusterListener(new ClusterSessionListener());\n\t\tengine.setCluster(cluster);\n\t}\n}\n\n\nTomcatClusterUtil 작성\n\n@Configuration\npublic class TomcatClusterUtil implements WebServerFactoryCustomizer&amp;lt;TomcatServletWebServerFactory&amp;gt;{\n\t@Autowired\n\tTomcatClusterContextCustomizer tomcatClusterContextCustomizer;\n\t\n\t@Override\n\tpublic void customize(final TomcatServletWebServerFactory factory) {\n\t\tfactory.addContextCustomizers(tomcatClusterContextCustomizer);\n\t}\n}\n\n\nspring boot와 같이 설치형 tomcat이 아닌 embedded tomcat에서는 server.xml, web.xml과 같은 설정 파일의 직접 수정이 불가능하므로\n\nTomcatContextCustomizer를 이용하여 Context에 설정한다.\n\n\n  항목별 자세한 설명은 공식문서 참조\n\n\n인스턴스별 포트 설정을 위한 application.yaml 작성\n\nspring:\n  profiles:\n    active: instance1\n\n---\nspring:\n  config:\n    activate:\n      on-profile: instance1\nserver:\n  port: 8080\nenv:\n  multicast.receiver.port: 4000\n\n---\nspring:\n  config:\n    activate:\n      on-profile: instance2\nserver:\n  port: 8081\nenv:\n  multicast.receiver.port: 4001\n\n\n\n  로컬 테스트시 같은 포트로 톰캣 인스턴스 실행이 불가능하므로 포트 설정을 하기 위함\n\n\nspring boot Run configuration 설정\n\n\n  \n    tomcat-session-replication-test1\n\n    \n      Profile: instance1\n      JVM Arguments: -Djava.net.preferIPv4Stack=true\n    \n  \n  \n    tomcat-session-replication-test2\n\n    \n      Profile: instance2\n      JVM Arguments: -Djava.net.preferIPv4Stack=true\n    \n  \n\n\n\n  preferIPv4Stack true로 설정해야 위에서 Receiver주소 설정값 receiver.setAddress(&quot;auto&quot;);가 작동한다.\n\n\n로드밸런서 설정\n\nTomcat에서 지원하는 session clustering을 사용하려면 로드밸런서를 통해 같은 도메인이름을 사용하여야 세션이 공유된다.\n\n서로 다른 도메인을 사용하거나 다른 서버인 경우 기본적으로 같은 session을 가지고 있다고 판단하지 않는 것 같다.\n\n따라서 같은 도메인 설정을 위한 로드밸런서 역할을 할 nginx를 사용하여 리버스 프록시 설정을한다.\n\n맥 로컬 기준 작성\n\nnginx 설치 및 설정\n\n$ brew install nginx\t# nginx 설치\n$ vim /usr/local/etc/nginx/nginx.conf\t# 로드밸런서 설정\n 17   http {\n 18     include       mime.types;\n 19     default_type  application/octet-stream;\n ...\n 35     upstream myproject {\n 36       server 127.0.0.1:8080;\n 37       server 127.0.0.1:8081;\n 38     }\n 39\n 40     server {\n 41         listen       80;\n 42         server_name  localhost;\n ...\n 48         location / {\n 49           proxy_pass http://myproject;\n 50         }\n $ nginx\t# local nginx 구동\n\n\nlocalhost:80 접속시 127.0.0.1:8080, 127.0.0.1:8081로 각각 로드밸런싱\n\n\n\n테스트(Local)\n\n테스트 순서\n\n  tomcat-session-replication-test1 구동\n  localhost 접속\n  session 확인\n  tomcat-session-replication-test2 구동\n  tomcat-session-replication-test1 중지\n  localhost 접속\n  session 확인\n\n\n테스트 진행 및 결과\n\n\n  tomcat-session-replication-test1 구동\n\n  localhost 접속\n  session 확인\n\n  tomcat-session-replication-test2 구동\n  tomcat-session-replication-test1 중지\n\n  localhost 접속\n  session 확인\n\n\n\nsession ID를 1번 인스턴스에서 생성했음에도 2번 tomcat으로 접속시 session ID가 서로 동일한 것을 확인할 수 있다.\n\n\n\n테스트(Dev EPC)\n\n현시점 기준 172.x 대역의 개발 클러스터에 설정하여 세션 공유가 kubernetes 환경에서도 적용되는지 테스트한다.\n\nkubernetes 배포\n\n  \n    gitlab 프로젝트 생성 및 commit\n  \n  \n    이미지 빌드를 위한 Dockerfile, k8s 배포를 위한 yaml 작성\n\n    \n      \n        Dockerfile\n      \n    \n\n    FROM openjdk:8-jdk-alpine\n   \n ADD build/libs/tomcat-session-replication-test-0.0.1-SNAPSHOT.jar TomcatSessionClusteringTest.jar\n RUN apk --no-cache add tzdata &amp;amp;&amp;amp; \\\n         cp /usr/share/zoneinfo/Asia/Seoul /etc/localtime &amp;amp;&amp;amp; \\\n         echo &quot;Asia/Seoul&quot; &amp;gt; /etc/timezone\n   \n ENTRYPOINT [&quot;java&quot;, &quot;-Duser.timezone=Asia/Seoul&quot;, &quot;-Djava.net.preferIPv4Stack=true&quot;, &quot;-jar&quot;, &quot;/TomcatSessionClusteringTest.jar&quot;, &quot;--spring.profiles.active=instance1&quot;]\n    \n    \n    \n      \n        kubernetes.yaml\n      \n    \n\n    ---\n apiVersion: apps/v1\n kind: Deployment\n metadata:\n   namespace: development\n   name: tomcatsessionclustertest-deploy\n   labels:\n     app: tomcatsessionclustertest\n spec:\n   replicas: 2\t# pod를 2개 띄움\n   selector:\n     matchLabels:\n       app: tomcatsessionclustertest\n   template:\n     metadata:\n       labels:\n         app: tomcatsessionclustertest\n     spec:\n       containers:\n       - name: tomcatsessionclustertest\n         image: gitlab.yourdomain.com/tomcatsessionclustertest:latest\n         volumeMounts:\n         - name: tz-seoul\n           mountPath: /etc/localtime\n         ports: \n         - containerPort: 8080\n       imagePullSecrets:\n       - name: reg\n       volumes:\n       - name: tz-seoul\n         hostPath: \n           path: /usr/share/zoneinfo/Asia/Seoul\n       affinity:\n       # 같은 node에 뜨지 않게 하기 위한 Affinity 설정\n         podAntiAffinity:\n           requiredDuringSchedulingIgnoredDuringExecution:\n           - labelSelector:\n               matchExpressions:\n               - key: app.kubernetes.io/name\n                 operator: In\n                 values:\n                 - tomcatsessionclustertest\n              topologyKey: kubernetes.io/hostname\n ---\n apiVersion: v1\n kind: Service\n metadata:\n   namespace: development\n   name: tomcatsessionclustertest-svc\n   labels:\n     app: tomcatsessionclustertest\n spec:\n   ports:\n   - port: 8080\n     nodePort: 32599\n     targetPort: 8080\n   selector:\n     app: tomcatsessionclustertest\n   type: NodePort\n      \n    \n  \n  \n    Jenkins 등록\n  \n\n\n파이프라인 아래와 같이 수정\nnode (&#39;jenkins-slave&#39;){\n    def gitBranch = &#39;master&#39;\n    def gitCredId = &#39;gitlab-ssh-key&#39;\n    def gitUrl = &#39;ssh://git@gitlab.yourdomain.com/group/&#39;\n    def projectUrl = gitUrl + &#39;tomcat-session-clustering-test.git&#39;\n    def dockerRepoUrl = &#39;http://nexus.yourdomain.com&#39;\n    def imageName = &#39;tomcatsessionclustertest&#39;\n    def defaultYaml = &#39;kubernetes.yaml&#39;\n    def deployYaml = &#39;kubernetes_deploy.yaml&#39;\n\n    stage(&#39;Checkout&#39;)    {\n         git(\n            url: projectUrl,\n            credentialsId: gitCredId,\n            branch: gitBranch\n        )\n    }\n    \n    stage(&#39;Build&#39;) {\n        sh &quot;chmod +x ./gradlew&quot;\n        sh &quot;./gradlew clean build -x test&quot;\n    }\n\n    stage(&#39;Analysis &amp;amp; Push&#39;) {\n        parallel (\n            &#39;Docker Build&#39;: {\n                def dockerfile = &#39;Dockerfile&#39;\n                app = docker.build(&quot;${imageName}:${env.BUILD_ID}&quot;,&quot;-f ${dockerfile} ./&quot;)\n            }\n        )\n        \n        parallel (\n            &#39;Registry Push&#39;: {\n                docker.withRegistry(&quot;${dockerRepoUrl}&quot;, &quot;dockerhub&quot;) {\n                    app.push(&quot;$BUILD_ID&quot;)\n                }\n            }\n        )\n    }\n    \n    stage(&#39;Kube Deploy&#39;){\n        \n        sh &quot;cat ${defaultYaml} | sed &#39;s/latest/${env.BUILD_ID}/g&#39; &amp;gt; ${deployYaml}&quot;\n\n        withKubeConfig(credentialsId: &#39;jenkins-builder&#39;, serverUrl: &#39;https://kubernetes&#39;) {\n            sh &quot;kubectl apply -f ${deployYaml}&quot;\n        }\n    }\n    \n}\n\n\n\n  CI/CD 환경 및 Kubernetes 환경은 각각 다르므로 위 파이프라인은 참고만 할 것\n\n\n\n  \n    kubernetes 배포 확인\n  \n\n\nkubernetes 대시보드 접속 또는 명령어 확인\n\n# alias k=kubectl\n$ k get po -n development\n$ k get svc -n development\n\n\n외부 접속 설정 및 접속 테스트\n\n\n  방화벽 오픈\n    \n      외부 -&amp;gt; kubernetes 32599 NodePort\n    \n  \n  접속 확인\n\n  접속 로그 확인\n\n  접속된 파드 제거\n    $ k get pod -n development\n$ k delete pod tomcatsessionclustertest-deploy-7d75d5548b-rknfh -n development\n    \n  \n  재접속\n\n\n\n테스트 결과\n\n\n\n\n\n세션 공유가 되지 않는것으로 확인되었다.\n\nkubernetes 환경에서 host 서버간 multicast 통신은 되는 것으로 판단되나 파드간 multicast 통신을 위해서는 multus-cni를 사용하여 NIC로 호스트와 연결해야함\n\n\n  참고: https://stackoverflow.com/a/61234801/15263734\n\n\nCode dependency도 높고 kubernetes 환경에 별도의 CNI 설치를 요구하므로 사용이 어렵다.\n\n\nTomcat session persistence replication 테스트\n\nRedis, dynamoDB, hazelcast 등의 in-memory DB를 이용하여 세션을 저장하고 공유하는 방법 테스트\n\n현재 EPC에서 Redis를 사용하기 때문에 redis를 이용하여 세션 저장하는 방법 테스트\n\n테스트(LOCAL)\n\n맥 로컬 기준 작성\n\n1. Redis 설치\n\nhttps://hub.docker.com/_/redis/\n\n$ docker pull redis\n$ docker run --name redis -p 6379:6379 -d redis\n$ docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                                       NAMES\n2e96b0e93c0a   redis     &quot;docker-entrypoint.s…&quot;   4 seconds ago   Up 2 seconds   0.0.0.0:6379-&amp;gt;6379/tcp, :::6379-&amp;gt;6379/tcp   redis\n\n\n2. 샘플 프로젝트 작성\n\nbuild.gradle\n\ndependencies {\n\timplementation &#39;org.springframework.boot:spring-boot-starter-web&#39;\n\ttestImplementation &#39;org.springframework.boot:spring-boot-starter-test&#39;\n\timplementation &#39;org.springframework.boot:spring-boot-starter-data-redis&#39;\n\timplementation &#39;org.springframework.session:spring-session-data-redis&#39;\n}\n\n\napplication.yaml\n\nspring:\n  profiles:\n    active: local\n\n---\nspring:\n  config:\n    activate:\n      on-profile: local\n  redis:\n    host: 127.0.0.1\n    port: 6379\nserver:\n  port: 8080\n\n---\nspring:\n  config:\n    activate:\n      on-profile: development\n  redis:\n    host: 172.25.0.163\n    port: 6379\nserver:\n  port: 8080\n\n\nMainController.java\n\n@RestController\npublic class MainController {\n\t\n\t@GetMapping(&quot;/&quot;)\n\tpublic String getSession(HttpServletRequest request) {\n\t\tHttpSession session = request.getSession();\n\t\tString result = &quot;session ID: &quot; + session.getId() + &quot;\\n\\nsession created: &quot; + session.getCreationTime() + &quot;\\n\\nsession accessed: &quot; + session.getLastAccessedTime();\n      \tSystem.out.println(request.getLocalPort());\n\t\treturn result;\n\t}\n\n}\n\n\nSpringBootApplication에 @EnableRedisHttpSession 추가\n@SpringBootApplication\n@EnableRedisHttpSession\npublic class TomcatSessionReplicationTestApplication {\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(TomcatSessionReplicationTestApplication.class, args);\n\t}\n}\n\n\nSpring boot Run configuration 설정\n\n\n  \n    tomcat-session-replication-test1\n\n    \n      JVM Arguments: -Djava.net.preferIPv4Stack=true\n      Program Arguments: –server.port=8080\n    \n  \n  \n    tomcat-session-replication-test2\n\n    \n      JVM Arguments: -Djava.net.preferIPv4Stack=true\n      Program Arguments: –server.port=8081\n    \n  \n\n\n테스트(Local)\n\n테스트 순서\n\n\n  tomcat-session-replication-test1 구동\n  localhost 접속\n  session 확인\n  tomcat-session-replication-test2 구동\n  tomcat-session-replication-test1 중지\n  localhost 접속\n  session 확인\n  redis key 확인\n\n\n테스트 진행\n\n  tomcat-session-replication-test1 구동\n\n  localhost 접속\n  session 확인\n\n  tomcat-session-replication-test2 구동\n  tomcat-session-replication-test1 중지\n\n  localhost 접속\n  session 확인\n\n  redis key 확인\n    $ docker run -it redis bash\nroot@c5aeb4bf0493:/data# redis-cli -h 192.168.0.56\t\t# host local ip\n192.168.0.56:6379&amp;gt; keys *\n1) &quot;spring:session:expirations:1622602500000&quot;\n2) &quot;spring:session:sessions:expires:063e4085-0cfc-4c49-b0c9-65a334327e13&quot;\n3) &quot;spring:session:sessions:063e4085-0cfc-4c49-b0c9-65a334327e13&quot;\n    \n  \n\n\nredis에 session 정보가 저장이 되는 것을 확인 가능\n\n\n\n\n\nsession ID를 1번 인스턴스에서 생성했음에도 2번 tomcat으로 접속시 session ID가 서로 동일한 것을 확인할 수 있다.\n\n테스트(Dev EPC)\n\nKubernetes 배포\n\n2번 테스트 항목의 Kubernetes 배포 과정과 똑같음\n\nDockerfile &quot;--spring.profiles.active=development&quot;로 수정\n\n테스트 진행(Pod fail)\n\n  접속 확인\n\n  접속 로그 확인\n\n  접속된 파드 중지\n    $ k get pod -n development\n$ k delete pod tomcatsessionclustertest-deploy-79d5796f9c-dwxv2 -n development\n    \n  \n  재접속\n\n\n\n톰캣 세션이 끊기지 않은 것을 확인할 수 있다.\n\n\n  Redis가 죽으면 세션 접속 시점에 redis에 접속하므로 계속 reconnecting해 서비스가 죽는 문제가 존재하기는 한다.\n\n\n\n\n결론\n\n1. Sticky session\n\n\n\n클라이언트가 세션을 맺은 tomcat에 대해서만 통신을 하여 일반적인 상황에서는 세션이 유지되지만\n\n정상적인 상황에서 로드밸런싱이 전혀 되지 않으며 해당 pod, node가 죽었을 때 세션이 끊기는 문제가 있다.\n\n2. Tomcat session in-memory replication(Multicast)\n\nTomcat에서 기본적으로 가이드하고 있는 Session Replication 방법\n\n\n\n\n\n로컬 환경에서는 세션이 유지되었으나 클라우드 환경에서는 다른 노드간 세션이 유지되지 않는 것으로 확인되었다.\n\nTomcat 서버 인메모리에 세션 정보를 저장하고 SimpleTCPCluster와 같은 클래스를 이용하여 리플리케이션 한다.\n\nmulticast 기능을 지원해야하지만 multicast는 대부분의 cloud 환경에서는 제공하지 않는다고 한다.\n\n1. 유지보수가 쉬울 것(관리 포인트 적을 것)\n2. 어플리케이션(서비스) 코드에 영향이 없을 것\n3. 구성하는데 추가적인 리소스가 들어가지 않을 것\n\n\n무엇보다도 세션 클러스터링 요구사항의 2, 3 번에 위배되므로 합리적인 방법이 아니다.\n\n3. Tomcat session persistence replication\n\nredis, dynamoDB와 같은 DB에 세션 정보를 저장하고 각 서버로 레플리케이션 하는 방법\n\n\n\n\n\n다른 노드간 세션이 유지되는 것을 확인할 수 있었다.\n\nsession이 in-memory가 아닌 별도의 저장소에 저장되어 사용되는 장점이 있으며 코드 수정이 거의 없다는 장점이 있다.\n\n\n  redis 라이브러리 때문에 의존성이 아예 없을 수는 없다.\n\n\n따라서 가장 합리적인 Session clustering / Session replication이라고 할 수 있다.\n\n\n\nReference\n\n\n  Clustering/Session Replication How-To\n  https://stackoverflow.com/a/61234801/15263734\n\n"
} ,
  
  {
    "title"    : "SonarQube 설정 및 트러블슈팅",
    "category" : "",
    "tags"     : " Sonarqube, Jenkins, Webhook, Ingress, Kubernetes, GitLab",
    "url"      : "/2021/05/10/sonarqube-config.html",
    "date"     : "May 10, 2021",
    "excerpt"  : "SonarQube 설정 및 트러블슈팅\n\n\n\nSonarQube Documentation\n\n본 문서에서는 Sonarqube Jenkins 연동, GitLab 연동, Ingress 인증서 설정, 웹훅 설정 등 내용을 모아 정리하였으며\n\n설정시 오류를 해결했던 내역을(트러블슈팅) 정리하였다.\n\n\n\nSonarQube 설정\n\nSonarQube Ingress SSL 인증서 적용\n\n1. k8s Secret 생성\n\n\n  k8s secret 생성은 Kube...",
  "content"  : "SonarQube 설정 및 트러블슈팅\n\n\n\nSonarQube Documentation\n\n본 문서에서는 Sonarqube Jenkins 연동, GitLab 연동, Ingress 인증서 설정, 웹훅 설정 등 내용을 모아 정리하였으며\n\n설정시 오류를 해결했던 내역을(트러블슈팅) 정리하였다.\n\n\n\nSonarQube 설정\n\nSonarQube Ingress SSL 인증서 적용\n\n1. k8s Secret 생성\n\n\n  k8s secret 생성은 Kubernetes Nginx Ingress 적용 참고\n\n\n2. Ingress 리소스 설정\n\n$ vim sonarqube-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\n metadata:\n   name: sonarqube\n   namespace: default\n   annotations:\n     kubernetes.io/ingress.class: &quot;nginx&quot;\n     nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;\n     nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot;\n     nginx.ingress.kubernetes.io/proxy-body-size: &quot;20M&quot;\t\t// 20M 초과시 HTTP 413 방지하기 위하여\n spec:\n   rules:\n   - host: sonarqube.yourdomain.com # 추가\n     http:\t# -(하이픈) 제거\n       paths:\n       - pathType: Prefix\n         path: &quot;/sonarqube&quot;\n         backend:\n           service:\n             name: sonarqube-sonarqube\n             port:\n               number: 9000\n   tls:\t# tls 이하 부분 모두 추가\n   - hosts:\n     - sonarqube.yourdomain.com # 등록한 도메인명 입력\n     secretName: secret-tls # k8s secret 리소스명 입력\n\n$ kubectl apply -f sonarqube-ingress.yaml\n\n\n\n  Nginx Ingress Controller 사용하였다.\nIngress HTTP, HTTPS(/sonarqube) -&amp;gt; sonarqube 서비스(9000)\n\n  \n    tls 설정에 도메인 입력시 IP 주소로는 Ingress 동작하지 않음\n  \n\n\n3. 인증서 적용 확인\n\n\n\n도메인 접속 및 확인\n\n\n  Jenkins의 SonarQube Server 설정 값에 기존의 IP주소로 되어있는 경우 도메인 주소로 변경 필요(http)\n\n\n\nJenkins Webhook 설정\n\nSonarQube가 검사를 끝낸 뒤 성공/실패 여부를 전달하기 위하여 SonarQube에 Jenkins로의 Webhook이 설정되어 있어야함\n\n\n\n1. SonarQube 대시보드 로그인\n\n2. Administration &amp;gt; Configuration &amp;gt; Webhooks\n\n3. Create\n\n\n  Name: 원하는 이름(jenkins-webhook)\n  URL: jenkins 주소 (http 사용할 것)\n  Secret: (선택)\n\n\n4. 빌드시 웹훅 동작 여부를 이 페이지에서 실시간으로 확인 가능\n\n\nJenkins 파이프라인 - JavaScript, TypeScript 연동\n\nJavaScript는 기본적으로 브라우저 위에서 실행되는 언어이기 때문에 이를 실행할 수 있는 프레임워크인 NodeJS가 젠킨스에 필요하다.\n\n1. NodeJS 플러그인 설치\n\n\n  Jenkins 관리 &amp;gt; 플러그인 관리 &amp;gt; 설치가능 &amp;gt; nodejs 검색\n  재시작 없이 설치하기(업데이트, 디펜던시 추가 등으로 재시작 필요시 재시작 후 설치하기)\n\n\n\n\n2. NodeJS Tools 설정\n\n\n  Jenkins 관리 &amp;gt; Global Tool Configuration\n  NodeJS &amp;gt; NodeJS installations…\n  nodejs installer 정보 입력\n    \n      Name: 원하는 이름(nodejs)\n      Install automatically: 체크\n      Install from nodejs.org\n      Version: NodeJS 14.17.0 (원하는 버전 but latest LTS 버전인 14버전 권장)\n    \n  \n  저장\n\n\n3. 적용할 pipeline에 nodejs tool 환경 추가\n\nnode (&#39;jenkins-slave&#39;){\n    ...\n    env.NODEJS_HOME = &quot;${tool &#39;nodejs&#39;}&quot;\t// 2-3 항목에서 입력한 nodejs installer 이름\n    env.PATH=&quot;${env.NODEJS_HOME}/bin:${env.PATH}&quot;\n    sh &#39;npm --version&#39;\n    ...\n\n위와같이 설정해주면 jenkins-slave에서 node, npm 명령어 사용 가능\n\n4. sonar property 추가\n\nsonarqube {\n       properties {\n                property &quot;sonar.sources&quot;, &quot;src/main&quot;\n                property &quot;sonar.tests&quot;, &quot;src/test&quot;\n       }\n}\n\n\n  위 코드는 gradle 프로젝트 build.gradle 파일 기준\n\n\n소나큐브 검증시 기본적으로 java 경로만 잡는 경우\n\nsonar.sources = src/main, sonar.tests = src/test 등으로 경로를 지정해주어야 JS 코드도 검사 가능\n\n\nSonarQube for Maven 설정\n\nMaven의 경우 설치형이기 때문에 별도로 dependency 설정이 필요하지 않다.\n\nsonar-maven-plugin을 저장소에서 가져와서 바로 실행한다.\n\n    stage(&quot;SonarQube analysis&quot;) {\n        withSonarQubeEnv(&#39;sonarqube server&#39;) {\n            sh &#39;mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.9.0.2155:sonar&#39;\n        }\n    }\n\n    stage(&quot;Quality Gate&quot;){\n        timeout(time: 1, unit: &#39;HOURS&#39;) {\n            def qg = waitForQualityGate()\n            if (qg.status != &#39;OK&#39;) {\n                error &quot;Pipeline aborted due to quality gate failure: ${qg.status}&quot;\n            }\n        }\n    }\n\n\n\nSonarQube for Gradle 설정\n\nGradle 프로젝트의 경우 보통 gradlew의 wrapper 형태로 제공되는 경우가 대부분이다.\n\n따라서 sonarqube 플러그인을 프로젝트의 build.gradle dependency에 추가해주어야한다.\n\n아래 부분 프로젝트의 build.gradle에 추가\n\n# plugin 부분 추가\n\tplugin {\n    \tid &#39;org.sonarqube&#39; version &#39;3.0&#39;\n    }\n# 또는\n    buildscript {\n        repositories {\n            jcenter()\n        }\n        dependencies {\n            classpath(&quot;org.sonarsource.scanner.gradle:sonarqube-gradle-plugin:3.0&quot;)\n        }\n    }\n    \n\tapply plugin: &#39;org.sonarqube&#39;\n\n\n# repository 추가\n\trepositories {   \t\n\t\tjcenter()\n\t}\n    \n# sonarqube 프로퍼티 추가\n    sonarqube {\n\t\tproperties {\n\t\t\tproperty &quot;sonar.sources&quot;, &quot;src/main&quot;\n\t\t}\n\t}\n    \n# multi module project의 경우\n# 아래와 같이 검사할 모듈명 명시 후 안에 적용\nproject(&quot;:project-web&quot;) {\n\tapply plugin: &#39;org.sonarqube&#39;\n\n\tsonarqube {\n\t\tproperties {\n\t\t\tproperty &quot;sonar.sources&quot;, &quot;src/main&quot;\n\t\t}\n\t}\n}\n# 또는 전체 적용시\nsubprojects {\n\tapply plugin: &#39;org.sonarqube&#39;\n\n\tsonarqube {\n\t\tproperties {\n\t\t\tproperty &quot;sonar.sources&quot;, &quot;src/main&quot;\n\t\t}\n\t}\n}\n\n\n\n빌드시 sonarqube gradle plugin 다운로드 및 검사 확인\n\n\nSonarQube Quality Profiles 적용\n\n기본적으로 각 언어별 Sonar way라는 Default Profile이 적용되어 있음\n\n\n\n\n  sonarqube 대시보드 로그인\n  (선택) Quality Profiles &amp;gt; Create 또는 Java Sonar way 프로필 설정 &amp;gt; Copy\n    \n      Name: 원하는 이름\n      Language: 원하는 언어\n      Parent: none\n    \n  \n  새로 생성한 Profile 설정 &amp;gt; Set as Default\n  설정 &amp;gt; Activate More Rules에서 추가로 적용할 Rule 탐색\n  Activate 클릭하여 적용\n\n\n\nSonarQube GitLab 계정/그룹 연동\n\nSonarQube 로그인시 GitLab OAuth2를 사용하여 gitlab 계정 연동을 할 수 있다.\n\n1. GitLab 사이드\n\n  GitLab OAuth2 Application 등록\n    \n      Admin Area &amp;gt; Applications &amp;gt; New application\n      New application 입력\n        \n          Name: 원하는 어플리케이션 이름 입력(GitLab SonarQube)\n          Redirect URI: ${sonarqube-uri}/oauth2/callback/gitlab 입력 (public URL이어야한다.)\n          Trusted: 현재 등록하는 어플리케이션을 신뢰할 것인지? 체크\n          Confidential: Client Secret을 암호화통신할 것인지? 체크\n          Scopes: api 체크(gitlab oauth2 api 사용권한)\n        \n      \n      Submit 후 앱 정보 확인\n      Application ID, Secret 복사\n    \n  \n\n\n2. SonarQube 사이드\n\n\n\n\n  Server Base URL 설정\n    \n      Administration &amp;gt; Configuration &amp;gt; General Settings\n      Server base URL 입력\n        \n          외부 연동시 기본적으로 public 통신이기 때문에 public url 입력\n            \n              https://sonarqube.yourdomain.com/sonarqube\n            \n          \n        \n      \n      Save\n    \n  \n  ALM Integration 설정\n    \n      Administration &amp;gt; Configuration &amp;gt; ALM Integrations &amp;gt; GitLab\n      GitLab Authentication 항목 입력\n        \n          Enabled: true\n          GitLab URL: public gitlab URL 입력(https://gitlab.yourdomain.com)\n          Application ID, Secret: 1-4에서 복사한 항목 각각 입력\n          Allow users to sign-up: gitlab oauth2로 처음 로그인하는 사용자를 sonarqube에 등록할 것인지? 체크\n          Synchronize user groups: 소나큐브에 gitlab 그룹명과 일치하는 그룹이 생성되어 있다면 유저를 자동으로 등록한다. 체크\n        \n      \n    \n  \n  \n    Java caCerts 인증서 설정(Kubernetes TLS Secret)\n\n    \n      Java에서는 https 통신시 기본적으로 java keystore에 인증서를 요구한다.위에서 public 경로를 http로 설정하였으면 상관 없으나 https로 설정한 경우 진행\n\n      현재 Java 기반 서비스들은 Kubernetes 환경에 떠있으므로 해당 환경으로 진행\n      \n        기존 k8s에 kubernetes.io/tls 타입으로 떠있는 tls 타입 시크릿은 사용불가하므로 opaque 타입(기본타입) secret을 생성한다.\n        sonarqube-secret.yaml 생성\n           \t$ vim sonarqube-secret.yaml\n          \n          apiVersion: v1\nkind: Secret\nmetadata:\n  name: sonarqube-secret\ndata:\n  cert-1.crt: MIIFIzCC... # .crt(X.509 포맷)의 인증서값\n          \n          $ kubectl apply -f sonarqube-secret.yaml\n          \n        \n        helm values.yaml 수정\n          $ vim values.yaml\n          \n          ...\n caCerts:\n   image: adoptopenjdk/openjdk11:alpine\n   enabled: true\n   secret: sonarqube-secret\n...\n          \n          # helm upgrade\n$ helm upgrade sonarqube oteemocharts/sonarqube -f values.yaml\n          \n        \n      \n    \n  \n  \n    테스트\n\n    \n      로그아웃 후 메인페이지에 들어가면 아래 이미지와 같이 Log in with GitLab 버튼 생성 확인\n \n      버튼 누르면 GitLab에 로그인되어 있는 경우 자동으로 소나큐브 사용자 생성 및 연동이 완료된다.\n      gitlab 그룹명과 일치하는 그룹을 미리 생성해 놓으면 로그인시 해당 유저 그룹 자동 연동\n    \n  \n\n\n\nSonarQube 배지\n\n\n\n각 프로젝트 대시보드 &amp;gt; 우상단 Project Information\n\nmarkdown 형태로 실시간 정보 배지를 embed 가능\n\n대신 해당 배지를 사용하려면 SonarQube 프로젝트가 Public으로 설정되어야하며 이는 소스 취약점을 외부에서 누구나 볼 수 있다는 것을 의미한다.\n\n\nTroubleshooting\n\n1. 분석 권한 없음\n\n[ERROR] Error during SonarScanner execution\n[ERROR] You&#39;re not authorized to run analysis. Please contact the project administrator.\n\n\n빌드 후 검증 수행시 위와 같은 오류가 날 때\n\nSonarQube 플러그인 설치 및 실행은 완료되었으나 Jenkins - SonarQube 연동 계정이 Analysis 실행 권한이 없는 경우이다.\n\n\n\n\n  SonarQube 대시보드 로그인(관리자 계정 필요)\n  Administration &amp;gt; Security &amp;gt; Global Permissions\n  연동된 유저가 속한 그룹 혹은 유저 자체에 Execute Analysis 권한 체크\n\n\n2. class 파일 없음\n\n[ERROR] Failed to execute goal org.sonarsource.scanner.maven:sonar-maven-plugin:3.7.0.1746:sonar (default-cli) on project sample-project: Your project contains .java files, please provide compiled classes with sonar.java.binaries property, or exclude them from the analysis with sonar.exclusions property. -&amp;gt; [Help 1]\n\n\npipeline 또는 job 실행 순서에 빌드 이전에 분석을 시도하고 있는지 체크할 것.\n\nJava의 경우 sonarqube는 신뢰도를 높이기 위하여 .java 파일만으로 코드 분석을 하지 않고 .java와 .class 파일을 함께 분석한다고 한다.\n\n\n  https://docs.sonarqube.org/latest/analysis/languages/java/#header-2 참조\n\n\n따라서 컴파일된 .class 파일이 있어야하므로 maven 또는 gradle 플러그인을 사용하여야함.\n\n만약 없을 경우, 수동으로 컴파일하여 .class 파일을 넣어줘야함.\n\n3. 소스 분석시 HTTP 413 REQUEST ENTITY TOO LARGE\n\n소스 분석시 HTTP 413 REQUEST ENTITY TOO LARGE가 나오는 경우\n\n검증할 소스가 sonarqube의 max body size(기본값: 20m)를 초과하여 나오는 경우로\n\n아래와 같이 sonarqube-ingress.yaml에 proxy body size 설정\n\n...\n   apiVersion: networking.k8s.io/v1\n   kind: Ingress\n    metadata:\n      name: sonarqube\n      namespace: default\n      annotations:\n        kubernetes.io/ingress.class: &quot;nginx&quot;\n        nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;\n        nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot;\n        nginx.ingress.kubernetes.io/proxy-body-size: &quot;20M&quot;\t\t// 20M 초과시 HTTP 413 방지하기 위하여\n...\n\n\n\n\nReference\n\n\n  SonarScanner for Jenkins\n  SonarQube Scanner for Jenkins\n  SonarQube GitLab Integration\n\n"
} ,
  
  {
    "title"    : "SonarQube 설치 및 Jenkins pipeline 연동하기",
    "category" : "",
    "tags"     : " Sonarqube, Jenkins, Helm, Docker, Docker-Compose",
    "url"      : "/2021/05/07/sonarqube-jenkins.html",
    "date"     : "May 7, 2021",
    "excerpt"  : "SonarQube 설치 및 Jenkins pipeline 연동하기\n\n\n\nSonarQube Documentation\n\n본 문서에서는 Sonarqube 설치 및 Jenkins 파이프라인 연동한 내용을 정리하였다.\n\n\nSonarQube란\n\n\n\n소스 품질 관리를 위한 자동화된 정적 코드 검증/분석 툴\n\n  버그 탐색\n  취약점 탐색\n  코드 냄새(Code Smell) 탐색\n  보안 핫스팟 탐색\n\n\n안전(Safe) 하고 깨끗한(Clean) 코드를 ...",
  "content"  : "SonarQube 설치 및 Jenkins pipeline 연동하기\n\n\n\nSonarQube Documentation\n\n본 문서에서는 Sonarqube 설치 및 Jenkins 파이프라인 연동한 내용을 정리하였다.\n\n\nSonarQube란\n\n\n\n소스 품질 관리를 위한 자동화된 정적 코드 검증/분석 툴\n\n  버그 탐색\n  취약점 탐색\n  코드 냄새(Code Smell) 탐색\n  보안 핫스팟 탐색\n\n\n안전(Safe) 하고 깨끗한(Clean) 코드를 유지하게끔 도움을 줌(소스코드 품질 관리)\n\n\n  커밋/머지(SCM)\n  체크아웃, 빌드, 테스트(CI/CD)\n  분석/검증(SonarQube)\n\n\n\n  Sonarlint: 코딩/컴파일시 소스 품질 관리를 위한 IDE 플러그인\n\n\n\nSonarQube 설치\n\n두 가지 방법으로 설치해 보았다.\n\n\n  Using Helm Chart\n    \n      k8s 위에서 관리하는 자원의 형태로 사용시\n    \n  \n  Using Docker Compose\n    \n      호스트 위에서 컨테이너형으로 간단하게 사용시\n    \n  \n\n\nUsing Helm Chart\n\n\n  k8s helm을 이용하여 설치하는 방법\n\n\n1. Helm 설치\n\n# Ubuntu\n$ curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -\n$ sudo apt-get install apt-transport-https --yes\n$ echo &quot;deb https://baltocdn.com/helm/stable/debian/ all main&quot; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\n$ sudo apt-get update\n$ sudo apt-get install helm\n\n\n2. Helm Chart values.yaml 가져오기\n\n   # helm 저장소 추가\n   $ helm repo add oteemocharts https://oteemo.github.io/charts\n   $ wget https://raw.githubusercontent.com/Oteemo/charts/master/charts/sonarqube/values.yaml\n\n\n\n  현시점 기준 8.5.1-community\n\n\n3. ingress 리소스 설정\n\n   $ vim sonarqube-ingress.yaml\n\n   apiVersion: networking.k8s.io/v1\n   kind: Ingress\n    metadata:\n      name: sonarqube\n      namespace: default\n      annotations:\n        kubernetes.io/ingress.class: &quot;nginx&quot;\n        nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;\n        nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot;\n    spec:\n      rules:\n      - http:\n          paths:\n          - pathType: Prefix\n            path: &quot;/sonarqube&quot;\n            backend:\n              service:\n                name: sonarqube-sonarqube\n                port:\n                  number: 9000\n\n   $ kubectl apply -f sonarqube-ingress.yaml\n\n\n  /sonarqube context로 설정\n\n  \n    Ingress HTTP(/sonarqube) -&amp;gt; sonarqube 서비스(9000)\n  \n\n\n4. PersistenceVolume, PersistenceVolumeClaim 설정\n\n   $ vim sonarqube-storage.yaml\n\n    kind: PersistentVolume\n    apiVersion: v1\n    metadata:\n      name: sonarqube-pv-volume\n    spec:\n      storageClassName: manual\n      capacity:\n        storage: 25Gi\n      accessModes:\n        - ReadWriteMany\n      hostPath:\n        path: &quot;/nfs_nas/volumes/sonarqube/data&quot;\n    ---\n    kind: PersistentVolumeClaim\n    apiVersion: v1\n    metadata:\n      name: sonarqube-pv-claim\n    spec:\n      storageClassName: manual\n      accessModes:\n        - ReadWriteMany\n      resources:\n        requests:\n          storage: 25Gi\n    ---\n    kind: PersistentVolume\n    apiVersion: v1\n    metadata:\n      name: sonarqube-db-pv-volume\n    spec:\n      storageClassName: manual\n      capacity:\n        storage: 25Gi\n      accessModes:\n        - ReadWriteMany\n      hostPath:\n        path: &quot;/nfs_nas/volumes/sonarqube/postgres&quot;\n    ---\n    kind: PersistentVolumeClaim\n    apiVersion: v1\n    metadata:\n      name: sonarqube-db-pv-claim\n    spec:\n      storageClassName: manual\n      accessModes:\n        - ReadWriteMany\n      resources:\n        requests:\n          storage: 25G\n\n   $ kubectl apply -f sonarqube-storage.yaml\n\n\nsonarqube에서 사용할 volume을 지정해주기 위하여 설정\n\n용량과 hostPath는 사용 환경에 맞추어 변경할 것\n\n5. values.yaml 수정\n\n아래 해당하는 부분 모두 수정\n\n    ...\n    readinessProbe:\n      initialDelaySeconds: 60\n      periodSeconds: 30\n      failureThreshold: 6\n      sonarWebContext: /sonarqube/ # 변경(끝에 &#39;/&#39; 포함)\n    livenessProbe:\n      initialDelaySeconds: 60\n      periodSeconds: 30\n      sonarWebContext: /sonarqube/ # 변경(끝에 &#39;/&#39; 포함)\n    ...\n    # 사용할 플러그인 추가(아래 기본 플러그인은 이미 있으므로 충돌남 외부 플러그인만 적용할 것, 아래는 잘못된 예시)\n    plugins:\n      install: [\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-java-plugin/sonar-java-plugin-6.9.0.23563.jar&quot;,\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-javascript-plugin/sonar-javascript-plugin-7.4.3.15529.jar&quot;,\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-xml-plugin/sonar-xml-plugin-2.2.0.2973.jar&quot;,\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-css-plugin/sonar-css-plugin-1.4.2.2002.jar&quot;,\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-html-plugin/sonar-html-plugin-3.4.0.2754.jar&quot;,\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-typescript-plugin/sonar-typescript-plugin-2.1.0.4359.jar&quot;\n        ]\n      lib: []\n    ...\n    env:\n      - name: SONAR_WEB_CONTEXT # 추가\n        value: /sonarqube\n    ...\n    persistence:\n      enabled: true\n      annotations: {}\n      existingClaim: sonarqube-pv-claim # 추가\n    ...\n    postgresql:\n      enabled: true\n      ...\n      persistence:\n        enabled: true\n        existingClaim: sonarqube-db-pv-claim # 추가\n    ...\n\n\n6. helm install\n   # helm v3\n   # helm install [name] [chart] [-f:value]\n   $ helm install sonarqube oteemocharts/sonarqube -f values.yaml\n   \n   # helm v2\n   # helm install [--name:name] [chart] [-f:values]\n   $ helm install --name sonarqube oteemocharts/sonarqube -f values.yaml\n\n\n참고\n\nhelm uninstall\n\n   # helm v3\n   # helm uninstall [name]\n   $ helm uninstall sonarqube\n   \n   # helm v2\n   # helm del [--purge] [name]\n   $ helm del --purge sonarqube\n\n\nhelm upgrade\n\n   # helm upgrade [release] [chart] [-f:values]\n   $ helm upgrade sonarqube oteemocharts/sonarqube -f values.yaml\n\n\n7. kubernetes pods 확인 및 대시보드 접속\n   $ kubectl get pods -A | grep sonarqube\n   $ kubectl logs [pod name]\n   $ curl localhost:9000\n\n\n  http://external-ip:port/sonarqube 접속 확인초기 계정: admin/admin\n\n  접속 후 admin 비밀번호 변경 또는 비활성화 할 것\n\n\nUsing Docker-Compose\n\n1. docker-compose.yaml 작성\n    version: &quot;3.1&quot;\n    services:\n      sonarqube:\n        image: sonarqube:latest\n        restart: always\n        depends_on:\n          - sonarqube-db\n        container_name: sonarqube\n        ports:\n          - &quot;9000:9000&quot;\n        networks:\n          - sonarnet\n        environment:\n          TZ: Asia/Seoul\n          SONAR_HOME: /opt/sonarqube\n          SONAR_JDBC_USERNAME: sonar\n          SONAR_JDBC_PASSWORD: sonar\n          SONAR_JDBC_URL: jdbc:postgresql://sonarqube-db:5432/sonar\n        ulimits:\n          nofile:\n            soft: 65536\n            hard: 65536\n          memlock:\n            soft: -1\n            hard: -1\n        volumes:\n          - /nfs_nas/volumes/sonarqube/data:/opt/sonarqube/data\n          - /nfs_nas/volumes/sonarqube/extensions:/opt/sonarqube/extensions\n          - /nfs_nas/volumes/sonarqube/logs:/opt/sonarqube/logs\n          - /nfs_nas/volumes/sonarqube/temp:/opt/sonarqube/temp\n\n      sonarqube-db:\n        image: postgres\n        container_name: sonarqube-db\n        networks:\n          - sonarnet\n        environment:\n          TZ: Asia/Seoul\n          POSTGRES_USER: sonar\n          POSTGRES_PASSWORD: sonar\n        volumes:\n          - /nfs_nas/volumes/sonarqube/postgres:/var/lib/postgresqli/data\n\n\n2. docker-compose 실행 및 접속 확인\n   $ docker-compose up -d\n   $ docker-compose ps\n   $ curl localhost:9000\n\n\n\n  외부 접속의 경우 포트포워딩 필요\n\n  \n    이 부분은 각 인프라의 환경별로 상이하기 때문에 별도로 기술하지는 않는다.\n  \n\n\n\n\nJenkins 연동\n\n1. SonarQube 사이드\n\n\n  SonarQube Web 접속\n  Administration &amp;gt; Security &amp;gt; Users\n  (선택) admin 계정 deactivate 및 새 관리자 계정 설정\n    \n      admin 계정의 설정 &amp;gt; Deactivate\n      Create User\n      생성된 계정의 Groups 리스트 &amp;gt; Unselected &amp;gt; sonar-administrators 체크\n    \n  \n  위의 관리자 계정에서 Token 메뉴 진입\n  Generate Tokens &amp;gt; Token Name(jenkins-token) 입력 후 Generate\n    \n      SonarQube 입장에서는 Jenkins에서 사용할 토큰이므로 jenkins-token이라고 이름 지음\n    \n  \n\n\n\n\n\n  생성된 토큰 복사\n    \n      토큰값은 생성시에만 볼 수 있으므로 필히 복사 및 다른 곳에 저장할 것\n    \n  \n\n\n2. Jenkins 사이드\n\n\n  SonarQube 플러그인 설치\n    \n      Jenkins 관리 &amp;gt; 플러그인 관리\n      SonarQube 검색 &amp;gt; SonarQube Scanner for Jenkins 설치\n    \n  \n  SonarQube 토큰 등록\n    \n      Jenkins 관리 &amp;gt; Manage Credentials\n      아래 Stores scoped to Jenkins의 Domains 항목 클릭\n      Add Credentials\n        \n          Kind: Secret text\n          Scope: Global\n          Secret: 위 SonarQube에서 생성했던 토큰 붙여넣기\n          ID: 원하는 ID(sonarqube-token) 입력\n        \n      \n    \n  \n  SonarQube 서버 연동\n    \n      Jenkins 관리 &amp;gt; 시스템 설정 &amp;gt; SonarQube servers\n      Environment variables 체크\n        \n          SonarQube servers 항목에 입력하는 config 정보들을 Jenkins에서 환경변수로 사용할 것인지 확인하는 항목\n        \n      \n    \n\n    \n  \n  SonarQube Installations 항목 입력\n    \n      Name: 원하는 서버 이름 입력(ex: sonarqube server)\n      Server URL: SonarQube 서버 주소 입력\n      Server authentication token: 2번 항목에서 생성한 토큰 선택\n    \n  \n\n\n3. Jenkins Pipeline\n\n테스트용 메이븐 프로젝트 기준으로 작성\n\nGradle 프로젝트 또는 다른 설정을 보려면 SonarQube for Gradle을 참고\n\nSonarQube와 연동할 파이프라인 스크립트에 아래와 같이 Stage 추가\n\n    stage(&quot;SonarQube analysis&quot;) {\n        withSonarQubeEnv(&#39;sonarqube server&#39;) {\n            sh &#39;mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.9.0.2155:sonar&#39;\n        }\n    }\n\n    stage(&quot;Quality Gate&quot;){\n        timeout(time: 1, unit: &#39;HOURS&#39;) {\n            def qg = waitForQualityGate()\n            if (qg.status != &#39;OK&#39;) {\n                error &quot;Pipeline aborted due to quality gate failure: ${qg.status}&quot;\n            }\n        }\n    }\n\n\nStage 설명\n\n  \n    &quot;SonarQube analysis&quot;: mvn 빌드 파일 기준 SonarQube 검사 진행\n  \n  \n    &quot;Quality Gate&quot;: 검사한 코드의 품질을 통해 통과/미통과 판별(Quality Gate 프로파일 설정은 SonarQube에서 진행)\n  \n\n\n\n  withSonarEnv()에는 위의 설정에서 Jenkins에서 설정한 SonarQube Server 이름으로 설정해야함Qulaity Gate의 경우 SonarQube에서 Jenkins로 Webhook이 설정되어 있어야함 (Webhook 설정 참고)\n\n\n\n\nJenkins 빌드시 각 Stage 정상 통과 및 SonarQube Webhook 동작 여부 확인\n\n\n  연동 성공시 Build History에 SonarQube 대시보드로 이동할 수 있는 아이콘이 나옴\n\n\n\n\nReference\n\n\n  SonarScanner for Jenkins\n  SonarQube Scanner for Jenkins\n\n"
} ,
  
  {
    "title"    : "Kubernetes Nginx Ingress 적용기",
    "category" : "",
    "tags"     : " Kubernetes, K8S, Ingress, Ingress Controller, Certificate, TLS, SSL, Nginx, Security",
    "url"      : "/2021/04/03/kubernetes-ingress.html",
    "date"     : "April 3, 2021",
    "excerpt"  : "Kubernetes Nginx Ingress 적용기\n\n\n\nKubernetes Ingress\n\n본 문서는 온프레미스 환경에서의 Ingress, SSL 인증서 적용 내용을 정리하였다.\n\nnginx ingress controller를 이용하여 jenkins 서비스에 연결하는 과정을 정리하였다.\n\n\n선행 사항\n\n  Let’s Encrypt 인증서 발급\n  Kubernetes 환경 구축\n  Ingress controller 설치\n\n\n\nIngress...",
  "content"  : "Kubernetes Nginx Ingress 적용기\n\n\n\nKubernetes Ingress\n\n본 문서는 온프레미스 환경에서의 Ingress, SSL 인증서 적용 내용을 정리하였다.\n\nnginx ingress controller를 이용하여 jenkins 서비스에 연결하는 과정을 정리하였다.\n\n\n선행 사항\n\n  Let’s Encrypt 인증서 발급\n  Kubernetes 환경 구축\n  Ingress controller 설치\n\n\n\nIngress 개요\n\nflowchart LR\n  A[User]\n  subgraph Kubernetes Cluster\n  subgraph Ingress\n  B[Service: Ingress Contoller]\n  J[Pod: Ingress Controller]\n  F[Resource: Ingress]\n  end\n  subgraph Deployment1\n  C[Service1]\n  G[Pod1]\n  end\n  subgraph Deployment2\n  D[Service2]\n  H[Pod2]\n  end\n  subgraph Deployment3\n  E[Service3]\n  I[Pod3]\n  end\n  end\n  A -.Ingress.-&amp;gt; B\n  B---J\n  C---G\n  D---H\n  E---I\n  B---F\n  B --&amp;gt; C &amp;amp; D &amp;amp; E\n\n\n외부에서 클러스터에 접근할 때 요청받는 것이 Ingress이다. 쉽게 얘기하면 일반적인 proxy, gateway라고 보면 된다.\n\n동일하게 로드밸런서, 서비스 메쉬의 역할도 수행하며 경우에 따라서는 Blue Green 배포나 Canary 배포도 가능하다.\n\n외부 → Ingress → Service → Pod\n\n내부 서비스에 SSL 인증서를 적용하는 것이 아닌 Ingress에 인증서를 적용한다.\n\n이와 같이 적용하면 서비스 종속성 없이 앞단에서 인증서 적용이 가능하므로 뒷단의 서비스가 추가되어도 쉽게 적용이 가능하다.\n\nIngress Controller와 Ingress Resource의 차이\n\n\n\nIngress Controller\n\n\n\n\n  이미지: Certified Kubernetes Administrator(CKA) with Practice Tests 발췌\n\n\n\n  80:31280/TCP, 443:32443/TCP등의 서비스와 함께 NodePort 형식으로 외부와의 연결을 담당\n  보통 nginx, haproxy 등 proxy 서버로 구현되어 있다.\n  Ingress Resource의 설정값(Ingress Rule)에 따라 클러스터 내의 Service로 연결한다.\n\n\nIngress Resource\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n...\nspec:\n  rules:\n  - host: service.yourdomain.com # 이 부분 추가 (하이픈은 맨 위에만 있어야함)\n    http:\n      paths:\n      - pathType: Prefix\n        path: &quot;/service&quot;\n        backend:\n          service:\n            name: service-name\n            port:\n              number: 8080\n\n\n\n  Ingress Controller에서 어떤 서비스로 라우팅할 것인지 규칙을 명세한 Resource이다.\n  path등의 값으로 어떤 service에 연결할 것인지에 대한 명세이다.\n  Kubernetes secret 리소스를 참조하여 tls 연결을 설정할 수 있다.\n\n\n\nIngress 적용\n\n인증서 파일 구성\n\nPEM 포맷의 RSA crt, key가 필요하다.\n\n본 문서에서는 Let’s Encrypt의 DNS 포맷으로 발급된 *.yourdomain.com 도메인 인증서를 사용한다.\n\n# yourdomain.com.crt\n-----BEGIN CERTIFICATE-----\nMII...\n-----END CERTIFICATE-----\n\n# yourdomain.com.crt\nPrivate-Key: (2048 bit)\nmodulus:\n...\n-----BEGIN RSA PRIVATE KEY-----\nMII...\n-----END RSA PRIVATE KEY-----\n\n\nKubernetes secret 생성\n\nkubernetes에서는 여러가지 시크릿 타입을 제공하는데\n\n그 중 SSL 인증서에 대한 정보를 담고 있는 Secret 타입은 kubernetes.io/tls 이다.\n\nsecret 생성하는 두 가지 방법이 존재\n\n1. secret yaml 파일 생성\n\n$ vim service-secret.yaml\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-tls\ntype: kubernetes.io/tls\ndata:\n  # the data is abbreviated in this example\n  tls.crt: |\n        MIIC2DCCAcCgAwIBAgIBATANBgkqh ...\n  tls.key: |\n        MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...\n\n\n이 경우 인증서 값은 —–BEGIN CERTIFICATE—–와 —–END CERTIFICATE—– 사이에 있는 인코딩값을 넣어야한다고 한다.\n\n\n  확인결과 .crt 안에 있는 인증서 값을 모두 포함하여야함\n\n\n$ kubectl create -f service-secret.yaml\n\n\nkubernetes secret 생성\n\n2. 직접 생성(권장)\n\n# kubectl create secret {type} {name} --cert {certPath} --key {keyPath}\n$ kubectl create secret tls secret-tls --cert ssl/yourdomain.com.crt --key ssl/yourdomain.com.key\n\n\nImperative 방식으로 생성\n\n인증서 파일을 설정값으로 물고갈 수 있어서 권장\n\n# secret 생성 확인\n$ kubectl get secret\n$ kubectl describe secret secret-tls\n\n\nIngress TLS 적용\n\n$ vim jenkins-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n...\nspec:\n  rules:\n  - host: jenkins.yourdomain.com # 이 부분 추가 (하이픈은 맨 위에만 있어야함)\n    http: \n      paths:\n      - pathType: Prefix\n        path: &quot;/jenkins&quot;\n        backend:\n          service:\n            name: jenkins\n            port:\n              number: 8080\n  # 아래 부분 추가\n  tls:\n  - hosts:\n    - jenkins.yourdomain.com\n    secretName: secret-tls\n\n\n# 적용\n$ kubectl apply -f jenkins-ingress.yaml\n\n위에서 생성한 Secret을 이용하면 Ingress에 인증서를 적용할 수 있다.\n\n인증서 적용 확인\n\n1. Ingress Controller 접속 확인\n\n# ingress-nginx 서비스 포트매핑 확인\n$ kubectl get svc -A\n\nNAMESPACE       NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx   ingress-nginx-controller             NodePort    10.104.33.237    &amp;lt;none&amp;gt;        80:31623/TCP,443:32452/TCP   29h\n\n$ curl -k -v https://10.104.33.237:443\n...\n* Connected to 10.104.33.237 (10.104.33.237) port 443 (#0)\n...\n\n\n2. 외부 도메인 접속 확인\n\n$ curl -k -v https://jenkins.yourdomain.com:32452\n...\n* Connected to jenkins.yourdomain.com (...) port 32452 (#0)\n...\n\n\n3. 인증서 검증\n\n$ openssl s_client -debug -connect https://jenkins.yourdomain.com:32452\n\n...\nsubject=CN = *.yourdomain.com\n\nissuer=C = US, O = Let&#39;s Encrypt, CN = R3\n...\n---\n\n\n4. 웹 브라우저로 접속하여 인증서 확인\n\n\n\n\nSSL Redirect 끄기\n\n위처럼 Ingress 설정에 tls를 적용하면 https 접속이 강제되므로 기존에 http에 운영중인 서비스에 영향이 있다.\n\n따라서 기존의 http 서비스는 https로 redirect 되지 않도록 설정하여야 한다.\n\n1. Ingress rule 변경\n\n$ vim jenkins-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  ...\n  namespace: default\n  # 아래 annotations 추가\n  annotations:\n    kubernetes.io/ingress.class: &quot;nginx&quot;\n    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;\n    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot;\n\n\n\n  Ingress rule의 ssl-redirect 설정은 host 주소와 port는 그대로 둔채 http 요청을 https로 리다이렉트한다.(서버에서 https redirect 응답)\n\n  예시) http://jenkins.yourdomain.com:31623/jenkins → https://jenkins.yourdomain.com:31623/jenkins\n\n\n2. ConfigMap 설정 변경\n\n# vim ~/ingress-controller/ingress-nginx/deploy/static/provider/baremetal/deploy.yaml\n\n# Source: ingress-nginx/templates/controller-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-3.23.0\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: 0.44.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\n# 아래 부분 추가\ndata:\n  hsts: &quot;false&quot;\n  ssl-redirect: &quot;false&quot;\n\n\n\n  Ingress controller의 ssl-redirect 설정은 host 주소는 그대로지만 https의 default port인 443으로 redirect한다.\n\n  예시) http://jenkins.yourdomain.com:31623/jenkins → https://jenkins.yourdomain.com/jenkins\n\n\ningress-controller의 baremetal template에 있는 ConfigMap으로 띄워져 있으므로 해당 부분 수정\n\n만약 다른 환경으로 인하여 ConfigMap이 없는 경우 새로 생성하여 적용할 것\n\n$ kubectl apply -f deploy.yaml\n\n설정 변경 적용\n\n추가로 적용기간에는 http와 https의 포트를 서로 다르게하여 둘 다 접속 가능하도록 만드는 것이 옳다.\n\n현재 http와 https를 동시 서비스 중이기 때문에 서로 다른 포트에서 접근을 한다.\n\n따라서 수동으로 Host 주소와 port를 변경해주어야 한다.\n\n3. Ingress rule 변경\n\n$ vim jenkins-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  ...\n  namespace: default\n  # 아래 annotations 추가\n  annotations:\n  \tkubernetes.io/ingress.class: &quot;nginx&quot;\n    nginx.ingress.kubernetes.io/proxy-redirect-from: http://jenkins.yourdomain.com:31623\n    nginx.ingress.kubernetes.io/proxy-redirect-to: https://jenkins.yourdomain.com:32452\n    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;\n    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot;\n\n\n$ curl -v http://jenkins.jenkins.yourdomain.com:32452\n* About to connect() to jenkins.yourdomain.com port 32452 (#0)\n*   Trying ...\n* Connected to jenkins.yourdomain.com (...) port 32425 (#0)\n&amp;gt; GET /jenkins HTTP/1.1\n&amp;gt; User-Agent: curl/7.29.0\n&amp;gt; Host: jenkins.yourdomain.com:32452\n&amp;gt; Accept: */*\n&amp;gt;\n&amp;lt; HTTP/1.1 302 Found\n&amp;lt; Date: Fri, 05 Mar 2021 04:51:59 GMT\n&amp;lt; Content-Length: 0\n&amp;lt; Connection: keep-alive\n&amp;lt; Location: https://jenkins.yourdomain.com:32452/\n\n테스트해보면 redirect가 정상적으로 되는 것을 확인할 수 있다.\n\n크롬의 경우 HSTS 기능 끄기\n\n위와 같이 설정해도 크롬과 같은 웹브라우저의 경우 HSTS 기능때문에 웹브라우저 레벨에서\n\n위에서 설정한 proxy가 아니라 호스트와 포트는 그대로인 상태로 http → https 리다이렉션이 강제된다.\n\n\n  http 접속을 한 후에 서버에서 https 응답을 주는데 http 접속 이전에 브라우저에서 https로 변경하기 때문이다.\n\n\n\n시크릿 모드로 들어가서 테스트하거나 이미 크롬에 설정된 HSTS 설정을 제거해야한다.\n\n\n  chrome://net-internals/#hsts 접속\n  Query HSTS/PKP domain에서 HSTS 설정되어 있는지 검색\n\n  Delete domain security policies에서 해당 도메인 HSTS 설정 지우기\n\n\n이후 크롬을 통해 재접속하면 https redirect가 성공적으로 되는 것을 확인할 수 있다.\n\n\n참고사항\n\n  Ingress controller 기본 인증서를 가져간다?\n\n\n* successfully set certificate verify locations:\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\n\n\n위처럼 Ingress Controller에서 기본 설정 SSL Cert를 가져간다고 나오는데 Ingress Controller에서 SSL을 설정하는 것이 아닌 Ingress의 Secret 설정에 있는 인증서 파일을 적용하는 것이기 때문에 무시해도 된다.\n\n\nTroubleshooting\n\nIngress controller 기본 인증서를 가져간다?\n\n* successfully set certificate verify locations:\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\n\n\n위처럼 Ingress Controller에서 기본 설정 SSL Cert를 가져간다고 나오는데 Ingress Controller에서 SSL을 설정하는 것이 아닌 Ingress의 Secret 설정에 있는 인증서 파일을 적용하는 것이기 때문에 무시해도 된다.\n\nKubernetes Ingress Controller Fake Certificate 인증서가 적용될 때\n\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            0a:2a:7b:52:02:51:fe:7d:ff:ad:65:ea:41:8a:95:44\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: O = Acme Co, CN = Kubernetes Ingress Controller Fake Certificate\n\n\nIngress에 tls 설정을 추가할 경우 기본적으로 해당 서비스는 https를 사용하게 되며 인증서 설정을 별도로 해주지 않은 경우 Ingress Controller fake certificate를 기본적으로 사용하게 된다.\n\n이 경우는 위에서 설정한 인증서 적용이 안 된 경우이다.\n\n1. kubernetes secret에서 cert와 key를 잘 물고 갔는지 확인\n\n2. Ingress yaml 파일에 tls 적용이 제대로 되었는지 확인\n\n\n  ssl을 적용할 경우 연동되는 서비스와 tls 설정에 host 이름을 꼭 추가해주어야한다.\n\n\n\n\nReference\n\n\n  Ingress\n  Ingress Controllers\n  Secret\n  Certified Kubernetes Administrator(CKA) with Practice Tests\n\n"
} ,
  
  {
    "title"    : "Kubernetes Dashboard 설치 및 외부접속 설정",
    "category" : "",
    "tags"     : " Kubernetes, K8S, Authorization, Authentication, RBAC, ABAC, Security",
    "url"      : "/2021/03/04/kubernetes-dashboard.html",
    "date"     : "March 4, 2021",
    "excerpt"  : "Kubernetes Dashboard 설치 및 외부접속 설정\n\n\n\nkubernetes dashboard\n\nKubernetes dashboard를 구성 후 외부접속 설정\n\n\nKubernetes dashboard 설치\n\nUsing K8S Deployment\n\n1. kubernetes dashboard template 가져오기\n\n$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2....",
  "content"  : "Kubernetes Dashboard 설치 및 외부접속 설정\n\n\n\nkubernetes dashboard\n\nKubernetes dashboard를 구성 후 외부접속 설정\n\n\nKubernetes dashboard 설치\n\nUsing K8S Deployment\n\n1. kubernetes dashboard template 가져오기\n\n$ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.2.0/aio/deploy/recommended.yaml\n$ mv recommended.yaml kubernetes-dashboard.yaml # 이름 변경(편의상)\n\n\n2. ServiceAccount 생성 및 RBAC 설정\n\n$ vim kubernetes-dashboard-rbac.yaml\n\n\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: admin-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: admin-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-admin\nsubjects:\n- kind: ServiceAccount\n  name: admin-user\n  namespace: kubernetes-dashboard\n\n\n위 설정은 현재 k8s에 미리 정의되어 있는 cluster-admin 역할을 바인딩하므로 해당 SA의 토큰으로 접속시 cluster의 모든 권한을 획득한다.\n\n모니터링 SA의 경우 아래와 같이 신규 Role을 생성하여 권한을 지정하여 바인딩한다.\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: monitoring-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-monitoring\nrules:\n- apiGroups: [&quot;*&quot;]\n  resources: [&quot;namespaces&quot;, &quot;deployments&quot;, &quot;pods&quot;]\n  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: monitoring-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-monitoring\nsubjects:\n- kind: ServiceAccount\n  name: monitoring-user\n  namespace: kubernetes-dashboard\n\n\n\n  네임스페이스, 디플로이먼트, 파드 3가지 리소스에 대해서 읽기 권한만 부여\n\n\n3. 로컬 접속 테스트\n\n# 로컬 k8s 프록시 실행\n$ kubectl proxy\nStarting to serve on 127.0.0.1:8001\n# 접속 확인\n$ curl -v http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n\n\n\n  참고: kubernetes-dashboard.yaml의 모든 리소스는 모두 kubernetes-dashboard라는 namespace안에 존재한다.\n\n\n4. 베어러 토큰 발급\n\n# 토큰 발급\n\n$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/{위에서 생성한 SA명} -o jsonpath=&quot;{.secrets[0].name}&quot;) -o go-template=&quot;{{.data.token | base64decode}}&quot; &amp;gt;&amp;gt; token\n\n\n# 토큰 확인\n$ cat token\n\n\nKubernetes dashboard는 기본적으로 API형태로의 로컬 접근만 허용하고 있어 외부 접속 설정이 따로 필요하다.\n\n\nKubernetes Dashboard 외부 접속 설정\n\n\n  참고: Kubernetes Dashboard는 기본적으로 https 연결을 하도록 설정되어 있으며 로컬에서의 접근만 허용한다\n\n\nKubernetes를 외부에 노출시키는 방법은 크게 4가지가 있다.\n\n1. Proxy 방식\n\n2. NodePort 방식\n\n3. API Server 방식\n\n4. Ingress 방식\n\n본 문서에서는 4번 Ingress 방식을 사용한다.\n\n1. Proxy 방식\n\n\n  kubectl proxy를 이용하여 포트와 주소를 지정한 후 Host 머신에 띄우는 방식\n\n\n$ kubectl proxy --port=9090 --address=10.109.190.106 --accept-hosts=&#39;^*$&#39;\n\n$ curl -k -v http://10.109.190.106:9090/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/\n\n\nHost가 외부에 노출되어 있는 상황이라면 해당 IP를 이용하여 외부에서도 접근이 가능하다.\n\n단점\n\n\n  Host가 외부와 직접 연결되는 환경에서만 사용할 수 있다.\n  대시보드가 localhost에서 띄워져야만 로그인 기능을 사용할 수 있다.\n  외부에서 무차별 접속이 가능하며 모든 권한을 갖는다.\n  방화벽 등 보안 설정이 추가로 필요하다.\n\n\n2. NodePort 방식\n\n\n  NodePort 방식의 서비스를 사용하여 포트를 직접 외부에 노출시키는 방식\n\n\n$ vim kubernetes-dashboard.yaml\n\n\n\n---\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  type: NodePort\n  ports:\n    - port: 9090\n      targetPort: 9090\n  selector:\n    k8s-app: kubernetes-dashboard\n\n---\n\n\nService type을 NodePort로 변경\n\n$ kubectl get svc -n kubernetes-dashboard\nkubernetes-dashboard\tNodePort\t10.109.190.106\t&amp;lt;none&amp;gt;\t443:31384/TCP\t28s\n\n# master ip 확인\n$ kubectl cluster-info\n\n\nNodePort 설정 확인\n\n31384 포트로 외부에 오픈되어 있는 것을 확인할 수 있다.\n\nhttps://master-ip:31384로 접속 확인\n\n단점\n\n\n  Host가 외부와 직접 연결되는 환경에서만 사용할 수 있다.\n  대시보드는 https 연결을 기본으로 하므로 인증서가 없어 접속 불가\n\n\n3. API Server 방식\n\n\n  위의 Proxy 방식과 비슷하나 API 서버가 외부에 노출되어 있어 API 서버에 직접 접속하는 방식\n\n\n$ curl -k -v https://&amp;lt;master-ip&amp;gt;:&amp;lt;apiserver-port&amp;gt;/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/\n\n\n이 경우 kubernetes dashboard 인증서를 브라우저 자체에 따로 추가해주어야 하는 추가 작업이 존재한다. (본 문서에서는 다루지 않는다.)\n\n게다가 kube-apiserver를 외부에 직접 노출하는것은 권장하지 않는다.\n\n단점\n\n\n  Host가 외부와 직접 연결되는 환경에서만 사용할 수 있다.\n  Kubernetes 관리자가 아닌 사용자가 직접 브라우저에 인증서를 설치하는 과정이 필요하다.\n  Kubernetes API Server를 외부와 직접 노출하는 것은 바람직하지 않다.\n\n\n4. Ingress 방식\n\n\n  Kubernetes service를 외부와 연결시켜주는 Ingress를 사용하는 방식\n\n\n선행사항\n\n\n  Ingress Controller 설치\n  도메인 연결 및 SSL 인증서 발급\n\n\nkubernetes-dashboard는 https 요청을 강제하지만 외부에서 Ingress Controller로 들어오는 부분이\n\nTLS 인증 처리가 되어 있으므로 kubernetes 내부에서 동작하는 dashboard 서비스 및 pod는 http를 사용 가능하다.\n\n1. kubernetes-dashboard.yaml 수정\n\n$ vim kubernetes-dashboard.yaml\n\n\n\n...\n---\nkind: Service\napiVersion: v1\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\nspec:\n  ports:\n    - port: 9090\t\t\t# 포트 수정\n      targetPort: 9090\t\t# 포트 수정\n  selector:\n    k8s-app: kubernetes-dashboard\n---\n...\n---\n\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  ...\n  spec:\n      containers:\n        - name: kubernetes-dashboard\n          image: kubernetesui/dashboard:v2.2.0\n          imagePullPolicy: Always\n          ports:\n            - containerPort: 9090 # 포트 수정\n              protocol: TCP\n          args:\n            #- --auto-generate-certificates 주석 처리\n            - --namespace=kubernetes-dashboard\n            - --insecure-bind-address=0.0.0.0 # 추가\n            - --enable-insecure-login # 추가\n            - --token-ttl=10800\t# 토큰 세션 유지시간(선택)\n            # Uncomment the following line to manually specify Kubernetes API server Host\n            # If not specified, Dashboard will attempt to auto discover the API server and connect\n            # to it. Uncomment only if the default does not work.\n            # - --apiserver-host=http://my-address:port\n            ...\n          livenessProbe:\n            httpGet:\n              scheme: HTTP # HTTP로 변경\n              path: /\n              port: 9090 # 포트 수정\n            initialDelaySeconds: 30\n            timeoutSeconds: 30\n...\n\n\n\n$ kubectl apply -f kubernetes-dashboard.yaml\n\n\n참고\n\n\n  --namespace=kubernetes-dashboard: namespace 설정\n  --insecure-bind-address=0.0.0.0: http 모든 IP 접속 허용\n  --enbale-insecure-login: http 접속 허용\n\n\n2. kubernetes-dashboard-ingress.yaml 생성\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/ingress.class: &quot;nginx&quot; # kong ingress의 경우 kong\nspec:\n  rules:\n  - http:\n      paths:\n      - pathType: Prefix\n        path: &quot;/&quot;\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 9090\n\n\n\n$ kubectl apply -f kubernetes-dashboard-ingress.yaml\n\n\n3. 접속 확인\n\n# Ingress Controller 외부 노출 포트 확인\n$ kubectl get svc -n ingress-ngninx\ningress-nginx-controller             NodePort    10.104.33.237   &amp;lt;none&amp;gt;        80:31623/TCP,443:32452/TCP   5h\n\n\nhttp://external-ip:31623/ 접속\n(방화벽 포트포워딩 된 경우 해당 포트로 접속)\n\n\n\nhttp를 이용하여 접속할시 로그인이 비활성화 된다.\n\nhttps://external-ip:32452/ 접속\n(방화벽 포트포워딩 된 경우 해당 포트로 접속)\n\n\n\n\n\nhttps를 이용하여 접속시 로그인은 활성화 되지만 인증서 정보가 유효하지 않다고 나온다.\n\n이제 SSL 인증서를 서버 Ingress에 적용할 차례이다.\n\n4. Kubernetes secret 생성\n\n# kubectl create secret {secretType} {secretName} --namespace={namespace} --cert={certPath} --key={keyPath}\n$ kubectl create secret tls secret-tls --namespace=kubernetes-dashboard --cert=ssl/yourdomain.com --key=ssl/yourdomain.com\n\n# 생성 확인\n$ kubectl get secret -n kubernetes-dashboard\n$ kubectl describe secret secret-tls -n kubernetes-dashboard\n\n\nssl 인증서 정보를 담고 있는 kubernetes secret을 생성한다.\n\n이 때, namespace는 Pod의 namespace와 동일해야하므로 설정해준다.\n\n\n  인증서 경로는 ssl 하위에 있다고 가정\n\n\n5. kubernetes-dashboard-ingress.yaml 수정\n\n$ vim kubernetes-dashboard-ingress.yaml\n\n\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/ingress.class: &quot;nginx&quot;\nspec:\n  # 아래 tls 부분 추가\n  tls:\n  - hosts:\n    - k8s.yourdomain.com\n    secretName: secret-tls\n  rules:\n  - host: k8s.yourdomain.com # host 도메인 추가\n    http:\n      paths:\n      - pathType: Prefix\n        path: &quot;/&quot;\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 9090\n              \n\n\n$ kubectl apply -f kubernetes-dashboard-ingress.yaml\n\n\n6. HTTPS 접속 및 인증서 확인\n\n# Ingress Controller 외부 노출 포트 확인\n$ kubectl get svc -n ingress-ngninx\ningress-nginx-controller             NodePort    10.104.33.237   &amp;lt;none&amp;gt;        80:31623/TCP,443:32452/TCP   5h\n\n\nhttps://external-ip:32452/ 접속\n(방화벽 포트포워딩 된 경우 해당 포트로 접속)\n\n\n\n\n\n인증서 적용 확인\n\n7. 베어러 토큰 발급\n\n# 토큰 발급\n\n$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=&quot;{.secrets[0].name}&quot;) -o go-template=&quot;{{.data.token | base64decode}}&quot; &amp;gt;&amp;gt; token\n\n\n# 토큰 확인\n$ cat token\n\n\n\n\n로그인 화면에서 Token 입력 후 로그인\n\n\n\n로그인 확인\n\n\n\nMetric-server 설치\n각 서버에서 cpu, memory metric을 가져와 dashboard에서 그래프로 보여주려면 설치해야한다.\n$ kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml\n\n\nmetrics-server error because it doesn’t contain any IP SANs 에러 발생할 경우\n\ndeployment에 아래를 추가한다.\n\n - args:\n        - --kubelet-insecure-tls\n\n\n\nTroubleshooting\n\n1. 404 page not found\n\n\n위와 같이 404 page not found가 나오는 경우\n\nIngress에서 서비스를 찾지 못한 경우로 경로 설정을 잘 확인해본다.\n\n나는 ingress Path 설정을 &quot;/&quot;가 아니라 &quot;/dashboard&quot;로 해놓았었는데 root path가 아니면 적용이 되지 않는 버그가 존재하였다.\n\n2. kubernetes-dashboard pod가 자꾸 죽어서 재시작되는 경우\n\n위에서 대시보드가 http 프로토콜로 작동하도록 설정한 부분에 누락되거나 잘못 설정한 값이 있는지 확인\n\n나는 deployment쪽의 Liveness probe 설정에 http가 아닌 https로 되어 있어서 문제가 발생하였다.\n\n\n\nReference\n\n\n  kubernetes dashboard\n\n"
} ,
  
  {
    "title"    : "Let&#39;s Encrypt 인증서 발급 및 갱신",
    "category" : "",
    "tags"     : " Certificate, Certbot, Domain, DNS",
    "url"      : "/2021/02/25/letsencrypt-renew.html",
    "date"     : "February 25, 2021",
    "excerpt"  : "Let’s Encrypt 인증서 발급 및 갱신\n\n\n\nLet’s Encrypt Docs\n\n본 문서는 무료 SSL 인증서를 제공하는 Let’s Encrypt를 사용하여 인증서 발급 및 갱신을 하는 방법을 정리하였다.\n\n\n사전 준비 사항\n\n\n  SSL 인증서 적용을 위해서는 도메인 구매 필수\n  구입한 도메인과 웹서버 연결: 가비아와 같은 도메인 구매 기관에서 설정 필요\n  root 유저 상태로 가정하고 진행\n\n\n인증서 발급 방법은 수동으로 c...",
  "content"  : "Let’s Encrypt 인증서 발급 및 갱신\n\n\n\nLet’s Encrypt Docs\n\n본 문서는 무료 SSL 인증서를 제공하는 Let’s Encrypt를 사용하여 인증서 발급 및 갱신을 하는 방법을 정리하였다.\n\n\n사전 준비 사항\n\n\n  SSL 인증서 적용을 위해서는 도메인 구매 필수\n  구입한 도메인과 웹서버 연결: 가비아와 같은 도메인 구매 기관에서 설정 필요\n  root 유저 상태로 가정하고 진행\n\n\n인증서 발급 방법은 수동으로 certbot 설치 후 발급 진행 하는 방법과 docker image로 발급하는 방법이 있다.\n\n\ndocker image로 인증서 발급 방법\ndocker volume path의 mount 지점에 인증서가 생성된다.\n$ docker run -it --rm --name certbot -v &#39;/etc/letsencrypt:/etc/letsencrypt&#39; -v &#39;/var/lib/letsencrypt:/var/lib/letsencrypt&#39; certbot/certbot certonly -d *.yourdomain.com --manual --preferred-challenges dns --server https://acme-v02.api.letsencrypt.org/directory\n\n\ncertbot 설치 후 발급 방법\n\n\n  Ubuntu 20.04 LTS 대상 certbot 설치 시 ppa repo는 deprecated 됨.\n\n  \n    nginx + ubuntu 20.04 환경 시 certbot 설치 매뉴얼 참조하여 설치 진행 참고: https://certbot.eff.org/lets-encrypt/ubuntufocal-nginx\n  \n\n\n\nUbuntu 20.04 환경에서 certbot 설치\n# 필요 시 snapd package 설치. \n# ubuntu 16.04 부터 기본 설치 되어 있음.(https://snapcraft.io/docs/installing-snap-on-ubuntu)\n\n# snap 최신으로 업데이트\n$ sudo snap install core; sudo snap refresh core\n\n# 기존 certbot 설치되어 있으면 삭제\n$ sudo apt-get remove certbot\n\n# certbot 설치\n$ sudo snap install --classic certbot\n\n# certbot 실행파일 link 설정\n$ sudo ln -s /snap/bin/certbot /usr/bin/certbot\n\n# certbot 버전 확인\n$ certbot --version\n\n\nUbuntu\n$ apt-get update\n$ apt-get install software-properties-common\n$ add-apt-repository ppa:certbot/certbot\n$ apt-get update\n$ apt-get install certbot\n\n\nCentOS\n$ sudo yum install epel-release -y\n$ sudo yum install certbot -y\n\n\nOption) certbot 웹서비스 플러그인 설치\n\n# apach 인 경우\n$ apt-get install python-certbot-apach\n\n# nginx 인 경우\n$ apt-get install python-certbot-nginx\n\n\n\n인증서 발급\n\n\n  webroot, standalone, dns의 3가지 방식 중 dns 방식으로 설치 진행\n  DNS 방식 인증서는 와일드카드 url 지원하므로 *.yourdomain.com 형식으로 발급 요청\n  certonly 옵션으로 인증서만 발급 → 웹 서비스의 설정 파일 자동으로 변경하지 않음\n  standalone 방식으로 발급 시 web serivce를 잠시 중단해야 함\n  하루 동안 3번만 발급 가능하므로 발급 시 주의 필요\n\n\nmanual 옵션으로 dns 방식 지정\n\n# dns 방식 인증서 발급 \n$ certbot certonly --manual --preferred-challenges dns-01 --server https://acme-v02.api.letsencrypt.org/directory --agree-tos -m csupreme19@gmail.com -d *.yourdomain.com\n\n\nDNS TXT 값 표시되는 화면에서 엔터 눌러 진행하지 않고 대기\n\ndns 방식 인증서 발급 command 입력 후 더 이상 진행하지 않고 대기한 후 아래 도메인 설정 등을 진행한다.\n\n\n\nDNS TXT record 값 설정\n\n도메인 구입한 곳(가비아 등)의 DNS 관리메뉴로 진입하여 dns txt record 값을 설정.\n\n예를 들어 가비아의 경우 아래 그림과 같이 ‘DNS 레코드 추가’ 메뉴에서 TXT 레코드를 추가 가능\n\n\n\n호스트 컬럼에는 _acme-challenge 만 입력\n\n가비아의 경우 설정한 값 예시는 아래 이미지 참조. record 값 앞/뒤로 쌍따옴표 추가.\n\n\n\n해당 record 설정 후 저장하여 반영. 아래 command로 도메인 서버에 레코드가 반영 되었는지 확인 가능.\n\n도메인 서버 레코드 확인\n\n$ nslookup -type=txt _acme-challenge.yourdomain.com\n# 또는\n$ dig +noall +answer _acme-challenge.yourdomain.com txt\n\n\n정상 반영 시 아래 이미지와 같이 확인 가능.\n\n\n\n이후 앞 과정의 dns 방식 인증서 발급 대기 화면에서 enter를 눌러 진행\n\n\n\n인증서 저장 위치\n\n인증서 생성 후 저장되는 위치:\n\n\n  Certificate is saved at: /etc/letsencrypt/live/{domain name}/fullchain.pem\n  Key is saved at:         /etc/letsencrypt/live/{domain name}/privkey.pem\n\n\n인증서 확인\n\n$ certbot-auto certificates\n\n\n인증서 삭제\n\n$ certbot-auto delete\n\n\n\n인증서 갱신\n\nLet’s Encrypt의 경우 무료이지만 인증서 유효 기간이 3개월로 짧다.\n\n인증서 만료 전 반드시 갱신을 해야만 site에 접속이 안되는 현상을 방지할 수 있다.\n\ngoogle calendar에 3개월 주기로 알람을 등록하여 인증서 갱신 관리는 하는 것을 추천.\n\n또한 인증서를 manual 로 생성했을 경우 certbot 자동 갱신 명령어로는 갱신 되지 않는다.\n\n이 경우 아래 명령어로 갱신가능\n\n$ certbot --server https://acme-v02.api.letsencrypt.org/directory -d &quot;*.yourdomain.com&quot; --manual --preferred-challenges dns-01 certonly\n\n\n명령어 입력 후 나오는 DNS TXT record 값을 도메인 구입처(예를 들어 가비아 등)에서 DNS 설정에 들어가 업데이트 후 진행한다.\n\n\n  인증서를 갱신하여 새로운 인증서를 받아도 기존 인증서는 만료되지 않는다.\n\n\n\n인증서 갱신 후 서비스별 적용 가이드\n\n기본적으로 NAS에 인증서 정보를 저장하고 각 서비스에 가져다 사용한다.\n\n본 예제에서는 인증서를 /nas/volumes/ssl에서 관리\n\n\n  k8s의 경우 kubernetes secret 형태로 떠져 있음\n  그 외 docker로 직접 올라가있는 경우 각 서비스별 인증서 정보 적용\n\n\n위에서 발급받은 인증서파일을 관리하는 인증서 폴더로 옮긴다.\n\n# 인증서 복사\n$ cp /etc/letsencrypt/live/yourdomain.com-0001/fullchain.pem /nas/volumes/ssl\n$ cp /etc/letsencrypt/live/yourdomain.com-0001/privkey.pem\n\n# 인증서 포맷 변환\n$ openssl x509 -inform PEM -in fullchain.pem -out yourdomain.com.crt\n$ openssl rsa -in privkey.pem -text -text &amp;gt; yourdomain.com.key\n\n\n\n  기존의 인증서는 백업할 것\n\n\n\nTroubleshooting\n\n1. NET::ERR_CERT_COMMON_NAME_INVALID\n\n인증서 갱신 및 발급 후에 인증서 적용이 성공적으로 되었으나 NET::ERR_CERT_COMMON_NAME_INVALID로 인증서가 유효하지 않음\n\n인증서의 주소와 인증서가 적용된 도메인의 주소가 일치하지 않을 때 발생하는 오류\n\n\n  인증서의 도메인 주소가 제대로 적용되었는지 확인한다.\n  * wildcard가 포함되어 있는지 확인한다.\n  도메인 주소가 정상인 경우 전세계의 DNS 서버에 적용이 바로 되지 않고 시간이 어느정도 소요되어서 발생하는 문제로 기다리면 해결된다.\n\n\n도메인 서버 레코드 확인\n\n$ nslookup -type=txt _acme-challenge.subdomain.yourdomain.com\n# 또는\n$ dig +noall +answer _acme-challenge.sbubdomain.yourdomain.com txt\n\n\nAuthoritative answers can be found from:에 리스트로 도메인 서버 정보가 나오면 등록이 완료된 것\n\n\n\nReference\n\n\n  Let’s Encrypt Docs\n  Certbot\n  Secret\n  Certified Kubernetes Administrator(CKA) with Practice Tests\n\n"
} ,
  
  {
    "title"    : "GitLab 백업 및 기타 설정 그리고 트러블슈팅",
    "category" : "",
    "tags"     : " Gitlab, Docker, SSH, Git, Backup, Restore, SSL, TLS, SMTP, Nginx",
    "url"      : "/2021/02/25/gitlab-config.html",
    "date"     : "February 25, 2021",
    "excerpt"  : "GitLab 백업 및 기타 설정 그리고 트러블슈팅\n\n\n\nConfiguring Omnibus GitLab\n\nGitLab 백업, SSH, 메일 서버 등 설정\n\n\nBackup 설정\nBackup 플랜\n\n\n  백업 주기: 3일\n  백업 범위(기본값)\n    \n      db (database)\n      uploads (attachments)\n      builds (CI job output logs)\n      artifacts (CI job a...",
  "content"  : "GitLab 백업 및 기타 설정 그리고 트러블슈팅\n\n\n\nConfiguring Omnibus GitLab\n\nGitLab 백업, SSH, 메일 서버 등 설정\n\n\nBackup 설정\nBackup 플랜\n\n\n  백업 주기: 3일\n  백업 범위(기본값)\n    \n      db (database)\n      uploads (attachments)\n      builds (CI job output logs)\n      artifacts (CI job artifacts)\n      lfs (LFS objects)\n      registry (Container Registry images)\n      pages (Pages content)\n      repositories (Git repositories data)\n    \n  \n  백업 위치: GitLab 로컬, nas의 nfs\n  백업 실행: 3일마다 새벽 2시\n  보관 주기: 30일\n\n\n\n  gitlab.rb, gitlab-secrets.json과 같은 설정파일들은 기본적으로 백업되지 않으며 필요시 별도로 백업 실행\n\n\n수동 Backup\n\n1. 백업 실행\n\n$ docker exec -it gitlab-ce bash # -t: 실행 쉘 확인 용도\n$ gitlab-backup create\n\n# 12.1 이하 버전인 경우\n$ gitlab-rake gitlab:backup:create\n\n\n2. 백업 파일 확인\n\n# cd {docker_volume}/gitlab/data/backups\n$ cd /data/docker_volumes/gitlab/data/backups\n\n\n[TIMESTAMP]_gitlab_backup.tar 생성 확인\n\n3. 설정 파일 백업시\n\n/data/docker_volumes/gitlab/config/gitlab-secrets.json\n/data/docker_volumes/gitlab/config/gitlab.rb\n\n\n위 두 파일 별도로 백업할 것\n\n수동 Restore\n\n1. 복구 준비\n\n\n  \n    복구된 버전과 복구하려는 gitlab 버전이 같아야함\n  \n  \n    gitlab reconfigure 실행 필요\n  \n  \n    gitlab이 실행되고 있어야함\n  \n  \n    복구할 파일(.tar) 위치는 /var/opt/gitlab/backups 에 있다고 가정\n  \n\n\nDB 접근하는 프로세스 종료\n\n# gitlab shell 진입\n$ docker exec -it gitlab-ce bash\n\n# process 정지 처리\n$ gitlab-ctl stop unicorn\n$ gitlab-ctl stop puma\n$ gitlab-ctl stop sidekiq\n\n# 종료 확인\n$ gitlab-ctl status \n\n\n2. 복구 실행\n\n$ chmod 0644 1612940314_2021_092_10_13.8.1_gitlab_backup.tar\t# 모든 사용자 읽기 권한 추가\n$ docker exec -t gitlab-ce gitlab-backup restore BACKUP={TIMESTAMP} force=yes\n\n# 12.1 이하 버전인 경우\n$ docker exec -t gitlab-ce gitlab-rake gitlab:backup:restore BACKUP={TIMESTAMP} force=yes\n\n\n예를 들어 파일명이 1612940314_2021_092_10_13.8.1_gitlab_backup.tar이라면\n\n[TIMESTAMP]_gitlab_backup.tar 이므로 TIMESTAMP는 1612940314_2021_092_10_13.8.1\n\n복구 시점에 psql must be owner of 관련 오류는 정상임\n\n3. 서비스 재구동 및 점검\n\n$ docker exec -it gitlab-ce bash\n$ gitlab-ctl reconfigure\n$ gitlab-ctl restart\n\n$ gitlab-rake gitlab:check SANITIZE=true\n\n\n자동 Backup 설정\n\n1. NFS 설정\n\nmount 및 권한 설정은 완료되었다는 가정하에 작성하였습니다.\n\nGitLab 백업 폴더 생성\n\n# mkdir -p {nfs 볼륨 경로}/gitlab/data/backups\n$ mkdir -p /nfs_nas/volumes/gitlab/data/backups\n\n\n2. docker compose 설정\n\n# docker_compose yaml 설정 파일\n$ vim ~/gitlab-ce/docker-compose.yml\n\n\n...\n    environment:\n      GITLAB_OMNIBUS_CONFIG: |\n        gitlab_rails[&#39;backup_keep_time&#39;] = 2592000\n...\n\n\ngitlab_rails 설정값 변경(추가)\n\n\n  backup_path: 백업 경로(local)\n  backup_keep_time: 백업 파일 보관 주기 (2592000초 = 30일)\n\n\ngitlab이 설치된 volume의  gitlab.rb 설정파일 수정해도 됨\n\n3. 백업 스크립트 작성\n\n$ vim ~/gitlab-ce/gitlab_backup.sh\n\n\n #!/bin/sh\n BACKUP_DIR=&#39;/data/docker_volumes/gitlab/data/backups&#39;\n BACKUP_REMOTE_DIR=&#39;/nfs_nas1/volumes/gitlab/data/backups&#39;\n BACKUP_REMOTE_DIR2=&#39;/nfs_nas2/backups/gitlab&#39;\n BACKUP_LIFETIME=30\n docker exec gitlab-ce gitlab-backup create CRON=1\n BACKUP_FILE=`ls -t $BACKUP_DIR | head -n 1`\n echo &quot;gitlab-backup create::: $BACKUP_DIR/$BACKUP_FILE &quot;\n\n BACKUP_REMOVE_FILES=`find $BACKUP_DIR -name &quot;*_gitlab_backup.tar&quot; -mtime +$BACKUP_LIFETIME -type f`\n if [ -n &quot;$BACKUP_REMOVE_FILES&quot; ]; then\n echo &quot;gitlab-backup remove old backups::: $BACKUP_REMOVE_FILES&quot;\n rm -rf $BACKUP_REMOVE_FILES\n fi\n\n BACKUP_REMOTE_REMOVE_FILES=`find $BACKUP_REMOTE_DIR -name &quot;*_gitlab_backup.tar&quot; -mtime +$BACKUP_LIFETIME -type f`\n if [ -n &quot;$BACKUP_REMOTE_REMOVE_FILES&quot; ]; then\n echo &quot;gitlab-backup remove old backups::: $BACKUP_REMOTE_REMOVE_FILES&quot;\n rm -rf $BACKUP_REMOTE_REMOVE_FILES\n fi\n\n BACKUP_REMOTE_REMOVE_FILES2=`find $BACKUP_REMOTE_DIR2 -name &quot;*_gitlab_backup.tar&quot; -mtime +$BACKUP_LIFETIME -type f`\n if [ -n &quot;$BACKUP_REMOTE_REMOVE_FILES2&quot; ]; then\n echo &quot;gitlab-backup remove old backups::: $BACKUP_RETMOE_REMOVE_FILES2&quot;\n rm -rf $BACKUP_REMOTE_REMOVE_FILES2\n fi\n\n cp $BACKUP_DIR/$BACKUP_FILE $BACKUP_REMOTE_DIR\n echo &quot;gitlab-backup copy to storage::: $BACKUP_REMOTE_DIR/$BACKUP_FILE $BACKUP_REMOTE_DIR/$BACKUP_FILE&quot;\n cp $BACKUP_DIR/$BACKUP_FILE $BACKUP_REMOTE_DIR2\n echo &quot;gitlab-backup copy to storage::: $BACKUP_REMOTE_DIR2/$BACKUP_FILE $BACKUP_REMOTE_DIR2/$BACKUP_FILE&quot;\n echo &quot;gitlab-backup done.&quot;\n\n\n\n  위 스크립트는 백업 명령 실행 후 백업 파일을 mv, rm 하는 간단한 스크립트이며 상황별로 다를 수 있음\n\n\n4. 백업 cron 등록\n\n$ crontab -e\n0 2 */3 * * /root/gitlab-ce/gitlab_backup.sh &amp;gt; /root/gitlab-ce/gitlab_backup.sh.log\n\n\ngitlab 컨테이너 내부에서 crontab 설정도 가능\n\n\n  CRON=1: 에러가 발생한 경우에만 출력\n\n\n5. cron 테스트\n\n$ run-parts /var/spool/cron -v\n\n\n\nEmail 설정(SMTP)\n\n\n  docker container 환경에서 gitlab의 email 설정\n\n\n사전 준비 사항\n\n  mail server 정보 확인: gmail 또는 별도 무료 사용 가능한 mail server 확보\n  \n    본 문서에서는 mailgun 서버를 적용\n  \n  SMTP Settings#Mailgun\n\n\nSMTP 설정\n\n1. gitlab의 설정파일 수정\ndocker-compose.yml의 gitlab environment에 추가 또는 gitlab 설치된 volume에서 gitlab.rb 파일 찾아 수정\n\n해당 파일 내용 중 mailgun 메일서버 정보를 추가\n\n# mailgun smtp 설정\ngitlab_rails[&#39;smtp_enable&#39;] = true\ngitlab_rails[&#39;smtp_address&#39;] = &quot;smtp.mailgun.org&quot;\ngitlab_rails[&#39;smtp_port&#39;] = 587\ngitlab_rails[&#39;smtp_authentication&#39;] = &quot;plain&quot;\ngitlab_rails[&#39;smtp_enable_starttls_auto&#39;] = true\ngitlab_rails[&#39;smtp_user_name&#39;] = &quot;mail서버 발급받은 id&quot;\ngitlab_rails[&#39;smtp_password&#39;] = &quot;암호&quot;\ngitlab_rails[&#39;smtp_domain&#39;] = &quot;mg.gitlab.com&quot;\n\n\n2. docker container 재기동\n\n3. mail 보내기 테스트\n\ngitlab의 shell로 진입\n\n$ docker exec -it &amp;lt;container id&amp;gt; bash\n\ngitlab-rails console 실행\n\n$ gitlab-rails console\n\ngitlab-rails console 상에서 mail 보내기 테스트\n\n$ Notify.test_email(&#39;&amp;lt;받는 email address&amp;gt;&#39;, &#39;&amp;lt;email 제목&amp;gt;&#39;, &#39;&amp;lt;email 내용&amp;gt;&#39;).deliver_now\n\nemail이 왔는지 확인\n\n\nSSH Key 설정\n\nSSH Key 등록\n\n1. 접속하고자 하는 사용자에서 SSH Key 발급\n\n$ ssh-keygen -t rsa\n\n\nRSA 키 발급, passphrase는 일반적으로 비워둠(Enter)\n\n2. 발급된 Public key 확인\n\nchmod 755 ~/.ssh/id_rsa.pub\nvi ~/.ssh/id_rsa.pub\n\n\nid_rsa.pub안의 퍼블릭키 복사해두기\n\n3. GitLab 계정에 public key 등록\n\n\n\nUser Settings - SSH Keys\n\nid_rsa.pub에 있는 전체 키 내용 복사하여 붙여넣기\n\nTitle은 자신이 구분할  수 있는 제목 아무거나\n\n만료일은 해당 키의 만료일로 설정\n\n4. git 계정 설정\n\ngit config --global user.name &quot;{USERNAME}&quot;\ngit config --global user.email {USER_EMAIL}\n\n\nUSERNAME: gitlab 계정명\nUSER_EMAIL: gitlab 계정의 이메일 주소\n\n5. SSH 접속 확인\n\n# ssh -T git@{host} -p {port} -v\n$ ssh -T git@gitlab.yourdomain.com -p 20722 -v\nWelcome to GitLab, @csupreme!\n\n\n-p: 포트 설정\n\n-v: 디버그 로그 확인\n\n처음 접속시 호스트 키 저장 여부를 물으면 yes\n\n6. Git clone 확인\n\n# git clone ssh://git@{host}:{port}/{repository}.git\ngit clone ssh://git@gitlab.yourdomain.com:20722/csupreme/commit-test.git\n\n\n별도의 사용자, 암호 입력 없이 SSH Key를 이용하여 clone 확인\n\n\nNginx SSL 설정\n선행사항\n\n\n  Docker Container 환경(GitLab Omnibus)\n  Let’s Encrypt Key 발급 완료\n\n\nNginx 설정\n\n1. docker_compose.yml 수정\n\n    environment:\n      GITLAB_OMNIBUS_CONFIG: |\n        letsencrypt[&#39;enable&#39;] = false\n        external_url &#39;https://gitlab.yourdomain.com:20443&#39;\n        nginx[&#39;redirect_http_to_https&#39;] = true\n    ports:\n        - &#39;20780:20443&#39;\n        - &#39;20443:20443&#39;\n        - &#39;20722:22&#39;\n\n\nletsencrypt[‘enable’] = false로 설정하는 이유\n\ngitlab-ctl reconfigure시 자동으로 인증서를 갱신하여 덮어쓰기 때문\n\n수동으로 발급한 인증서이므로 letsencrypt의 자동 갱신 기능을 비활성화 하기 위해\n\n2. SSL 인증서 복사\n\n# 참고) 컨테이너 /etc/gitlab에 마운트된 경로\n$ sudo mkdir -p /data/docker_volumes/gitlab/config/ssl\n$ sudo chmod 755 /data/docker_volumes/gitlab/config/ssl\n$ cp /etc/letsencrypt/live/yourdomain.com/privkey.pem /data/docker_volumes/gitlab_2/config/ssl\n$ cp /etc/letsencrypt/live/yourdomain.com/fullchain.pem /data/docker_volumes/gitlab_2/config/ssl\n\n\n3. PEM 변환\n\n$ openssl rsa -in privkey.pem -text &amp;gt; gitlab.yourdomain.com.key\n$ openssl x509 -inform PEM -in fullchain.pem -out gitlab.yourdomain.com.crt\n\n\n4. GitLab 재설정\n\n$ docker exec -t gitlab-ce gitlab-ctl reconfigure\n\n# 도커 포트 매핑이 변경된 경우 컨테이너 재실행 필요\n$ docker-compose down\n$ docker-compose up -d\n\n\n5. Https 접속 확인 및 인증서 확인\n\n$ cat /data/docker_volumes/gitlab/logs/nginx/gitlab_access.log\n\n\n\n\n\nTroubleshooting\n\n1. SSH Connection은 성공이지만 서버에서 Connection을 닫을 때\n\n\n\n로그 확인\n\n$ cat /data/docker_volumes/gitlab/logs/sshd/current\n\n\n2021-02-10_01:55:53.62004 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n2021-02-10_01:55:53.62005 @         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n2021-02-10_01:55:53.62005 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n2021-02-10_01:55:53.62005 Permissions 0755 for &#39;/etc/gitlab/ssh_host_rsa_key&#39; are too open.\n2021-02-10_01:55:53.62006 It is required that your private key files are NOT accessible by others.\n2021-02-10_01:55:53.62006 This private key will be ignored.\n2021-02-10_01:55:53.62006 key_load_private: bad permissions\n2021-02-10_01:55:53.62006 Could not load host key: /etc/gitlab/ssh_host_rsa_key\n2021-02-10_01:55:53.62007 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n2021-02-10_01:55:53.62007 @         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n2021-02-10_01:55:53.62007 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n2021-02-10_01:55:53.62007 Permissions 0755 for &#39;/etc/gitlab/ssh_host_ecdsa_key&#39; are too open.\n2021-02-10_01:55:53.62007 It is required that your private key files are NOT accessible by others.\n2021-02-10_01:55:53.62007 This private key will be ignored.\n2021-02-10_01:55:53.62007 key_load_private: bad permissions\n2021-02-10_01:55:53.62014 Could not load host key: /etc/gitlab/ssh_host_ecdsa_key\n2021-02-10_01:55:53.62014 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n2021-02-10_01:55:53.62015 @         WARNING: UNPROTECTED PRIVATE KEY FILE!          @\n2021-02-10_01:55:53.62015 @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n2021-02-10_01:55:53.62019 Permissions 0755 for &#39;/etc/gitlab/ssh_host_ed25519_key&#39; are too open.\n2021-02-10_01:55:53.62019 It is required that your private key files are NOT accessible by others.\n2021-02-10_01:55:53.62019 This private key will be ignored.\n2021-02-10_01:55:53.62020 key_load_private: bad permissions\n2021-02-10_01:55:53.62020 Could not load host key: /etc/gitlab/ssh_host_ed25519_key\n\n\n\nPrivate Key 파일 권한이 755로 되어 있어 SSHD에서 자체적으로 Bad Permission을 리턴\n\n권한 수정\n\n$ chmod 600 ssh_host_ecdsa_key\n$ chmod 600 ssh_host_ed25519_key\n$ chmod 600 ssh_host_rsa_key\n$ chmod 644 ssh_host_ecdsa_key.pub\n$ chmod 644 ssh_host_ed25519_key.pub\n$ chmod 644 ssh_host_rsa_key.pub\n\n\n접속 및 커밋 확인\n\n$ ssh -T git@gitlab.yourdomain.com -p 20722\nWelcome to GitLab, @csupreme!\n\n\n$ git clone ssh://git@gitlab.yourdomain.com:20722/csupreme/commit-test.git\n\n\n2. ssh 접속시 아래와 같이 Host 정보가 변경되었다고 나올 때\n\n\n\n기존 서버의 IP는 동일하지만 서버의 SHA256 fingerprint가 바뀐 상황\n\nssh의 known_host에 있는 해당 서버의 RSA Host key키 부분을 삭제한 뒤 접속하면 된다.\n\n$ vim /Users/{USERNAME}/.ssh/known_hosts\n\n\n\n\n접속시 바뀐 호스트 키 등록 yes후 진행\n\n\n\nReference\n\n\n  Configuring Omnibus GitLab\n  SMTP Settings#Mailgun\n\n"
} ,
  
  {
    "title"    : "CentOS 파티션 생성 및 마운트",
    "category" : "",
    "tags"     : " Centos, RHEL, Linux, OS, Partition, Mount",
    "url"      : "/2021/02/25/centos-partition-mount.html",
    "date"     : "February 25, 2021",
    "excerpt"  : "CentOS 파티션 생성 및 마운트\n\n1. 디스크 정보 확인\n\nfdisk -l\n\n\n\n\n2. 파티션 구성하기\n\nfdisk /dev/sda\n\n\n\n\n별도의 파티션을 구성하는 것이 아니라면 기본값 사용\n\nPartition type: p\n\nPartition number: 1 (시스템에 따라 다를 수 있음)\n\nFirst sector: 2048 (시스템에 따라 다를 수 있음)\n\nLast sector: 4294967294 (시스템에 따라 다를 수 있음)...",
  "content"  : "CentOS 파티션 생성 및 마운트\n\n1. 디스크 정보 확인\n\nfdisk -l\n\n\n\n\n2. 파티션 구성하기\n\nfdisk /dev/sda\n\n\n\n\n별도의 파티션을 구성하는 것이 아니라면 기본값 사용\n\nPartition type: p\n\nPartition number: 1 (시스템에 따라 다를 수 있음)\n\nFirst sector: 2048 (시스템에 따라 다를 수 있음)\n\nLast sector: 4294967294 (시스템에 따라 다를 수 있음)\n\np 명령어로 파티션 설정 확인\n\nw 명령어로 저장하고 fdisk 나오기\n\n3. 파티션 포맷하기\n\nmkfs.ext4 /dev/sda\n\n\n\n\n위에서 구성한 파티션을 포맷한다\n\next4 포맷 사용\n\n4. 파티션 마운트하기\n\nmount /dev/sda /data\n\n\n/dev/sda 파티션을 /data에 마운트\n\n5. 마운트 확인\n\ndf -h\nmount -a\n\n\n\n\n\n\n마운트 디렉토리에 lost+found 디렉토리 확인\n\n6. 재부팅시 자동 마운트를 위한 설정\n\nblkid\n\n\n\n\n하드디스크 UUID 확인\n\nvim /etc/fstab\n\n\n\n\nUUID={UUID} {마운트 디렉토리} {파티션 포맷} defaults 0 0 추가\n\n\n"
} ,
  
  {
    "title"    : "GitLab 구축하기",
    "category" : "",
    "tags"     : " Gitlab, Docker, SSH, Git",
    "url"      : "/2021/02/24/gitlab-install.html",
    "date"     : "February 24, 2021",
    "excerpt"  : "GitLab 구축하기\n\n\n\nGitlab Docker Images\n\nDocker를 활용한 Gitlab 컨테이너 구축 및 기초 설정\n\n\n\nGitLab 구축\n\n\nDocker Compose 설치\ndocker-compose 버전이 매번 변경 됨\n\n최신 버전 설치 스크립트는 docker-compose home 에서 확인\n\n1. Docker compose stable 설치\n\n$ curl -L &quot;https://github.com/docker/compos...",
  "content"  : "GitLab 구축하기\n\n\n\nGitlab Docker Images\n\nDocker를 활용한 Gitlab 컨테이너 구축 및 기초 설정\n\n\n\nGitLab 구축\n\n\nDocker Compose 설치\ndocker-compose 버전이 매번 변경 됨\n\n최신 버전 설치 스크립트는 docker-compose home 에서 확인\n\n1. Docker compose stable 설치\n\n$ curl -L &quot;https://github.com/docker/compose/releases/download/1.28.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose\n\n\n\n\n2. 바이너리 실행 권한 설정 및 시스템 바이너리 등록\n\n$ chmod +x /usr/local/bin/docker-compose\n$ ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n\n\n3. 설치 및 버전 확인\n\n$ docker-compose --version\n\n\n\n\n\nGitlab 설치\n\n1. GitLab working directory 생성\n\n$ mkdir ~/gitlab-ce\n\n\n2. docker-compose.yml 작성\n\n$ vim ~/gitlab-ce/docker-compose.yml\n\n\nversion: &#39;3&#39;\n\nservices:\n  gitlab:\n    # gitlab 백업 후 복원 시 gitlab의 버전이 일치해야 하기 때문에 최신 버전 대신 특정 버전을 사용\n    # image: &#39;gitlab/gitlab-ce:latest&#39;\n    image: &#39;gitlab/gitlab-ce:13.8.4-ce.0&#39;\n    restart: always\n    container_name: &#39;gitlab-ce&#39;\n    hostname: &#39;gitlab.yourdomain.com&#39;\n    environment:\n      TZ: &quot;Asia/Seoul&quot;\n      GITLAB_OMNIBUS_CONFIG: |\n      external_url &#39;http://gitlab.yourdomain.com:20780&#39;\n      gitlab_rails[&#39;time_zone&#39;] = &#39;Asia/Seoul&#39;\n    ports:\n      - &#39;20780:20780&#39;\n      - &#39;20443:443&#39;\n      - &#39;20722:22&#39;\n    volumes:\n      - &#39;/data/docker_volumes/gitlab/config:/etc/gitlab&#39;\n      - &#39;/data/docker_volumes/gitlab/logs:/var/log/gitlab&#39;\n      - &#39;/data/docker_volumes/gitlab/data:/var/opt/gitlab&#39;\n\n\n\n  현재 gitlab wiki의 file attach API 버그로 접속한 포트가 아닌 설정값의 external_url을 host로 물고 가고 있어서 도커 포트 매핑을 동일한 포트로 설정\n\n\n3. Docker-compose 실행\n\n실행\n\n$ cd ~/gitlab-ce\n$ docker-compose up -d\n\n\n로그 확인\n\n$ docker-compose logs -f\n\n\n컨테이너 목록 확인\n\n$ docker-compose ps -a\n\n\n4. GitLab Web 접속 확인\n\n\n\n5. 초기 접속시 root 계정 비밀번호 설정\n\n\nGitLab 설정\n\n1. 사용자 계정 등록 비활성화\n\n\n\n\n\nAdmin Area - Settings - General - Sign-up restrictions에서\n\nSign-up enabled 체크 해제\n\n2. 저장소 Https Clone 비활성화\n\n\n\nAdmin Area - Settings - General - Visibility and access controls\n\nEnabled Git access protocols - Only SSH로 변경 - Save Changes\n\n\n  Application level에서 막는 것으로 Https 프로토콜 및 포트 접근을 막는 것은 아님\n\n\n\n\n추가 설정\n\nSSH, Backup, Mail 등 추가 설정은 GitLab 백업 및 기타 설정 문서 참고\n\n\n\nReference\n\n\n  Gitlab Docker Images\n\n"
} 
  
  ,
  
  {
  
  "title"    : "Ninja",
  "category" : "",
  "tags"     : " Lorem",
  "url"      : "/portfolio/ninja",
  "date"     : "April 8, 2014",
  "excerpt"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem ...",
  "content"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, \nadipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\n"
  
} ,
  
  {
  
  "title"    : "Creative",
  "category" : "",
  "tags"     : " Ipsum",
  "url"      : "/portfolio/safe",
  "date"     : "August 16, 2014",
  "excerpt"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem qu...",
  "content"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, \nsed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\nUse this area of the page to describe your project. \nThe icon above is part of a free icon set by Flat Icons.\n"
  
} ,
  
  {
  
  "title"    : "Circus",
  "category" : "",
  "tags"     : " Ipsum",
  "url"      : "/portfolio/circus",
  "date"     : "September 1, 2014",
  "excerpt"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem ...",
  "content"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, \nadipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\n"
  
} ,
  
  {
  
  "title"    : "Tower of Hanoi",
  "category" : "",
  "tags"     : " ",
  "url"      : "/portfolio/hanoi",
  "date"     : "September 1, 2014",
  "excerpt"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem ...",
  "content"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, \nadipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\n"
  
} ,
  
  {
  
  "title"    : "Tic tac toe",
  "category" : "",
  "tags"     : " ",
  "url"      : "/portfolio/tictactoe",
  "date"     : "September 1, 2014",
  "excerpt"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem qu...",
  "content"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, \nsed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\nUse this area of the page to describe your project. \nThe icon above is part of a free icon set by Flat Icons.\n\n"
  
} ,
  
  {
  
  "title"    : "Cake",
  "category" : "",
  "tags"     : " Lorem, Ipsum",
  "url"      : "/portfolio/cake",
  "date"     : "September 27, 2015",
  "excerpt"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem qu...",
  "content"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, \nsed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\nUse this area of the page to describe your project. \nThe icon above is part of a free icon set by Flat Icons.\n"
  
} ,
  
  {
  
  "title"    : "Jekyll",
  "category" : "",
  "tags"     : " ",
  "url"      : "/portfolio/jekyllblog",
  "date"     : "May 26, 2017",
  "excerpt"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem qu...",
  "content"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, \nadipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\n"
  
} ,
  
  {
  
  "title"    : "Lorem Ipsum",
  "category" : "",
  "tags"     : " ",
  "url"      : "/portfolio/submarine",
  "date"     : "September 3, 2017",
  "excerpt"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem qu...",
  "content"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, \nsed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\nUse this area of the page to describe your project. \nThe icon above is part of a free icon set by Flat Icons.\n"
  
} ,
  
  {
  
  "title"    : "Github",
  "category" : "",
  "tags"     : " Lorem",
  "url"      : "/portfolio/gitlecture",
  "date"     : "October 20, 2017",
  "excerpt"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem ...",
  "content"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, \nadipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\n"
  
} 
  
]
