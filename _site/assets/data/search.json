[
  
  {
    "title"    : "Jekyll과 Github pages를 사용하여 블로그 만들기",
    "category" : "",
    "tags"     : " Blog, Github, Github pages, Jekyll",
    "url"      : "/2021/11/18/github-pages-with-jekyll.html",
    "date"     : "November 18, 2021",
    "excerpt"  : "Jekyll과 Github pages를 사용하여 간단하게 블로그를 만들어보자\n\n\n\nGitHub Pages\n\n본 문서는 현재 보고 있는 블로그를 구현하였던 내용을 담고 있다.\n\n\n\n시작하게된 계기\n\n문득 개발을 하고 있는데 왜 남는게 없지? 라는 생각이 들기 시작하였다.\n\n요즘엔 양질의 개발자료를 손가락 몇번 놀리면 쉽게 찾을 수 있다보니 정보를 검증도 하지 않고 여과없이 받아들일 수도 있겠다는 생각이 들었고 정보를 내것으로 만들어야겠다는 ...",
  "content"  : "Jekyll과 Github pages를 사용하여 간단하게 블로그를 만들어보자\n\n\n\nGitHub Pages\n\n본 문서는 현재 보고 있는 블로그를 구현하였던 내용을 담고 있다.\n\n\n\n시작하게된 계기\n\n문득 개발을 하고 있는데 왜 남는게 없지? 라는 생각이 들기 시작하였다.\n\n요즘엔 양질의 개발자료를 손가락 몇번 놀리면 쉽게 찾을 수 있다보니 정보를 검증도 하지 않고 여과없이 받아들일 수도 있겠다는 생각이 들었고 정보를 내것으로 만들어야겠다는 필요성이 느껴졌다.\n\n문서를 작성하면서 헷갈리는 부분도 정리할 수 있고 무엇보다 남들에게 설명하도록 나중에 내가 봐도 이해할 수 있도록 작성하려면 확실한 이해가 바탕이 되어야 하기 때문에 시작하게 되었다.\n\n\n\n어떤걸 써야할까\n\n웹개발이라는 것은 익숙하지만 결코 간단한 것이 아니다. (자세히 말하면 public하게 오픈하는 것)\n\n\n  JS, CSS 등의 Frontend 스택을 활용하여 프론트 웹을 개발한다.\n  웹에서 사용할 REST API를 정의 및 개발하고 경우에 따라 DB를 구축한다.\n  Cloud 환경이든 자체 구축 서버든 웹을 빌드하여 올릴 서버가 필요하다.\n  도메인, 인증서 구매 및 적용을 통해 외부에 오픈이 필요하다.\n  추가적으로) 최소한의 보안과 무중단 배포를 위한 proxy 서버가 필요\n\n\n선택기준\n\n\n  간단한 기술 블로그를 위하여 웹개발을 하기는 싫었다.\n  자체 서버를 구축하기 싫었다. (비용적으로든 노력으로든)\n  추가적인 비용을 들이기 싫었다.\n\n\n따라서 자체 호스팅을 하는 사이트를 이용하는 방법을 선택하였다.\n\n\n\n블로그 플랫폼\n\n조사 결과 여러가지 블로그 플랫폼 서비스들이 존재하였다.\n\n\n  https://velog.io\n    \n      \n        개발자를 위한 블로그 서비스(플랫폼)\n      \n      \n        마크다운 작성 가능\n      \n    \n  \n  \n    https://wordpress.org\n\n    \n      \n        전세계적으로 가장 유명한 블로그 서비스\n      \n      \n        커스터마이징 기능이 강력\n      \n      \n        방대한 생태계\n      \n    \n  \n  \n    https://www.tistory.com\n\n    \n      \n        유명한 블로그 플랫폼\n      \n      \n        WYSIWYG 방식의 에디터를 제공\n      \n      \n        특정 환경에서 접속이 매우 느림\n      \n    \n  \n  \n    https://blog.naver.com\n\n    \n      \n        유명한 블로그 플랫폼2\n      \n      \n        WYSIWYG 방식의 에디터를 제공\n      \n      \n        구글 검색이 안되는 치명적인 단점이 있음\n      \n      \n        개발 한정으로 매우 기능이 부실\n      \n    \n  \n  \n    https://pages.github.com\n\n    \n      \n        github과 연동됨\n      \n      \n        markdown 지원\n      \n      \n        퍼블릭 도메인 간편하게 설정 가능\n      \n    \n  \n\n\n결론은?\n\n웹 개발에 공을 들이기는 싫었고 현재 마크다운으로 작성되어 있는 문서들을 쉽게 올릴 수 있어야 하였고 github과 연동되는 Github pages를 사용하기로 하였다.\n\n\n\nJekyll\n\n\n\nhttps://jekyllrb.com/\n\nGitHub 공동 설립자 Tom Preston-Werner에 의해 개발된 Ruby 기반의 정적 사이트 생성기(Static site generator)이다.\n\n여기서 SSG(Static Site Generator)란 DB없이 static file 즉 html만으로 돌아가는 웹을 의미한다.\n\n마크다운 형태로 작성이 가능하여 개발자 freindly하며 구현도 매우 간편하게 할 수 있다.\n\nGithub pages와 궁합이 매우 좋으며 Github pages 공식 문서에서도 Jekyll을 이용하도록 안내하고 있다.\n\n\n\nGithub pages\n\n1. 저장소 생성\n\n\n\nGithub pages는 계정의 github repository를 기반으로 웹을 제공한다.\n\n{사용자명}.github.io 의 형태로 저장소를 생성하면 해당 저장소는 자동으로 github pages 저장소로 설정이 된다.\n\n(올바른 branch에 커밋시 자동으로 github pages를 빌드하도록 설정이 되어있다.)\n\n\n  사진은 이미 저장소를 만들었기 때문에 중복되었다고 나옴\n\n\n2. git 설정\n\n# 유저명은 csupreme19로 가정\n$ mkdir -p ~/git/csupreme19.github.io\n$ cd ~/git/csupreme19.github.io\n# git 사용자 설정\n$ git config --local user.name csupreme19\n$ git config --local user.email csupreme19@gmail.com\n\n# git 로컬 저장소 초기화\n$ git init\n\n# git remote 저장소 연결\n$ git remote add origin git@github.com:csupreme19/csupreme19.github.io.git\n$ git remote -v\norigin\tgit@github.com:csupreme19/csupreme19.github.io.git (fetch)\norigin\tgit@github.com:csupreme19/csupreme19.github.io.git (push)\n\n\n\n  나는 사내 gitlab의 계정이 global로 설정되어 있어 해당 git 저장소에만 config를 적용하도록 --local flag를 사용하였다.\n\n\n3. 테마 설정\n\nhttp://jekyllthemes.org/\n\n위 사이트에서 마음에 드는 테마를 선택 후 github에서 fork 또는 clone한다.\n\n\n  jekyll을 설치 하는 것이 원래 해야 할 일이지만 보통 테마에서 jekyll gemspec 명세를 제공하므로 jekyll 설치를 뒤로하고 테마 설정을 진행한다.\n\n\n\n\n본인은 Type on Strap 테마를 선택하였다.\n\n선택 이유?\n\n\n  현재까지도 유지보수가 되고 있는 점(선택 시점에 10시간 전에 릴리즈된 것을 확인)\n  컨텐츠 중심의 가독성 좋은 테마\n  반응형 웹\n  다크 테마 지원\n  mermaid, katex 등 다이어그램, 수식 툴 지원\n\n\n단점이라면 한글 폰트가 생각보다 크게 보인다는 것인데… 이후 폰트 및 사이즈 변경 예정이다.\n\n\n\nJekyll 설치 및 설정\n\n선행사항\n\n\n  Ruby 2.5.0 이상\n  RubyGems\n  GCC\n  Make\n\n\n1. Ruby 설치(macOS)\n\n# Homebrew 설치\n$ /bin/bash -c &quot;$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)&quot;\n\n# Ruby 설치\n$ brew install ruby\n# zsh 사용시\necho &#39;export PATH=&quot;/usr/local/opt/ruby/bin:/usr/local/lib/ruby/gems/3.0.0/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.zshrc\n# bash 사용시\necho &#39;export PATH=&quot;/usr/local/opt/ruby/bin:/usr/local/lib/ruby/gems/3.0.0/bin:$PATH&quot;&#39; &amp;gt;&amp;gt; ~/.bash_profile\n\n# 버전 확인\n$ ruby -v\n$ gem -v\n$ gcc -v\n\n\n다른 OS 설치 방법은 Jekyll 설치 문서 참조\n\n2. Jekyll 및 테마 설치\n\n$ cd ~/git/csupreme19.github.io\n$ git clone https://github.com/Sylhare/Type-on-Strap.git\n\n# install\n$ gem install jekyll\n$ bundle add webrick\n$ bundle install\n\n# jekyll 로컬 서버 실행\n$ bundle exec jekyll serve\nConfiguration file: /Users/csupreme19/git/csupreme19.github.io/_config.yml\n            Source: /Users/csupreme19/git/csupreme19.github.io\n       Destination: /Users/csupreme19/git/csupreme19.github.io/_site\n Incremental build: disabled. Enable with --incremental\n      Generating...\n       Jekyll Feed: Generating feed for posts\n                    done in 1.461 seconds.\n Auto-regeneration: enabled for &#39;/Users/csupreme19/git/csupreme19.github.io&#39;\n    Server address: http://127.0.0.1:4000/\n  Server running... press ctrl-c to stop.\n\n\n3. _config.yml 수정\n\n# SITE CONFIGURATION\nbaseurl: &quot;&quot;\t\t# 설정한 url이 subdomain이 된다. 즉 {url}/{baseurl} 도메인으로 접속해야함\nurl: &quot;https://csupreme19.github.io&quot;\t\t# 도메인 주소 설정\n\n# THEME-SPECIFIC CONFIGURATION\ntitle: 내 블로그                             # 타이틀\ndescription: &quot;한글테스트&quot;      # 구글 검색 엔진에서 사용하는 정보\navatar: assets/img/triangle.png                         # 상단 navbar 이미지\nfavicon: assets/favicon.ico                             # 웹 favicon\n\n# Header and footer text\nheader_text: 헤더 텍스트  # 블로그 헤더 텍스트\nheader_feature_image: assets/img/pexels/triangular.jpeg # 헤더 이미지\n# 푸터 텍스트\nfooter_text: &amp;gt;\n  Powered by &amp;lt;a href=&quot;https://jekyllrb.com/&quot;&amp;gt;Jekyll&amp;lt;/a&amp;gt; with &amp;lt;a href=&quot;https://github.com/sylhare/Type-on-Strap&quot;&amp;gt;Type on Strap&amp;lt;/a&amp;gt;\n  \n...\n# 나머지 설정은 github 문서 참조\n\n\n4. 로컬 접속 테스트\n\n\n\nhttp://localhost:4000 접속 확인\n\n5. git push 및 github pages 설정\n\n$ git add .\n$ git commit -m &quot;initial commit&quot;\n$ git push -u origin main\n\n\n\n\ngithub 저장소 &amp;gt; Settings &amp;gt; Pages\n\n빌드 소스 변경\n\nSource 부분 main 브랜치, / (root) 폴더로 변경\n\n변경 후 Your site is published at https://csupreme19.github.io/ 빌드 성공 메시지 확인\n\n6. 접속\n\nhttps://csupreme19.github.io/ 접속 확인\n\n\n\n댓글창 활성화\n\nType-on-Strap 테마는 3가지의 Comment 오픈소스를 지원한다.\n\n1. Disqus\n\ndisqus.com\n\n장점\n\n\n  로그인 안해도 댓글 달 수 있음\n\n\n단점\n\n\n  무겁다, 무료버전은 광고가 존재\n  개인적인 의견이지만 댓글창이 너저분해보인다.\n\n\n2. Cusdis\n\ncusdis.com\n\n장점\n\n\n  Disqus에 비해 매우 깔끔한 레이아웃\n\n\n단점\n\n\n  중국발이라 왠지 모를 거부감\n  disqus와 마찬가지로 무겁다.\n\n\n3. Utterance\n\nutterance.es\n\n장점\n\n\n  매우 깔끔한 레이아웃\n  성능이 위 2 오픈소스에 비해 좋다.\n  완전 무료 오픈소스로 광고 없음\n\n\n단점\n\n\n  댓글이 repo에 GitHub 이슈로 등록되는 구조라 GitHub 계정이 있어야만 댓글 가능\n\n\nUtterance를 사용하기로 하였다.\n\n일반적인 블로그가 아니라 GitHub Pages로 운영되는 GitHub 기반 블로그이며 주로 개발 내용을 다루기 때문에 GitHub 계정이 필요한 것은 큰 단점으로 다가오지 않았다.\n\n또한 GitHub Issue로 등록되므로 Webhook을 등록하여 Slack 알람을 받는등 Alert 기능도 활성화 가능하다고 생각하였다.\n\n\n\nUtterance 적용하기\n\n\n\n1. Public Repo 생성\n\nGithub pages repo가 public이므로 해당 repo 사용\n\n2. Utterance App 설치\n\nhttps://github.com/apps/utterances에서 설치\n\n3. _config.yml 설정\n\n# Comments\ncomments:\n  utterances:\n    repo: csupreme19/csupreme19.github.io\n    issue-term: comment\n\n\n위 두가지 설정만 하면 설정은 끝난다.\n\n4. 코멘트 적용 확인\n\n$ bundle exec jekyll serve\n\n\n\n\n\n\n폰트 변경\n\n맥 환경에서 봤을땐 폰트가 사이즈 말고는 괜찮았는데 윈도우 환경에서 보니 계단 현상이 존재하고 가독성이 떨어져 보이는 문제가 발생하였다.\n\n테마(템플릿)를 사용하는 이유가 UI 개발에 힘을 쏟기 싫었기 때문인데 어쩔 수 없이 입맛에 맞는 커스텀은 필요한 것 같다.\n\n1. 폰트 .scss 파일 생성\n\n일반 폰트는 케이티의 Y 너만을 비춤체, 소스 코드 폰트는 네이버의 D2 coding ligature를 사용하였다.\n\n$ cd _sass/external\n$ vim _y-spotlight.scss\n$ vim _d2-coding-ligature.scss\n\n\n폰트 배포시 별도의 @font-face 소스를 제공한다.\n\n없다면 기본 구성되어있는 _source-sans-pro.scss 파일을 복사하여 사용하자.\n\n// _y-spotlight.scss\n@font-face {\n    font-family: &#39;Y_Spotlight&#39;;\n    src: url(&#39;https://cdn.jsdelivr.net/gh/projectnoonnu/noonfonts-20-12@1.0/Y_Spotlight.woff&#39;) format(&#39;woff&#39;);\n    font-weight: normal;\n    font-style: normal;\n}\n\n// _d2-coding-ligature.scss\n@font-face {\n    font-family: &#39;D2 coding Ligature&#39;;\n    src: url(&#39;https://cdn.jsdelivr.net/gh/everydayminder/assets/subset-D2Codingligature.woff&#39;) format(&#39;woff&#39;);\n    font-weight: 400;\n    font-style: normal;\n}\n\n\n폰트 소스는 cdn을 사용하였는데 D2 coding 폰트의 경우 공식으로 CDN을 제공하고 있지 않는 것 같다.\n\n따라서 everyminder 에서 제공한 woff CDN을 사용하였다.\n\n2. _variables.scss 수정\n\n$ vim _sass/base/_variables.scss\n\n\n_variables.scss 파일에 테마 변수 정보들이 담겨있다.\n\n아래 부분처럼 위에서 추가했던 font-family를 추가한다.\n\n/* TYPOGRAPHY */\n$font-family-main: &#39;Y_Spotlight&#39;, &#39;Source Sans Pro&#39;, Helvetica, Arial, sans-serif;\n$font-family-headings: &#39;Y_Spotlight&#39;, &#39;Source Sans Pro&#39;, Helvetica, Arial, sans-serif;\n$font-family-logo: &#39;Y_Spotlight&#39;, &#39;Source Sans Pro&#39;, Helvetica, Arial, sans-serif;\n$font-size: 0.875em;\n\n$monospace: &#39;D2 coding ligature&#39;, Consolas, Menlo, Monaco, Lucida Console, Liberation Mono, DejaVu Sans Mono, Bitstream Vera Sans Mono, Courier New, monospace, sans-serif !default;\n$font-size-code: 0.85em !default;\n$font-height-code: 1.3em !default;\n$border-radius: 4px !default;\n\n\n하는 김에 글씨 크기가 커서 폰트 사이즈도 조정해 주었다.\n\n3. 확인\n\n\n\n폰트 적용이 된 것을 확인할 수 있다.\n\n\n\nReference\n\n\n  Jekyll Installation\n  GitHub Pages\n  Jekyll Themes\n  Type-on-Strap\n\n\n"
} ,
  
  {
    "title"    : "Kubernetes Pod 재시작 장애 확인하기",
    "category" : "",
    "tags"     : " Kubernetes, Pod, Fail",
    "url"      : "/2021/10/14/kubernetes-pod-fail-test.html",
    "date"     : "October 14, 2021",
    "excerpt"  : "Kubernetes Pod 재시작 장애 확인하기\n\nContainer restart policy\n\n강제로 죽는 pod를 배포 후 해당 파드가 어떻게 restart 되는지 살펴보았다.\n\n\n\n실험\n\n실험 절차\n\n1. Pod 배포\n\n# alias k=kubectl\n\n# Declarative way\n$ vim dummy-pod.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-pod\nspec:\n  c...",
  "content"  : "Kubernetes Pod 재시작 장애 확인하기\n\nContainer restart policy\n\n강제로 죽는 pod를 배포 후 해당 파드가 어떻게 restart 되는지 살펴보았다.\n\n\n\n실험\n\n실험 절차\n\n1. Pod 배포\n\n# alias k=kubectl\n\n# Declarative way\n$ vim dummy-pod.yaml\n\napiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-pod\nspec:\n  containers:\n    - name: dummy-pod\n      image: ubuntu\n  restartPolicy: Always\n  \n$ k create -f dummy-pod.yaml\npod/dummy-pod created\n\n# Imperative way\n$ k run dummy-pod --image ubuntu\npod/dummy-pod created\n\n\n2. Pod 상태 확인\n\n$ k get po\nNAME                       READY   STATUS              RESTARTS   AGE\ndummy-pod                  0/1     ContainerCreating   0          2s\n\n$ k get po\nNAME                       READY   STATUS      RESTARTS   AGE\ndummy-pod                  0/1     Completed   1          10s\n\n$ k get po\nNAME                       READY   STATUS      RESTARTS   AGE\ndummy-pod                  0/1     Completed   2          32s\n\n$ k get po\nNAME                       READY   STATUS      RESTARTS   AGE\ndummy-pod                  0/1     Completed   3          61s\n\n$ k get po\nNAME                       READY   STATUS      RESTARTS   AGE\ndummy-pod                  0/1     Completed   4          118s\n\n$ k get po\nNAME                       READY   STATUS             RESTARTS   AGE\ndummy-pod                  0/1     CrashLoopBackOff   4          2m34s\n\n$ k get po\nNAME                       READY   STATUS      RESTARTS   AGE\ndummy-pod                  0/1     Completed   5          3m33s\n\n\n10, 30, 60, 110, 210초에 각각 restart되는 것으로 확인\n\n공식 문서에 따르면 지수 백오프 지연(10초, 20초, 40초, …)로 10초로 시작하여 2배씩 재시작 간격이 증가한다고 함(최대 300초(5분))\n\n지수배는 아니지만 비슷하게 재시작 된 것을 확인할 수 있었다.\n\n3. 결론\n\n아래와 같이 다양한 기준을 적용하여 파드가 계속 재시작중인지 restart 횟수 만으로 판별 가능\n\n  파드 배포 후 1분 내 2번 이상(10, 30, 60초)\n  파드 배포 후 3분 내 4번 이상(10, 30, 60, 110, 210초)\n\n\n\n\nReference\n\n\n  Container restart policy\n\n"
} ,
  
  {
    "title"    : "Kubernetes RBAC Authorization 개요 및 적용",
    "category" : "",
    "tags"     : " Kubernetes, Authorization, Authentication, RBAC, ABAC, Security",
    "url"      : "/2021/10/01/kubernetes-RBAC-auth.html",
    "date"     : "October 1, 2021",
    "excerpt"  : "Kubernetes RBAC Authorization 개요 및 적용\n\n\n\nUsing RBAC Authorization\n\n본 문서에서는 쿠버네티스 API 서버에 접근하기 위한 4가지 인증 방식을 살펴보고 그 중 RBAC 인증 적용방법에 대하여 정리하였다.\n\n\n\n인가(Authentication) vs 인증(Authorization)\n\n\n\n인가(Authentication)\n\n해당 사용자가 누구인지 확인하는 것(회원가입, 로그인)\n\n인증(Auth...",
  "content"  : "Kubernetes RBAC Authorization 개요 및 적용\n\n\n\nUsing RBAC Authorization\n\n본 문서에서는 쿠버네티스 API 서버에 접근하기 위한 4가지 인증 방식을 살펴보고 그 중 RBAC 인증 적용방법에 대하여 정리하였다.\n\n\n\n인가(Authentication) vs 인증(Authorization)\n\n\n\n인가(Authentication)\n\n해당 사용자가 누구인지 확인하는 것(회원가입, 로그인)\n\n인증(Authorization)\n\n해당 사용자에 대한 권한을 허락하는 것(호, 자원 접근)\n\n\n\nKubernetes 인증 방식\n\nKubernetes API 서버에 접근하기 위해서는 인증 단계가 필요하다.\n\n쿠버네티스에서는 인증 방식이 크게 4가지가 존재한다.\n\n\n  Node Authorization\n  ABAC Authorization\n  RBAC Authorization\n  Webhook Authorization\n\n\nAPI Server의 --authorization-mode  flag를 확인하여 현재 활성화된 인증 모드를 확인할 수 있다.\n\n$ cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep -i authorization\n    - --authorization-mode=Node,RBAC\n\n\n기본적으로 Node, RBAC 인증 방식이 활성화되어 있는 것을 확인할 수 있다.\n\n\n\nNode Authorization\n\n\nflowchart LR\n  A[User]\n  B[Kube API]\n  C[&quot;KubeletNode&quot;]\n  subgraph Kubernetes Cluster\n    C--Node Authorization--&amp;gt;B\n  end\n    A--&amp;gt;B\n\n\nKubernetes Clsuter에 속하는 Node들은 Kubelet에서 API 서버에 요청할 때 TLS 인증을 이용한다.\n\n이 때 Kubelet의 Group은 system:node에 속해 있으며 해당 그룹에 속해있는 인증 요청은 Node Authorizer에 의하여 인증된다.\n\n이 방식은 보통 Kubernetes TLS 부트스트랩 과정에서 자동으로 설정되므로 더 자세한 내용은 공식 문서 참고\n\n\n\nABAC(Attribute Based Access Control)\n\nJSON 형식의 Policy 정의를 사용하여 해당 사용자를 인증하는 방식\n\nExamples\n\n\nflowchart LR\n  A[User: admin]\n  B[Group: system:authenticated]\n  C[Kube API]\n  subgraph Kuberntes Cluster\n  A--ABAC--&amp;gt;C\n  B--ABAC--&amp;gt;C\n  end\n\n\n{&quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;, &quot;kind&quot;: &quot;Policy&quot;, &quot;spec&quot;: {&quot;user&quot;:&quot;admin&quot;,     &quot;namespace&quot;: &quot;*&quot;,              &quot;resource&quot;: &quot;*&quot;,         &quot;apiGroup&quot;: &quot;*&quot;                   }}\n{&quot;apiVersion&quot;: &quot;abac.authorization.kubernetes.io/v1beta1&quot;, &quot;kind&quot;: &quot;Policy&quot;, &quot;spec&quot;: {&quot;group&quot;:&quot;system:authenticated&quot;,  &quot;nonResourcePath&quot;: &quot;*&quot;, &quot;readonly&quot;: true}}\n\n\n기본적으로 비활성화 되어 있으며 kube-apiserver에 --authorization-mode=ABAC, --authorization-policy-file=파일명 설정을 추가하여야 한다.\n\nPoilicy 정의 후 API 서버를 재시작해야하고 접근권한을 파일로 정의하기 때문에 후술할 RBAC에 비하여 관리하기가 어렵다는 단점이 있다.\n\n\n\nRBAC(Role Based Access Control)\n\n사용자, 서비스의 접근 권한(인증)을 Role(ClusterRole)과 RoleBinding(ClusterRoleBinding) 자원에 기반하여 처리하는 방식으로 일반적으로 가장 많이 사용하고 관리하기 쉬운 인증 방식이다.\n\n해당 User, SA(Service Account), Group등이 RoleBinding에 의하여 어떤 접근권한을 가지고 있는지 인증된다.\n\n\nflowchart LR\n  A[User]\n  B[Group]\n    subgraph Kubernetes Cluster\n    C[Service Account]\n  D[Role: Developer]\n  E[Role: Production]\n  end\n  A--RBAC--&amp;gt;D &amp;amp; E\n  B--RBAC--&amp;gt;D\n  C--RBAC--&amp;gt;E\n\n\n\nWebhook Authorization\n\nKubernetes 내부에서 제공하는 인증이 아닌 외부의 인증 정책을 사용하기 위한 방식\n\nOpen Policy Agent와 같은 외부 오픈소스, Admission Controller를 사용할 때 사용한다.\n\n\nflowchart LR\n  A[User]\n    subgraph Kubernetes Cluster\n  B[Kube API]\n    end\n  C[Open Policy Agent]\n  A--&amp;gt;B\n  B--Authorization--&amp;gt;C\n  C-.Authorization.-&amp;gt;B\n\n\n\n\nRBAC 리소스\n\n1. Role / ClusterRole\n\n\nflowchart LR\n  A[RoleCan view PodsCan watch PodsCan list Pods]\n  A\n\n\n어떤 리소스에 어떤 호출이 가능한지 권한/역할을 정의한 리소스이다.\n\nRole과 ClusterRole이 있으며 ClusterRole은 Role과 달리 클러스터 레벨로 가지고 있어 네임스페이스가 존재하지 않는다.\n\nRole 예제\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: Role\nmetadata:\n  namespace: default\n  name: pod-reader\nrules:\n- apiGroups: [&quot;&quot;] # &quot;&quot; indicates the core API group\n  resources: [&quot;pods&quot;]\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  # &quot;namespace&quot; omitted since ClusterRoles are not namespaced\n  name: secret-reader\nrules:\n- apiGroups: [&quot;&quot;]\n  resources: [&quot;secrets&quot;]\n  verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;]\n\n\nResources에 해당하는 자원의 verb에 해당하는 요청이 가능하다.\n\npod-reader를 예로 들면 pods를 get, watch, list 요청이 가능하다.\n\nkubernetes의 자원 종류는 아래 명령어로 확인 가능하다.\n\n$ kubectl api-resources\nNAME                              SHORTNAMES   APIVERSION                             NAMESPACED   KIND\nbindings                                       v1                                     true         Binding\ncomponentstatuses                 cs           v1                                     false        ComponentStatus\nconfigmaps                        cm           v1                                     true         ConfigMap\nendpoints                         ep           v1                                     true         Endpoints\nevents                            ev           v1                                     true         Event\nlimitranges                       limits       v1                                     true         LimitRange\nnamespaces                        ns           v1                                     false        Namespace\nnodes                             no           v1                                     false        Node\npersistentvolumeclaims            pvc          v1                                     true         PersistentVolumeClaim\npersistentvolumes                 pv           v1                                     false        PersistentVolume\n...\n\n\nRole 확인\n\n$ kubectl get role -n kube-system\n$ kubectl get clusterrole -n\n$ kubectl describe role/kube-proxy -n kube-system\n$ kubectl describe clusterrole/cluster-admin\nName:         cluster-admin\nLabels:       kubernetes.io/bootstrapping=rbac-defaults\nAnnotations:  rbac.authorization.kubernetes.io/autoupdate: true\nPolicyRule:\n  Resources  Non-Resource URLs  Resource Names  Verbs\n  ---------  -----------------  --------------  -----\n  *.*        []                 []              [*]\n             [*]                []              [*]\n\n\n2. ServiceAccount\n\n파드로 올라가있는 서비스에서 kubernetes 클러스터에 접근하기 위한 계정을 나타내는 리소스다.\n\n\n\n\n  이미지: Certified Kubernetes Administrator(CKA) with Practice Tests 발췌\n\n\n쉽게 말해 사용자 계정이 아닌 서비스의 계정이다.\n\n후술할 추상적인 User, Group과 달리 Kubernetes 자원 형태로 실존한다.\n\n\nflowchart LR\n  A[Service Account]\n  B[Secret]\n  A-.token.-B\n\n\nSA 예제\n\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: monitoring-user\n  namespace: kubernetes-dashboard\n---\n\nSA 확인\n\n$ kubectl get sa -n kube-system\n\n\nservice account 생성시 해당 서비스 어카운트의 토큰을 가지고 있는 secret이 자동 생성된다.\n\n서비스 어카운트 사용방법\n\n$ kubectl describe sa/monitoring-user -n kubernetes-dashboard\nName:                monitoring-user\nNamespace:           kubernetes-dashboard\nLabels:              &amp;lt;none&amp;gt;\nAnnotations:         &amp;lt;none&amp;gt;\nImage pull secrets:  &amp;lt;none&amp;gt;\nMountable secrets:   monitoring-user-token-r6nss\nTokens:              monitoring-user-token-r6nss\nEvents:              &amp;lt;none&amp;gt;\n\n\nservice account 생성시 해당 서비스 어카운트의 토큰을 가지고 있는 secret이 자동 생성된다.\n\n토큰 확인\n\n$ kubectl describe secret monitoring-user-token-r6nss -n kubernetes-dashboard\nName:         monitoring-user-token-r6nss\nNamespace:    kubernetes-dashboard\nLabels:       &amp;lt;none&amp;gt;\nAnnotations:  kubernetes.io/service-account.name: monitoring-user\n              kubernetes.io/service-account.uid: cca1bad1-fe4f-430c-978b-52d7f7ea53c5\n\nType:  kubernetes.io/service-account-token\n\nData\n====\nca.crt:     1066 bytes\nnamespace:  20 bytes\ntoken:      eyJhbGciOiJSUzI1NiIsImtpZCI6IlVmM2...\n\n# 참고) 한번에 토큰 조회\n$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/monitoring-user -o jsonpath=&quot;{.secrets[0].name}&quot;) -o go-template=&quot;&quot;\neyJhbGciOiJSUzI1NiIsImtpZCI6IlVmM2...\n\n\nAPI 호출시 Authorization 헤더에 해당 토큰을 넣어서 요청하면 인증을 할 수 있다.\n\n자원에 적용\n\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      containers:\n        - name: dashboard-metrics-scraper\n          image: kubernetesui/metrics-scraper:v1.0.6\n          ...\n      serviceAccountName: kubernetes-dashboard\t# 해당 부분 추가\n      ...\n\n\nPod 명세 serviceAccountName 에 위에서 생성한 SA 추가하면 해당 Pod는 API Server에 정의된 권한을 가지고 접근할 수 있다.\n\n요청 예시\n\n# 직접 호출시 토큰 명시\n$ curl -H &quot;Authorization: Bearer eyJhbGciOiJSUzI1...&quot; -ivk https://10.213.196.211:6443 \n\n\n3. RoleBinding\n\n위에서 생성한 ServiceAccount가 실질적으로 권한을 가지려면 해당 어카운트가 어느 자원에 접근할 수 있는지 허락하는 단계가 필요하다.\n\n이것을 인증(Authorization)이라고 한다.\n\nkubernetes에서는 RBAC, 즉 Role 역할을 기반으로하여 인증을 하기 때문에 Role을 ServiceAccount / User / Group에 바인딩하는 방법을 사용하며 이를 나타내기 위한 리소스가 RoleBinding이다.\n\nRoleBinding과 ClusterRoleBinding이 있으며 ClusterRole의 경우 ClusterRoleBinding을 이용한다.\n\n각각의 Role은 ServiceAccount, User, Group에 바인딩 될 수 있다.\n\nUser / Group\n\nServiceAccount와 달리 Kubernetes API server의 User와 Group은 별도로 정의된 Kubernetes Resource가 아니다.\n\nAPI server에 접근하기 위한 아래와 같은 보안 설정 파일에 정의되어 있는 User와 Group이라는 추상적인 개념이다.\n\n\n  Static Password file\n  Static Token file\n  Certificates\n\n\n보통은 인증서 설정에 User와 Group 정보가 정의되어 있으며 system:  으로 시작하는 그룹은 미리 정의된 그룹이다.\n\nUser 정보는 kubeconfig에서 확인할 수 있다.\n\n$ kubectl config view\n\n\napiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: DATA+OMITTED\n    server: https://10.213.196.211:6443\n  name: kubernetes\ncontexts:\n- context:\n    cluster: kubernetes\n    user: kubernetes-admin\n  name: kubernetes-admin@kubernetes\ncurrent-context: kubernetes-admin@kubernetes\nkind: Config\npreferences: {}\nusers:\n- name: kubernetes-admin\n  user:\n    client-certificate-data: REDACTED\n    client-key-data: REDACTED\n\n\n\nflowchart LR\n  A[User]\n  B[Group]\n  C[Service Account]\n  D[Role]\n  E[Secret]\n  subgraph RoleBinding\n  A &amp;amp; B &amp;amp; C---D\n  end\n  C -.token.- E\n\n\nRoleBinding 예제\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: RoleBinding\nmetadata:\n  name: read-pods\n  namespace: default\nsubjects:\n- kind: User\n  name: jane\n  apiGroup: rbac.authorization.k8s.io\nroleRef:\n  kind: Role\n  name: pod-reader\n  apiGroup: rbac.authorization.k8s.io\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: monitoring-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-monitoring\nsubjects:\n- kind: ServiceAccount\n  name: monitoring-user\n  namespace: kubernetes-dashboard\n---\n\n\n  ClusterRoleBinding 역시 클러스터 레벨이므로 네임스페이스가 따로 없음\n\n\nRoleBinding 확인\n\n$ kubectl get rolebinding\n$ kubectl get clusterrolebinding\n$ kubectl get pods --as jane\t# User 권한 확인\n\n\n\nRBAC 사용 예시\n\n\n  시나리오: k8s dashboard에 접근하기 위한 모니터링 인증을 생성한다.\n\n\n1. kubernetes-dashboard-rbac.yaml 작성\n\nServiceAccount, ClusterRole, ClusterRoleBinding을 설정한다.\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: monitoring-user\n  namespace: kubernetes-dashboard\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-monitoring\nrules:\n- apiGroups: [&quot;*&quot;]\n  resources: [&quot;namespaces&quot;, &quot;deployments&quot;, &quot;replicasets&quot;, &quot;pods&quot;, &quot;pods/log&quot;]\n  verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRoleBinding\nmetadata:\n  name: monitoring-user\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: cluster-monitoring\nsubjects:\n- kind: ServiceAccount\n  name: monitoring-user\n  namespace: kubernetes-dashboard\n\n\nCluster의 자원이 아닌 헬스체크등 URI로 API를 호출해야하는 경우 아래와 같이 resources 대신 nonResourceURLs를 사용한다.\n\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: cluster-monitoring\nrules:\n- nonResourceURLs: [&quot;/healthz*&quot;, &quot;/livez*&quot;, &quot;/readyz*&quot;, &quot;/version*&quot;]\n  verbs: [&quot;get&quot;]\n\n\n해당 ClusterRole을 가진 SA의 시크릿 토큰을 이용하여 위 URL에 해당하는 API를 호출할 수 있다.\n\n2. Token 확인\n\n$ kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/monitoring-user -o jsonpath=&quot;{.secrets[0].name}&quot;) -o go-template=&quot;&quot;\n\n\n3. Kubernetes 클러스터 접근 테스트\n\n$ curl -H &quot;Authorization: Bearer eyJhbGciOiJSUzI1...&quot; -k https://10.213.196.211:6443/livez\\?verbose\n[+]ping ok\n[+]log ok\n[+]etcd ok\n[+]poststarthook/start-kube-apiserver-admission-initializer ok\n[+]poststarthook/generic-apiserver-start-informers ok\n[+]poststarthook/priority-and-fairness-config-consumer ok\n[+]poststarthook/priority-and-fairness-filter ok\n[+]poststarthook/start-apiextensions-informers ok\n[+]poststarthook/start-apiextensions-controllers ok\n[+]poststarthook/crd-informer-synced ok\n[+]poststarthook/bootstrap-controller ok\n[+]poststarthook/rbac/bootstrap-roles ok\n[+]poststarthook/scheduling/bootstrap-system-priority-classes ok\n[+]poststarthook/priority-and-fairness-config-producer ok\n[+]poststarthook/start-cluster-authentication-info-controller ok\n[+]poststarthook/aggregator-reload-proxy-client-cert ok\n[+]poststarthook/start-kube-aggregator-informers ok\n[+]poststarthook/apiservice-registration-controller ok\n[+]poststarthook/apiservice-status-available-controller ok\n[+]poststarthook/kube-apiserver-autoregistration ok\n[+]autoregister-completion ok\n[+]poststarthook/apiservice-openapi-controller ok\nlivez check passed\n\n# 참고) 실패시\n{\n  &quot;kind&quot;: &quot;Status&quot;,\n  &quot;apiVersion&quot;: &quot;v1&quot;,\n  &quot;metadata&quot;: {\n  },\n  &quot;status&quot;: &quot;Failure&quot;,\n  &quot;message&quot;: &quot;Unauthorized&quot;,\n  &quot;reason&quot;: &quot;Unauthorized&quot;,\n  &quot;code&quot;: 401\n}\n\n\n\n\n요약\n\nKubernetes API 서버에 접근하기 위한 인증방식은 크게 4가지가 있다.\n\n\n  Node\n  ABAC\n  RBAC\n  Webhook\n\n\n일반적으로 사용하는 인증 방식으로는 RBAC가 있으며 Role에 기반하여 인증하는 방식이다.\n\n전체적인 구조를 보자면 아래와 같다. (ClusterRole도 동일)\n\n\nflowchart LR\n  A[User]\n  B[Group]\n  C[Service Account]\n  D[Role]\n  subgraph Kubernetes Cluster\n  E[Secret]\n  J[Pod]\n  F[Kube API]\n  RoleBinding\n  end\n  subgraph RoleBinding\n  A &amp;amp; B &amp;amp; C---D\n  end\n  C -.token.- E\n  C --&amp;gt; J\n  J --Request--&amp;gt; F\n  F -.Response.-&amp;gt; J\n\n\n\n\nReference\n\n\n  Using RBAC Authorization\n  Using Node Authorization\n  Using ABAC Authorization\n  Certified Kubernetes Administrator(CKA) with Practice Tests\n\n"
} ,
  
  {
    "title"    : "Elasticsearch Filebeat 모듈 설정",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Filebeat, Beat",
    "url"      : "/2021/08/06/elastic-filebeat-modules.html",
    "date"     : "August 6, 2021",
    "excerpt"  : "Elastic Filebeat 모듈 설정\n\n\n\nFilebeat Modules\n\n로그 정보를 수집하는 Filebeat의 모듈별 Output 설정방법을 정리하였다.\n\n\nModule 설정\n\n\nFilebeat 설치\n\nElastic Filebeat 설치 및 설정 참고\n\n\nModule 확인\n\n데이터를 수집하기 위한 filebeat의 module list 확인\n\n$ filebeat modules list\n\n\nSystem module 설정\n$ fileb...",
  "content"  : "Elastic Filebeat 모듈 설정\n\n\n\nFilebeat Modules\n\n로그 정보를 수집하는 Filebeat의 모듈별 Output 설정방법을 정리하였다.\n\n\nModule 설정\n\n\nFilebeat 설치\n\nElastic Filebeat 설치 및 설정 참고\n\n\nModule 확인\n\n데이터를 수집하기 위한 filebeat의 module list 확인\n\n$ filebeat modules list\n\n\nSystem module 설정\n$ filebeat modules enable system\n\n\n\nKafka module 설정\n\nKafka가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable postgresql\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/kafka.yml 수정\n\n- module: kafka\n  log:\n    enabled: true\n    \n    var.paths:\n      - &quot;/data/kafka-logs/controller.log*&quot;\n      - &quot;/data/kafka-logs/server.log*&quot;\n      - &quot;/data/kafka-logs/state-change.log*&quot;\n      - &quot;/data/kafka-logs/kafka-*.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-kafka.html\n\n\nkafka dashboards\n\n\n\nNginx module 설정\n\nNginx가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable nginx\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/nginx.yml 수정\n\n- module: nginx\n  # Access logs\n  access:\n    enabled: true\n    var.paths: \n      - &quot;/var/log/nginx/access.log*&quot;\n\n  # Error logs\n  error:\n    enabled: true\n      - &quot;/var/log/nginx/error.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-nginx.html\n\n\nnginx dashboards\n\n\n\n\n\n추가 사항\n\naccess.log에 관리자 페이지 관련하여 access_log가 많이 쌓이는 경우\n\n아래와 같이 access_log off; 추가\n\n$ cd /etc/nginx/conf.d\n$ vim management.conf\n        location / {\n                access_log off;\n        }\n\n\n\nPostgreSQL module 설정\n\nPostgreSQL이 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable postgresql\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/postgresql.yml 수정\n\n- module: postgresql\n  log:\n    enabled: true\n    var.paths:\n      - &quot;/data/postgres/pgdata/log/*.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-postgresql.html\n\n\npostgresql dashboards\n\n\n\n\nMySQL module 설정\n\nMySQL이 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable mysql\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/mysql.yml 수정\n\n- module: mysql\n  error:\n    enabled: true\n    var.paths:\n      - &quot;/data/mysql/logs/error.log*&quot;\n\n  slowlog:\n    enabled: true\n    var.paths:\n      - &quot;/data/mysql/logs/mysql-slow.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-mysql.html\n\n\nmysql dashboards\n\n\n\n\nMongoDB module 설정\n\nMongoDB가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable mongodb\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/mongodb.yml 수정\n\n- module: mongodb\n  log:\n    enabled: true\n    var.paths:\n      - &quot;/data/mongo/logs/*.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-mongodb.html\n\n\nmongodb dashboards\n\n\n\n\nRedis module 설정\n\nRedis가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ filebeat modules enable redis\n\n\n모듈 설정\n\n/etc/filebeat/modules.d/redis.yml 수정\n\n- module: redis\n  log:\n    enabled: true\n    var.paths:\n      - &quot;/data/redis/redis-server.log*&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/filebeat/7.x/filebeat-module-redis.html\n\n\nredis dashboards\n\n\n\n\nFilebeat setup\n\n# dashboard 생성을 위해 /usr/share/filebeat 이동하여 실행\n$ cd /usr/share/filebeat\n\n# -e 옵션으로 error 여부를 stdout 으로 출력. log에서 host에 정상 접속 했는지 확인 가능\n$ filebeat setup -e -c /etc/filebeat/filebeat.yml --dashboards\n\n\nsetup 완료 후 마지막 문장에 dashboard가 정상 로딩된 것을 확인\n\n\n\nFilebeat 구동\n\n$ systemctl start filebeat.service\n\n\nFilebeat log 확인\n\n$ journalctl -u filebeat\n\n\n\n\nReference\n\n\n  Filebeat Modules\n\n\n"
} ,
  
  {
    "title"    : "Elastic Filebeat 설치 및 설정",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Filebeat, Beat",
    "url"      : "/2021/08/05/elastic-filebeat-install.html",
    "date"     : "August 5, 2021",
    "excerpt"  : "Elastic Filebeat 설치 및 설정\n\n\n\nFilebeat Overview\n\n로그 정보를 수집하는 Filebeat를 각 VM(Ubuntu)에 설치하여 로그 파일을 Elasticsearch로 전송한다.\n\n\nFilebeat란\n\n\n\nElastic Stack에 포함되는 오픈소스로 파일 데이터와 로그 데이터를 경량화된 방식으로 수집하고 Logstash, Elasticsearch, Kibana 등으로 전달하는 수집기\n\n\nFilebeat 설치\n...",
  "content"  : "Elastic Filebeat 설치 및 설정\n\n\n\nFilebeat Overview\n\n로그 정보를 수집하는 Filebeat를 각 VM(Ubuntu)에 설치하여 로그 파일을 Elasticsearch로 전송한다.\n\n\nFilebeat란\n\n\n\nElastic Stack에 포함되는 오픈소스로 파일 데이터와 로그 데이터를 경량화된 방식으로 수집하고 Logstash, Elasticsearch, Kibana 등으로 전달하는 수집기\n\n\nFilebeat 설치\n\n# 설치\n$ curl -L -O https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.12.1-amd64.deb\n$ dpkg -i filebeat-7.12.1-amd64.deb\n\n# 설치 후 설치된 버전 upgrade 막기\n$ apt-mark hold filebeat\n\n# bin path를 .zshrc 파일에 추가 후 적용\nexport PATH=/usr/share/filebeat/bin:$PATH\n$ source .zshrc\n\n# filebeat 설정\n$ cd /etc/filebeat\n$ vim filebeat.yml\n\n# system 재부팅 시 자동 실행 설정\n$ systemctl enable filebeat\n\n\nyaml에 elasticsearch, kibana 호스트 정보 설정\n\n# =================================== Kibana ===================================\nsetup.kibana:\n  host: &quot;10.213.196.6:5601&quot;\n# ---------------------------- Elasticsearch Output ----------------------------\noutput.elasticsearch:\n  hosts: [&quot;10.213.196.68:9200&quot;,&quot;10.213.196.23:9200&quot;,&quot;10.213.196.44:9200&quot;]\n  protocol: &quot;http&quot;\n  username: &quot;elastic&quot;\n  password: &quot;elastic&quot;\n# ---------------------------- Filebeat inputs ----------------------------\nfilebeat.inputs:\n- type: log\n  enabled: false\n\n\nelasticsearch https 보안 설정 되어 있을 경우\n\n# ---------------------------- Elasticsearch Output ----------------------------\noutput.elasticsearch:\n  hosts: [&quot;10.213.196.68:9200&quot;,&quot;10.213.196.23:9200&quot;,&quot;10.213.196.44:9200&quot;]\n  protocol: &quot;https&quot;\n  username: &quot;elastic&quot;\n  password: &quot;elastic&quot;\n  \n  # ssl verification을 하지 않거나 인증서를 수동으로 등록하여야한다. 아래 택 1\n  \n  # ssl 검증하지 않기\n  ssl.verification_mode: &quot;none&quot;\n  \n  # ssl 인증서\n    ssl:\n    certificate_authorities: [&quot;/etc/elasticsearch/certs/ca.crt&quot;]\n    certificate: &quot;/etc/elasticsearch/certs/data1.crt&quot;\n    key: &quot;/etc/elasticsearch/certs/data1.key&quot;\n\n\n\nFilebeat 구동 및 확인\n\n$ cd /usr/share/filebeat\n$ filebeat setup -e -c /etc/filebeat/filebeat.yml --dashboards\n$ systemctl start filebeat\n$ systemctl status filebeat\n\n\n로그 확인\n\n$ journalctl -u filebeat\n\n\n\n\nFilebeat 모듈 설정\n\nElastic Filebeat 모듈 설정 참고\n\n\n\nReference\n\n\n  Filebeat Overview\n\n"
} ,
  
  {
    "title"    : "Elasticsearch Metricbeat 모듈 설정",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Metricbeat, Beat",
    "url"      : "/2021/08/02/elastic-metricbeat-modules.html",
    "date"     : "August 2, 2021",
    "excerpt"  : "Elastic Metricbeat 모듈 설정\n\n\n\nMetricbeat Modules\n\n메트릭 정보를 수집하는 Metricbeat의 모듈별 Output 설정방법을 정리하였다.\n\n\n\nModule 설정\n\n\nMetricbeat 설치\n\nMetricbeat 설치 및 설정 참고\n\n\nModule 확인\nmetric을 수집하기 위한 metricbeat의 module들 list 확인\n$ metricbeat modules list\n\n\nSystem module ...",
  "content"  : "Elastic Metricbeat 모듈 설정\n\n\n\nMetricbeat Modules\n\n메트릭 정보를 수집하는 Metricbeat의 모듈별 Output 설정방법을 정리하였다.\n\n\n\nModule 설정\n\n\nMetricbeat 설치\n\nMetricbeat 설치 및 설정 참고\n\n\nModule 확인\nmetric을 수집하기 위한 metricbeat의 module들 list 확인\n$ metricbeat modules list\n\n\nSystem module 설정\n# 필요 시 system 모듈 설정\n$ vi /etc/metricbeat/modules.d/system.yml\n\n# 추가 module 없이 활성화를 하면 system metric만 수집함.\n$ metricbeat modules enable\n\n\n\nPostgresql module 설정\n\nPostgres이 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable postgresql\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/postgresql.yml 수정\n\n- module: postgresql\n  enabled: true\n  metricsets:\n    - database\n    - bgwriter\n    - activity\n    - statement\n  period: 10s\n  hosts: [&quot;postgres://10.213.196.207:5432?sslmode=disable&quot;]\n  username: postgres\n  password: {암호}\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/current/metricbeat-module-postgresql.html\n\n\npostgresql dashboards\n\n\n\nRedis module 설정\n\nRedis가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable redis\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/redis.yml 수정\n\n- module: redis\n  metricsets:\n    - info\n    - keyspace\n  period: 10s\n\n  hosts: [&quot;127.0.0.1:6379&quot;]\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-redis.html\n\n\nredis dashboards\n\n\n\n\nMongodb module 설정\n\nMongodb가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable mongodb\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/mongodb.yml 수정\n\n- module: mongodb\n  metricsets:\n    - dbstats\n    - status\n    - collstats\n    - metrics\n  period: 10s\n\n  hosts: [&quot;localhost:27017&quot;]\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-mongodb.html\n\n\nmongodb dashboards\n\n\n\n\nMySQL module 설정\n\nMySQL이 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable mysql\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/mysql.yml 수정\n\n- module: mysql\n  metricsets:\n    - status\n    - performance\n    \n  period: 10s\n  \n  hosts: [&quot;metricbeat:rmakers2021@tcp(127.0.0.1:13306)/&quot;]\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-mysql.html\n\n\n접속 에러시\n\nmysql의 root 계정이 localhost에서만 접속하도록 되어 있는지 확인\n\nex) docker의 경우 컨테이너 접속시 host ip로 접속\n# host ip 확인\n$ ifconfig\n\n\nmysql 접속하여 계정 생성 및 권한부여\n$ docer exec -it mysql bash\nmysql&amp;gt; select host, user from mysql.user;\n+-----------+------------------+\n| host      | user             |\n+-----------+------------------+\n| %         | robot            |\n| localhost | healthchecker    |\n| localhost | mysql.infoschema |\n| localhost | mysql.session    |\n| localhost | mysql.sys        |\n| localhost | robot            |\n| localhost | root             |\n+-----------+------------------+\n7 rows in set (0.00 sec)\n\nmysql&amp;gt; create user &#39;metricbeat&#39;@&#39;172.18.0.1&#39; identified by &#39;rmakers2021&#39;;\nQuery OK, 0 rows affected (0.02 sec)\n\nmysql &amp;gt; grant all privilieges on *.* to &#39;metricbeat&#39;@&#39;172.18.0.1&#39; identified by &#39;rmakers2021&#39;;\nmysql &amp;gt; flush privileges;\n\n\n\n\n  자세한 내용은 외부 접속 host 권한 설정 참조\n\n\nmysql dashboards\n\n\n\n\nNginx module 설정\n\nNginx가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable nginx\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/nginx.yml 수정\n\n- module: nginx\n  metricsets: [&quot;stubstatus&quot;]\n  enabled: true\n  period: 10s\n\n  # Nginx hosts\n  hosts: [&quot;http://127.0.0.1:9000&quot;]\n\n  # Path to server status. Default server-status\n  server_status_path: &quot;server-status&quot;\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-nginx.html\n\n\nnginx의 ngx_http_stub_status_module 모듈 활성화\n\n$ cd /etc/nginx/conf.d\n$ vim status.conf\n    server {\n        listen 9000;\n\n\t\tlocation /server-status {\n          stub_status;\t# stub_status 활성화\n          access_log off;\t# 접속 로그 비활성화\n          allow 127.0.0.1;\t# localhost에서만 접속 허용\n          deny all;\n\t\t}\n    }\n$ chmod 750 status.conf\n$ chown nginx:nginx status.conf\n$ nginx -s reload\n\n\n\n  metricbeat가 10초마다 localhost:9000/server-status를 호출하여 stub status 정보 가져감\n\n\nnginx dashboards\n\n\n\n\n\n\n\n\nKafka module 설정\n\nKafka가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable kafka\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/kafka.yml 수정\n\n- module: kafka\n  metricsets:\n    - partition\n    - consumergroup\n    - broker\n    - consumer\n    - producer\n  period: 10s\n  hosts: [&quot;localhost:9092&quot;]\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-kafka.html\n\n\nKafka with jolokia\n\nbroker, consumer, producer metricset을 가져오려면 jolokia를 이용하여 jmx 모니터링하여야한다.\n\nkafka에 jolokia가 javaagent로 붙어서 jvm 위에 실행\n\nhttps://dev.to/martinhynar/monitoring-kafka-brokers-using-jolokia-metricbeat-and-elasticsearch-5678 참조\n\nkafka dashboards\n\n\n\n\nZookeeper module 설정\n\nZookeeper가 설치되어 있는 VM에 진행\n\n모듈 활성화\n\n$ metricbeat modules enable zookeeper\n\n\n모듈 설정\n\n/etc/metricbeat/modules.d/zookeeper.yml 수정\n\n- module: zookeeper\n  metricsets:\n    - connection\n    - mntr\n    - server\n  period: 10s\n  hosts: [&quot;localhost:2181&quot;]\n\n\n\n  ref: https://www.elastic.co/guide/en/beats/metricbeat/7.x/metricbeat-module-zookeeper.html\n\n\nzookeeper dashboards\n\n\n\n\nMetricbeat setup\n# dashboard 생성을 위해 /usr/share/metricbeat 이동하여 실행\n$ cd /usr/share/metricbeat\n\n# -e 옵션으로 error 여부를 stdout 으로 출력. log에서 host에 정상 접속 했는지 확인 가능\n$ metricbeat setup -e -c /etc/metricbeat/metricbeat.yml --dashboards\n\n\nsetup 완료 후 마지막 문장에 dashboard가 정상 로딩된 것을 확인\n\n\n\nMetricbeat 구동\n\n$ systemctl start metricbeat.service\n\n\nMetricbeat log 확인\n\n$ journalctl -u metricbeat\n\n\n\n\nReference\n\n\n  Metricbeat Modules\n\n\n"
} ,
  
  {
    "title"    : "Elastic Stack Observability 개요",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Kibana, Logstash, Monitoring, Observability",
    "url"      : "/2021/07/27/elastic-observability.html",
    "date"     : "July 27, 2021",
    "excerpt"  : "Elastic Stack Observability 개요\n\n\n\nObservability with the elastic stack\n\nElastic Stack을 이용하여 MSA 서비스 모니터링시 모니터링 중앙 집중화를 통한 가관측성(Observability)에 대하여 알아본다.\n\n\n모니터링의 3요소\n\n\n    \n    \n        \n    \n    \n\n\n\n\n운영시 모니터링 요소는 Logs, Metrics, APM 크게 3가지로 분류할 수 있...",
  "content"  : "Elastic Stack Observability 개요\n\n\n\nObservability with the elastic stack\n\nElastic Stack을 이용하여 MSA 서비스 모니터링시 모니터링 중앙 집중화를 통한 가관측성(Observability)에 대하여 알아본다.\n\n\n모니터링의 3요소\n\n\n    \n    \n        \n    \n    \n\n\n\n\n운영시 모니터링 요소는 Logs, Metrics, APM 크게 3가지로 분류할 수 있다.\n\n\n  Logs\n  Metrics\n  APM(Application Perfromance Monitoring)\n\n\n추가로 Heartbeat, Packetbeat 등을 이용한 서버의 가용성을 체크하는 Uptime Data가 있음\n\nLogs Data\n\n\n\n어플리케이션 로그, 서비스 로그 등의 각 모듈의 실제 로그로 Filebeat, FunctionBeat 등을 이용하여 수집한다.\n\nDB 롱쿼리, 호출 내역, 이벤트 데이터 수발신, Kafka 행등 서비스 레벨에서 작성하는 로그를 통한 가시성을 챙길 수 있다.\n\nMetrics Data\n\n\n\n\n\n호스트, 시스템의 리소스 정보 및 사용량, 부하율, 네트워크 트래픽 등\n\n주기적인 전체 요약이나 통계적인 정보로 일정 기간에 걸쳐서 샘플링되는 경향을 지닌 데이터이다.\n\nVM의 부하율, 메모리 점유율 등을 파악하여 Scale Out을 하거나 DB의 사용량을 파악하여 Worker node를 늘린다던지 하는 방향으로 사용할 수 있다.\n\nAPM Data\n\n\n\n\n\n어플리케이션 중점의 성능 정보로 실제 어플리케이션 오류 정보나 작업 수행 시간, 서비스간 상호 연동, 트랜잭션 추적, 병목 지점등 많은 정보를 담아낸다.\n\n성능 병목 파악, 에러 검출, Long Query, 트랜잭션 추적 등 어플리케이션 레벨의 가시성을 챙겨준다.\n\nApplication에 Agent가 사이드카 형태로 올라가는 방식으로 많이 사용한다.\n\nUptime Data\n\nLiveness, Readiness, Healthz 등 Uptime 정보\n\n\n\nELK Stack(Elastic Stack)을 채택한 이유\n\n위의 모니터링을 통하여 가시성을 챙기는 방법은 여러가지가 있을 수 있으나 ELK Stack을 채택한 이유는 크게 다음과 같다.\n\n\n  루씬(Lucene) 기반의 뛰어난 분산처리를 통한 실시간 검색 기능을 제공한다.\n  메트릭, 로그, APM 등의 모니터링 정보를 중앙 집중화할 수 있다.\n  오픈소스 기반으로 다양한 서드파티 라이브러리를 제공하여 여러 오픈소스들을 쉽게 연동 가능\n  다양한 시각화 라이브러리를 제공하여 큰 어려움 없이 데이터 시각화 가능\n  시계열 데이터를 운영하는데 최적화\n\n\nELK 적용 이전\n\n\n\n\n\n각기 다른 정보를 모니터링하기 위해 특화된 여러 툴들을 띄워놓고 모니터링한다.\n\n따라서 관리 포인트가 많아지고 데이터의 중복이 발생할 수 있다.\n\nELK 적용 이후\n\n\n\n\n  단일 페이지에서 중앙 집중화된 로그를 쉽게 모니터링 할 수 있다.\n\n\n모든 metric, apm, logs 정보들은 Elasticsearch의 인덱스이므로 데이터를 시각화, 머신러닝, 필터링 하는데 아무런 제약이 없다.\n\n로그 중앙집중화 및 가시성 확보뿐만 아니라 Elasticsearch는 데이터 분석 엔진으로도 탁월한 성능을 자랑한다.\n\n\n\nReference\n\n\n  Observability with the elastic stack\n\n"
} ,
  
  {
    "title"    : "Elastic Metricbeat 설치 및 설정",
    "category" : "",
    "tags"     : " Elasticsearch, Elastic, ELK, Metricbeat, Beat",
    "url"      : "/2021/07/27/elastic-metricbeat-install.html",
    "date"     : "July 27, 2021",
    "excerpt"  : "Elastic Metricbeat 설치 및 설정\n\n\n\nMetricbeat Overview\n\n로그 정보를 수집하는 Filebeat를 각 VM(Ubuntu)에 설치하여 로그 파일을 Elasticsearch로 전송한다.\n\n\nMetricbeat란\n\nElastic Stack에 포함되는 오픈소스로 시스템과 서비스의 규격화된 메트릭 정보를 경량화된 방식으로 수집하고 Logstash, Elasticsearch, Kibana 등으로 전달하는 수집기\n\n\nM...",
  "content"  : "Elastic Metricbeat 설치 및 설정\n\n\n\nMetricbeat Overview\n\n로그 정보를 수집하는 Filebeat를 각 VM(Ubuntu)에 설치하여 로그 파일을 Elasticsearch로 전송한다.\n\n\nMetricbeat란\n\nElastic Stack에 포함되는 오픈소스로 시스템과 서비스의 규격화된 메트릭 정보를 경량화된 방식으로 수집하고 Logstash, Elasticsearch, Kibana 등으로 전달하는 수집기\n\n\nMetricbeat 설치\n현재 설치된 elasticsearch 버전을 확인 후 호환되는 버전의 metricbeat를 설치한다.\n\n되도록 elaticsearch 버전과 동일한 버전의 metricbeat을 설치한다.\n\nelastic component들 과거버전 선택 download:\n\n  https://www.elastic.co/kr/downloads/past-releases\n\n\nmetricbeat 7.11.2 download url:\n\n  https://www.elastic.co/downloads/past-releases/metricbeat-7-11-2\n\n\nwget 으로 7.11.2 버전을 download 및 설치:\n# 다운로드\n$ wget https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.11.2-amd64.deb\n\n# RobotMakers EPC에는 7.12.1 설치됨\n$ wget https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.12.1-amd64.deb\n\n# CentOS 7 용(rpm)\n$ wget https://artifacts.elastic.co/downloads/beats/metricbeat/metricbeat-7.12.1-x86_64.rpm\n\n# 설치\n$ dpkg -i metricbeat-7.11.2-amd64.deb\n# 7.12.1 버전\n$ dpkg -i metricbeat-7.12.1-amd64.deb\n#CentOS rpm\n$ rpm -ivh ./metricbeat-7.12.1-x86_64.rpm\n\n# 설치 후 설치된 버전 upgrade 막기\n$ apt-mark hold metricbeat\n\n# 설치파일 삭제\n$ rm metricbeat-7.11.2-amd64.deb\n\n# bin path를 .zshrc 파일에 추가 후 적용\nexport PATH=/usr/share/metricbeat/bin:$PATH\n\n$ source .zshrc\n\n\nsystem booting 시 자동 실행 등록.\n$ systemctl enable metricbeat.service\n$ systemctl daemon-reload\n\n\n\nMetricbeat 설정\nmetricbeat 설치 후 elasticsearch로의 접속 설정 및 보안 설정 진행\n\nconfig directory:\n\n  /etc/metricbeat\n\n\nconfig file:\n\n  metricbeat.yml\n\n\nMetricbeat 접속 설정 및 보안 설정\n미리 구축해둔 elasticsearch에 연결할 예정이므로 output.elasticsearch 섹션에 연결 설정을 진행한다.\n\n설정 진행 시 ssl 부분도 설정하며 인증서 파일은 data node용으로 만들어둔 인증서를 사용한다.\n\nelasticsearch 연결 설정:\n\n  output.elasticsearch 값을 설정\n  username, password 는 hard-coded 하지 말고 serets keystore 등으로 대체 필요\n\n\nmetricbeat.yml 설정 내용 - SSL 인증 키 없이 http 접속 시 아래 설정 사용(RobotMakers의 경우):\n# kibana 접속 정보 설정.\nsetup.kibana:\n  host: &quot;http://10.213.196.6:5601&quot;\n\noutput.elasticsearch:\n  # master node들 \n  hosts: [&quot;10.213.196.68:9200&quot;,&quot;10.213.196.23:9200&quot;,&quot;10.213.196.44:9200&quot;]\n\n  # 보안설정 후 https 접속\n  protocol: &quot;http&quot;\n\n  username: &quot;elastic&quot;\n  password: &quot;elastic&quot;\n\n\nmetricbeat.yml 설정 내용 - SSL 인증 키 생성하여 적용 시 아래 설정 사용:\n# kibana 접속 정보 설정.\nsetup.kibana:\n  host: &quot;https://172.25.0.166:5601&quot;\n\n  ssl:\n    enabled: true\n    certificate_authorities: [&quot;/etc/elasticsearch/certs/ca.crt&quot;]\n    certificate: &quot;/etc/elasticsearch/certs/data1.crt&quot;\n    key: &quot;/etc/elasticsearch/certs/data1.key&quot;\n    \noutput.elasticsearch:\n  # master node들 \n  hosts: [&quot;172.25.0.92:9200&quot;, &quot;172.25.0.159:9200&quot;, &quot;172.25.0.121:9200&quot;]\n\n  # 보안설정 후 https 접속\n  protocol: &quot;https&quot;\n\n  #api_key: &quot;id:api_key&quot;\n  username: &quot;elastic&quot;\n  password: &quot;{암호}&quot;\n\n  # tls 설정 값. logstash-1 서버에 적용중이기 때문에 해당 node의 인증서를 적용\n  ssl:\n    certificate_authorities: [&quot;/etc/elasticsearch/certs/ca.crt&quot;]\n    certificate: &quot;/etc/elasticsearch/certs/data1.crt&quot;\n    key: &quot;/etc/elasticsearch/certs/data1.key&quot;\n\n\n설정파일 테스트. 설정 파일이 위치한 directory 에서 테스트 가능하며 테스트 후 OK가 출력되면 OK:\n# 테스트 진행 후 Confg OK 출력 확인\n$ metricbeat test config -e\n\n\n\n\nSystem module 설정\n# 필요 시 system 모듈 설정\n$ vi /etc/metricbeat/modules.d/system.yml\n\n# 추가 module 없이 활성화를 하면 system metric만 수집함.\n$ metricbeat modules enable\n\n\nMetricbeat Setup\n# asset 로딩 시 dashboard 생성을 위해 /usr/share/metricbeat 으로 이동하여 실행\n$ cd /usr/share/metricbeat\n\n# -e 옵션으로 error 여부를 stdout 으로 출력. log에서 host에 정상 접속 했는지 확인 가능\n$ metricbeat setup -e -c /etc/metricbeat/metricbeat.yml --dashboards\n\n\nMetricbeat 구동 &amp;amp; metric 값 전송\n$ systemctl start metricbeat.service\n\n\n\n\nMetricbeat 모듈 설정\n\nElastic Metricbeat 모듈 설정 참고\n\n\n\nReference\n\n\n  Metricbeat Overview\n\n"
} ,
  
  {
    "title"    : "MySQL dump &amp; restore",
    "category" : "",
    "tags"     : " MySQL, DB, RDBMS, SQL, Dump, Restore",
    "url"      : "/2021/06/15/mysql-dump.html",
    "date"     : "June 15, 2021",
    "excerpt"  : "MySQL dump &amp;amp; restore\n\n\n\nMySQL Dump 및 Restore 내용 정리\n\n\n\nDump &amp;amp; Restore\n\nDump\n\n현재 DB의 스키마와 데이터, 상태 등을 sql 쿼리 형태로 추출하는 것\n\nRestore\n\nDump된 결과물의 쿼리를 수행하는 것\n\n이론적으로는 dump 한 DB의 상태가 같아진다.\n\n1. MySQL 컨테이너 접속\n\n\n  \n    Using docker\n  \n\n\n# container id 확...",
  "content"  : "MySQL dump &amp;amp; restore\n\n\n\nMySQL Dump 및 Restore 내용 정리\n\n\n\nDump &amp;amp; Restore\n\nDump\n\n현재 DB의 스키마와 데이터, 상태 등을 sql 쿼리 형태로 추출하는 것\n\nRestore\n\nDump된 결과물의 쿼리를 수행하는 것\n\n이론적으로는 dump 한 DB의 상태가 같아진다.\n\n1. MySQL 컨테이너 접속\n\n\n  \n    Using docker\n  \n\n\n# container id 확인\n$ docker ps | grep -i mysql\ncd1ae64000ec   mysql/mysql-server:8.0   &quot;/entrypoint.sh --ch…&quot;   3 months ago   Up 3 months (healthy)   33060-33061/tcp, 0.0.0.0:13306-&amp;gt;3306/tcp   mysql\n\n# 컨테이너 접속\n$ docker exec -it cd1ae64000ec /bin/bash\n\n# 접속 확인\nbash-4.4#\nbash-4.4# whoami\nroot\n\n\n\n  \n    Using docker-compose\n  \n\n\n# 컨테이너 접속\n$ docker-compose exec mysql /bin/bash\n\n# 접속 확인\nbash-4.4#\nbash-4.4# whoami\nroot\n\n\n2. MySQL Dump\n# --databses: 복구할 db, --add-drop-database: database drop문 포함\n$ mysqldump --databases db1 db2 --add-drop-database -h localhost -u root -p{비밀번호} &amp;gt; {파일명}.sql\n$ mysqldump --databases mydb --add-drop-database -h localhost -u root -ppassword1234 &amp;gt; mysql_mydb_dump_20210615.sql\n\n\n3. MySQL Restore\n\n$ mysql -h localhost -u root -p{비밀번호} &amp;lt; {파일명}.sql\n# 예시\n$ mysql -h localhost -u root -ppassword1234 &amp;lt; mysql_mydb_dump_20210615.sql\n\n\n"
} ,
  
  {
    "title"    : "MySQL 설치 및 사용자 생성",
    "category" : "",
    "tags"     : " MySQL, DB, RDBMS, Docker, Docker-compose",
    "url"      : "/2021/05/31/mysql-install.html",
    "date"     : "May 31, 2021",
    "excerpt"  : "MySQL 설치 및 사용자 생성\n\n\n\nMySQL을 docker-compose로 설치 후 사용자 생성, 권한 설정을 진행한 내용이다.\n\n\n\nMySQL 설치\n\n1. 호스트 볼륨 생성\n$ mkdir -p /data/mysql\n\n\n2. docker-compose.yaml 작성\n접속 port는 13306(docker 외부) - 3306(docker 내부) 로 설정.\n\n$ vim docker-compose.yaml\n\n\nversion: &#39;3.1&#39;\nse...",
  "content"  : "MySQL 설치 및 사용자 생성\n\n\n\nMySQL을 docker-compose로 설치 후 사용자 생성, 권한 설정을 진행한 내용이다.\n\n\n\nMySQL 설치\n\n1. 호스트 볼륨 생성\n$ mkdir -p /data/mysql\n\n\n2. docker-compose.yaml 작성\n접속 port는 13306(docker 외부) - 3306(docker 내부) 로 설정.\n\n$ vim docker-compose.yaml\n\n\nversion: &#39;3.1&#39;\nservices:\n  mysql:\n    image: mysql/mysql-server:8.0\n    restart: always\n    container_name: mysql\n    ports:\n      - 13306:3306\t\t# 외부 port : 내부 port\n    environment:\n      TZ: Asia/Seoul\n      MYSQL_ROOT_PASSWORD: &quot;root&quot;\n    command: \n      - --character-set-server=utf8\n      - --collation-server=utf8_unicode_ci\n    volumes:\n      - /data/mysql:/var/lib/mysql\t# 호스트 볼륨 : 컨테이너 볼륨\n\n\n3. MySQL 구동\n\n$ docker-compose up -d mysql\n\n$ docker-compose ps mysql\nName               Command                  State                           Ports\n-----------------------------------------------------------------------------------------------------\nmysql   /entrypoint.sh --character ...   Up (healthy)   0.0.0.0:13306-&amp;gt;3306/tcp, 33060/tcp, 33061/tc\n\n$ docker-compose logs mysql\nAttaching to mysql\nmysql       | [Entrypoint] MySQL Docker Image 8.0.25-1.2.3-server\nmysql       | [Entrypoint] Starting MySQL 8.0.25-1.2.3-server\n\n\n\nMySQL 설정\n\n1. MySQL 컨테이너 접속\n\n\n  \n    Using docker\n  \n\n\n# container id 확인\n$ docker ps | grep -i mysql\ncd1ae64000ec   mysql/mysql-server:8.0   &quot;/entrypoint.sh --ch…&quot;   3 months ago   Up 3 months (healthy)   33060-33061/tcp, 0.0.0.0:13306-&amp;gt;3306/tcp   mysql\n\n# 컨테이너 접속\n$ docker exec -it cd1ae64000ec /bin/bash\n\n# 접속 확인\nbash-4.4#\nbash-4.4# whoami\nroot\n\n\n\n  \n    Using docker-compose\n  \n\n\n# 컨테이너 접속\n$ docker-compose exec mysql /bin/bash\n\n# 접속 확인\nbash-4.4#\nbash-4.4# whoami\nroot\n\n\n2. MySQL 사용자 생성\n\nMySQL 계정의 경우 계정명과 호스트가 같이 있다.\n\n위처럼 localhost로 host 생성하는 경우 localhost에서만 접속 가능(외부 접속 불가)하므로 외부 접속을 위한 계정을 생성해보자.\n\n1. MySQL 로그인\n\nbash-4.4# mysql -u root -p\nEnter password:\n\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 1600691\nServer version: 8.0.25 MySQL Community Server - GPL\n\nCopyright (c) 2000, 2021, Oracle and/or its affiliates.\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType &#39;help;&#39; or &#39;\\h&#39; for help. Type &#39;\\c&#39; to clear the current input statement.\n\nmysql&amp;gt;\n\n\n2. 사용자 생성\n\n# 사용자 확인\nmysql &amp;gt; select host, user from mysql.user;\n+-----------+------------------+\n| host      | user             |\n+-----------+------------------+\n| localhost | mysql.infoschema |\n| localhost | mysql.session    |\n| localhost | mysql.sys        |\n| localhost | root             |\n+-----------+------------------+\n\n# 전체 host에서 접속 가능한 사용자 생성\n# create user {사용자명}@{호스트} identified by {비밀번호};\nmysql &amp;gt; create user &#39;csupreme19&#39;@&#39;%&#39; identified by &#39;password1234&#39;;\n\n# 참고) 단일 IP host의 경우\nmysql &amp;gt; create user &#39;csupreme19&#39;@&#39;127.0.0.1&#39; identified by &#39;password1234&#39;;\n# 참고) IP 대역 host의 경우\nmysql &amp;gt; create user &#39;csupreme19&#39;@&#39;127.%&#39; identified by &#39;password1234&#39;;\n\n# 사용자 확인\nmysql &amp;gt; select host, user from mysql.user;\n+-----------+------------------+\n| host      | user             |\n+-----------+------------------+\n| %         | csupreme19       |\n| localhost | mysql.infoschema |\n| localhost | mysql.session    |\n| localhost | mysql.sys        |\n| localhost | root             |\n+-----------+------------------+\n\n# 접속 확인\n$ mysql -h 127.0.0.1 -u csupreme19 -p\n\n\n3. 권한 부여\n\n# 스키마 권한 추가\n# myschema 스키마의 모든 테이블\nmysql &amp;gt; grant all privileges on myschema.* to csupreme19@&#39;%&#39;;\n# 전체 스키마\nmysql &amp;gt; grant all privileges on *.* to csupreme19@&#39;%&#39;;\n\n# 권한 적용 필수\nmysql&amp;gt; flush privileges;\n\n\n"
} ,
  
  {
    "title"    : "Tomcat session clustering 적용기",
    "category" : "",
    "tags"     : " Tomcat, Session, Clustering, Multicast, Replication",
    "url"      : "/2021/05/24/tomcat-session-clustering.html",
    "date"     : "May 24, 2021",
    "excerpt"  : "Tomcat session clustering 적용기\n\n\n\nClustering/Session Replication How-To\n\n본 문서에서는 Kubernetes 환경에서의 Tomcat간 세션 공유를 위한 session clustering 적용기를 다룬다.\n\n\n\n문제점\n\n\n  flowchart LR\n  subgraph k8s\n  subgraph Node A\n  A[Tomcat A]\n  end\n  subgraph Node B\n  B[Tomc...",
  "content"  : "Tomcat session clustering 적용기\n\n\n\nClustering/Session Replication How-To\n\n본 문서에서는 Kubernetes 환경에서의 Tomcat간 세션 공유를 위한 session clustering 적용기를 다룬다.\n\n\n\n문제점\n\n\n  flowchart LR\n  subgraph k8s\n  subgraph Node A\n  A[Tomcat A]\n  end\n  subgraph Node B\n  B[Tomcat B]\n  end\n  C[Service]\n  end\n  D[User]\n  D--&amp;gt;C\n  C--&amp;gt;A &amp;amp; B\n  A x-.session.-x B\n\n\nSpring boot 기반 Tomcat  서비스는 클라이언트에서 요청시 세션을 Tomcat이 올라가 있는 노드에만 local 저장된다.\n\n이 경우 Pod replication, crash/shutdown 등으로 인하여 노드간 세션이 공유되지 않아 접속이 끊기는 문제가 발생할 수 있음\n\n\n\n요구사항\n\nKubernetes 클러스터 내에서 Pod 간 세션을 유지할 수 있는 방법 수립\n\n\n  유지보수가 쉬울 것(관리 포인트 적을 것)\n  어플리케이션(서비스) 코드에 영향이 없을 것\n  구성하는데 추가적인 리소스가 들어가지 않을 것\n\n\n\n\n방법\n\n1. Sticky session\n\n\n\n클라이언트가 세션을 맺은 서버랑만 통신하는 것\n\n사용자 입장에서는 세션이 끊기지 않고 유지되는 장점이 있으나\n\npod crash, shutdown 등의 문제로 pod 재시작, 중지시 세션이 끊기는 문제가 있다.\n\nkubernetes session affinity 설정으로 비교적 간단히 설정이 가능하나\n\n완벽한 session replication이 아니다.\n\n2. Tomcat session in-memory replication(Multicast)\n\nTomcat에서 기본적으로 제공하는 클러스터링\n\n서버 인메모리에 저장하고 SimpleTCPCluster와 같은 클래스를 이용하여 리플리케이션하는 방법\n\ntomcat xml(server.xml, pom.xml) 수정이 필요하여 spring boot에서 기본으로 제공하는 embedded tomcat에서 사용이 안된다고 하나 java config를 이용하여 embedded tomcat도 설정 가능한 것으로 보인다.\n\nmulticast 기능을 지원해야하지만 multicast는 대부분의 cloud 환경에서는 제공하지 않음\n\nTomcat 설정 파일을 바꾸려면 Spring boot의 경우 @Configuration 설정을 통하여 Bean을 주입해야하여 소스코드 추가가 필요하다.\n\n3. Tomcat session persistence replication\n\nredis, dynamoDB와 같은 DB에 세션 정보를 저장하고 각 서버로 클러스터링 하는 방법\n\nsession이 별도의 저장소에 저장되어 사용되는 장점이 있음\n\n\nTomcat session in-memory replication(Multicast) 테스트\n\nClustering/Session Replication How-To\n\nEPC Multicast 지원 테스트\n\n현재 사용중인 클라우드(EPC)에서 Multicast 기능을 지원하는지 테스트한다.\n\n멀티캐스트 IP 대역은 사설 224.0.1.0 ~ 238.255.255.255의 대역 IP가 예약되어 있으며\n\n멀티캐스트 사용을 위해서는 L2 장비가 IGMP 프로토콜을 지원해야한다고 한다.\n\n\n  AWS와 같은 메이저 클라우드에서는 지원을 안한다고 하니 확인 필요\n\n\niperf라는 네트워크 테스트 툴을 이용하여 EPC에서 멀티캐스트 IP 대역 사용이 가능한지 테스트한다.\n\nkubernetes worker 노드에서 진행\n\n# iperf 설치 (root 진행)\n$ wget -O /usr/bin/iperf https://iperf.fr/download/ubuntu/iperf_2.0.9\n$ chmod +x /usr/bin/iperf\n\n# Server side\n$ iperf -s -u -B 228.0.0.4 -i 1 -p 45564\n------------------------------------------------------------\nServer listening on UDP port 45564\nBinding to local address 228.0.0.4\nJoining multicast group  228.0.0.4\nReceiving 1470 byte datagrams\nUDP buffer size:  208 KByte (default)\n------------------------------------------------------------\n[  3] local 228.0.0.4 port 45564 connected with 10.213.196.14 port 49293\n[ ID] Interval       Transfer     Bandwidth        Jitter   Lost/Total Datagrams\n[  3]  0.0- 1.0 sec   129 KBytes  1.06 Mbits/sec   0.006 ms    0/   90 (0%)\n[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec   0.011 ms    0/   89 (0%)\n[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec   0.007 ms    0/   89 (0%)\n[  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec   0.007 ms    0/  269 (0%)\n\n# Client side (다른 노드에서 실행)\n$ iperf -c 228.0.0.4 -u -T 32 -t 3 -i 1 -p 45564\n------------------------------------------------------------\nClient connecting to 228.0.0.4, UDP port 45564\nSending 1470 byte datagrams, IPG target: 11215.21 us (kalman adjust)\nSetting multicast TTL to 32\nUDP buffer size:  208 KByte (default)\n------------------------------------------------------------\n[  3] local 10.213.196.14 port 49293 connected with 228.0.0.5 port 45564\n[ ID] Interval       Transfer     Bandwidth\n[  3]  0.0- 1.0 sec   131 KBytes  1.07 Mbits/sec\n[  3]  1.0- 2.0 sec   128 KBytes  1.05 Mbits/sec\n[  3]  2.0- 3.0 sec   128 KBytes  1.05 Mbits/sec\n[  3]  0.0- 3.0 sec   386 KBytes  1.05 Mbits/sec\n[  3] Sent 269 datagrams\n\n\n\n  Tomcat의 경우 기본 multicast 주소는 228.0.0.4에 포트는 45564 사용\n\n\n서버에서 클라이언트 각각 실행하여 클라이언트에서 송신하는 메시지가 서버에 멀티캐스트 되는지 확인\n\n다른 노드에서 실행시 정상 송수신을 확인하였다. (Zone: Prd-private)\n\n\n\nTomcat session clustering 샘플 프로젝트 작성\n\nbuild.gradle 작성\n\nplugins {\n\tid &#39;org.springframework.boot&#39; version &#39;2.5.0&#39;\n\tid &#39;io.spring.dependency-management&#39; version &#39;1.0.11.RELEASE&#39;\n\tid &#39;java&#39;\n}\n\ngroup = &#39;com.example&#39;\nversion = &#39;0.0.1-SNAPSHOT&#39;\nsourceCompatibility = &#39;1.8&#39;\n\nrepositories {\n\tmavenCentral()\n}\n\ndependencies {\n\timplementation &#39;org.springframework.boot:spring-boot-starter-web&#39;\n\timplementation &#39;org.projectlombok:lombok:1.18.12&#39;\n\tannotationProcessor &#39;org.projectlombok:lombok:1.18.12&#39;\n\timplementation &#39;org.apache.tomcat:tomcat-catalina-ha:9.0.46&#39;\n\ttestImplementation &#39;org.springframework.boot:spring-boot-starter-test&#39;\n}\n\ntest {\n\tuseJUnitPlatform()\n}\n\n\n세션 정보 확인을 위한 간단한 Controller 작성\n\n@RestController\npublic class MainController {\n\t@GetMapping(&quot;/&quot;)\n\tpublic String getSession(HttpServletRequest request) {\n\t\tHttpSession session = request.getSession();\n\t\tString result = &quot;session ID: &quot; + session.getId() + &quot;\\n\\nsession created: &quot; + session.getCreationTime() + &quot;\\n\\nsession accessed: &quot; + session.getLastAccessedTime();\n\t\treturn result;\n\t}\n}\n\n\nTomcat config 설정\n\nTomcatClusterContextCustomizer\n\n@Component\npublic class TomcatClusterContextCustomizer implements TomcatContextCustomizer {\n\t@Value(&quot;${env.multicast.receiver.port}&quot;)\n\tprivate Integer receiverPort;\n\t\n\t@Override\n\tpublic void customize(final Context context) {\n\t\tcontext.setDistributable(true);\t// web.xml &amp;lt;distribute/&amp;gt;\n\t\t// Manager setting\n\t\tBackupManager manager = new BackupManager();\n\t\tmanager.setNotifyListenersOnReplication(true);\n\t\tcontext.setManager(manager);\n\t\tconfigureCluster((Engine)context.getParent().getParent());\n\t}\n\t\n\tprivate void configureCluster(Engine engine) {\n\t\t// Cluster setting\n\t\tSimpleTcpCluster cluster = new SimpleTcpCluster();\n\t\tcluster.setChannelSendOptions(8);\n\t\t// Channel setting\n\t\tGroupChannel channel = new GroupChannel();\n\t\t// Membership setting\n\t\tMcastService mcastService = new McastService();\n\t\tmcastService.setAddress(&quot;228.0.0.4&quot;);\n\t\tmcastService.setPort(45564);\n\t\tmcastService.setFrequency(500);\n\t\tmcastService.setDropTime(3000);\n\t\tchannel.setMembershipService(mcastService);\n\t\t// Receiver setting\n\t\tNioReceiver receiver = new NioReceiver();\n\t\treceiver.setAddress(&quot;auto&quot;);\t// tomcat의 LAN 주소를 가져온다. Ipv4로 설정해야함\n\t\treceiver.setPort(4000);\n//\t\treceiver.setPort(receiverPort);   톰캣이 같은 node에서 replication 구동시 리시버 포트는 서로 달라야함 이 경우 별도 포트 지정\n\t\treceiver.setAutoBind(100);\n\t\treceiver.setSelectorTimeout(5000);\n\t\treceiver.setMaxThreads(6);\n\t\tchannel.setChannelReceiver(receiver);\n\t\t// Sender setting\n\t\tReplicationTransmitter sender = new ReplicationTransmitter();\n\t\tsender.setTransport(new PooledParallelSender());\n\t\tchannel.setChannelSender(sender);\n\t\t// Intercepter setting\n\t\tchannel.addInterceptor(new TcpPingInterceptor());\n\t\tchannel.addInterceptor(new TcpFailureDetector());\n\t\tchannel.addInterceptor(new MessageDispatchInterceptor());\n\t\tcluster.addValve(new ReplicationValve());\n\t\tcluster.addValve(new JvmRouteBinderValve());\n\t\tcluster.setChannel(channel);\n\t\tcluster.addClusterListener(new ClusterSessionListener());\n\t\tengine.setCluster(cluster);\n\t}\n}\n\n\nTomcatClusterUtil 작성\n\n@Configuration\npublic class TomcatClusterUtil implements WebServerFactoryCustomizer&amp;lt;TomcatServletWebServerFactory&amp;gt;{\n\t@Autowired\n\tTomcatClusterContextCustomizer tomcatClusterContextCustomizer;\n\t\n\t@Override\n\tpublic void customize(final TomcatServletWebServerFactory factory) {\n\t\tfactory.addContextCustomizers(tomcatClusterContextCustomizer);\n\t}\n}\n\n\nspring boot와 같이 설치형 tomcat이 아닌 embedded tomcat에서는 server.xml, web.xml과 같은 설정 파일의 직접 수정이 불가능하므로\n\nTomcatContextCustomizer를 이용하여 Context에 설정한다.\n\n\n  항목별 자세한 설명은 공식문서 참조\n\n\n인스턴스별 포트 설정을 위한 application.yaml 작성\n\nspring:\n  profiles:\n    active: instance1\n\n---\nspring:\n  config:\n    activate:\n      on-profile: instance1\nserver:\n  port: 8080\nenv:\n  multicast.receiver.port: 4000\n\n---\nspring:\n  config:\n    activate:\n      on-profile: instance2\nserver:\n  port: 8081\nenv:\n  multicast.receiver.port: 4001\n\n\n\n  로컬 테스트시 같은 포트로 톰캣 인스턴스 실행이 불가능하므로 포트 설정을 하기 위함\n\n\nspring boot Run configuration 설정\n\n\n  \n    tomcat-session-replication-test1\n\n    \n      Profile: instance1\n      JVM Arguments: -Djava.net.preferIPv4Stack=true\n    \n  \n  \n    tomcat-session-replication-test2\n\n    \n      Profile: instance2\n      JVM Arguments: -Djava.net.preferIPv4Stack=true\n    \n  \n\n\n\n  preferIPv4Stack true로 설정해야 위에서 Receiver주소 설정값 receiver.setAddress(&quot;auto&quot;);가 작동한다.\n\n\n로드밸런서 설정\n\nTomcat에서 지원하는 session clustering을 사용하려면 로드밸런서를 통해 같은 도메인이름을 사용하여야 세션이 공유된다.\n\n서로 다른 도메인을 사용하거나 다른 서버인 경우 기본적으로 같은 session을 가지고 있다고 판단하지 않는 것 같다.\n\n따라서 같은 도메인 설정을 위한 로드밸런서 역할을 할 nginx를 사용하여 리버스 프록시 설정을한다.\n\n맥 로컬 기준 작성\n\nnginx 설치 및 설정\n\n$ brew install nginx\t# nginx 설치\n$ vim /usr/local/etc/nginx/nginx.conf\t# 로드밸런서 설정\n 17   http {\n 18     include       mime.types;\n 19     default_type  application/octet-stream;\n ...\n 35     upstream myproject {\n 36       server 127.0.0.1:8080;\n 37       server 127.0.0.1:8081;\n 38     }\n 39\n 40     server {\n 41         listen       80;\n 42         server_name  localhost;\n ...\n 48         location / {\n 49           proxy_pass http://myproject;\n 50         }\n $ nginx\t# local nginx 구동\n\n\nlocalhost:80 접속시 127.0.0.1:8080, 127.0.0.1:8081로 각각 로드밸런싱\n\n\n\n테스트(Local)\n\n테스트 순서\n\n  tomcat-session-replication-test1 구동\n  localhost 접속\n  session 확인\n  tomcat-session-replication-test2 구동\n  tomcat-session-replication-test1 중지\n  localhost 접속\n  session 확인\n\n\n테스트 진행 및 결과\n\n\n  tomcat-session-replication-test1 구동\n\n  localhost 접속\n  session 확인\n\n  tomcat-session-replication-test2 구동\n  tomcat-session-replication-test1 중지\n\n  localhost 접속\n  session 확인\n\n\n\nsession ID를 1번 인스턴스에서 생성했음에도 2번 tomcat으로 접속시 session ID가 서로 동일한 것을 확인할 수 있다.\n\n\n\n테스트(Dev EPC)\n\n현시점 기준 172.x 대역의 개발 클러스터에 설정하여 세션 공유가 kubernetes 환경에서도 적용되는지 테스트한다.\n\nkubernetes 배포\n\n  \n    gitlab 프로젝트 생성 및 commit\n  \n  \n    이미지 빌드를 위한 Dockerfile, k8s 배포를 위한 yaml 작성\n\n    \n      \n        Dockerfile\n      \n    \n\n    FROM openjdk:8-jdk-alpine\n   \n ADD build/libs/tomcat-session-replication-test-0.0.1-SNAPSHOT.jar TomcatSessionClusteringTest.jar\n RUN apk --no-cache add tzdata &amp;amp;&amp;amp; \\\n         cp /usr/share/zoneinfo/Asia/Seoul /etc/localtime &amp;amp;&amp;amp; \\\n         echo &quot;Asia/Seoul&quot; &amp;gt; /etc/timezone\n   \n ENTRYPOINT [&quot;java&quot;, &quot;-Duser.timezone=Asia/Seoul&quot;, &quot;-Djava.net.preferIPv4Stack=true&quot;, &quot;-jar&quot;, &quot;/TomcatSessionClusteringTest.jar&quot;, &quot;--spring.profiles.active=instance1&quot;]\n    \n    \n    \n      \n        kubernetes.yaml\n      \n    \n\n    ---\n apiVersion: apps/v1\n kind: Deployment\n metadata:\n   namespace: development\n   name: tomcatsessionclustertest-deploy\n   labels:\n     app: tomcatsessionclustertest\n spec:\n   replicas: 2\t# pod를 2개 띄움\n   selector:\n     matchLabels:\n       app: tomcatsessionclustertest\n   template:\n     metadata:\n       labels:\n         app: tomcatsessionclustertest\n     spec:\n       containers:\n       - name: tomcatsessionclustertest\n         image: gitlab.yourdomain.com/tomcatsessionclustertest:latest\n         volumeMounts:\n         - name: tz-seoul\n           mountPath: /etc/localtime\n         ports: \n         - containerPort: 8080\n       imagePullSecrets:\n       - name: reg\n       volumes:\n       - name: tz-seoul\n         hostPath: \n           path: /usr/share/zoneinfo/Asia/Seoul\n       affinity:\n       # 같은 node에 뜨지 않게 하기 위한 Affinity 설정\n         podAntiAffinity:\n           requiredDuringSchedulingIgnoredDuringExecution:\n           - labelSelector:\n               matchExpressions:\n               - key: app.kubernetes.io/name\n                 operator: In\n                 values:\n                 - tomcatsessionclustertest\n              topologyKey: kubernetes.io/hostname\n ---\n apiVersion: v1\n kind: Service\n metadata:\n   namespace: development\n   name: tomcatsessionclustertest-svc\n   labels:\n     app: tomcatsessionclustertest\n spec:\n   ports:\n   - port: 8080\n     nodePort: 32599\n     targetPort: 8080\n   selector:\n     app: tomcatsessionclustertest\n   type: NodePort\n      \n    \n  \n  \n    Jenkins 등록\n  \n\n\n파이프라인 아래와 같이 수정\nnode (&#39;jenkins-slave&#39;){\n    def gitBranch = &#39;master&#39;\n    def gitCredId = &#39;gitlab-ssh-key&#39;\n    def gitUrl = &#39;ssh://git@gitlab.yourdomain.com/group/&#39;\n    def projectUrl = gitUrl + &#39;tomcat-session-clustering-test.git&#39;\n    def dockerRepoUrl = &#39;http://nexus.yourdomain.com&#39;\n    def imageName = &#39;tomcatsessionclustertest&#39;\n    def defaultYaml = &#39;kubernetes.yaml&#39;\n    def deployYaml = &#39;kubernetes_deploy.yaml&#39;\n\n    stage(&#39;Checkout&#39;)    {\n         git(\n            url: projectUrl,\n            credentialsId: gitCredId,\n            branch: gitBranch\n        )\n    }\n    \n    stage(&#39;Build&#39;) {\n        sh &quot;chmod +x ./gradlew&quot;\n        sh &quot;./gradlew clean build -x test&quot;\n    }\n\n    stage(&#39;Analysis &amp;amp; Push&#39;) {\n        parallel (\n            &#39;Docker Build&#39;: {\n                def dockerfile = &#39;Dockerfile&#39;\n                app = docker.build(&quot;${imageName}:${env.BUILD_ID}&quot;,&quot;-f ${dockerfile} ./&quot;)\n            }\n        )\n        \n        parallel (\n            &#39;Registry Push&#39;: {\n                docker.withRegistry(&quot;${dockerRepoUrl}&quot;, &quot;dockerhub&quot;) {\n                    app.push(&quot;$BUILD_ID&quot;)\n                }\n            }\n        )\n    }\n    \n    stage(&#39;Kube Deploy&#39;){\n        \n        sh &quot;cat ${defaultYaml} | sed &#39;s/latest/${env.BUILD_ID}/g&#39; &amp;gt; ${deployYaml}&quot;\n\n        withKubeConfig(credentialsId: &#39;jenkins-builder&#39;, serverUrl: &#39;https://kubernetes&#39;) {\n            sh &quot;kubectl apply -f ${deployYaml}&quot;\n        }\n    }\n    \n}\n\n\n\n  CI/CD 환경 및 Kubernetes 환경은 각각 다르므로 위 파이프라인은 참고만 할 것\n\n\n\n  \n    kubernetes 배포 확인\n  \n\n\nkubernetes 대시보드 접속 또는 명령어 확인\n\n# alias k=kubectl\n$ k get po -n development\n$ k get svc -n development\n\n\n외부 접속 설정 및 접속 테스트\n\n\n  방화벽 오픈\n    \n      외부 -&amp;gt; kubernetes 32599 NodePort\n    \n  \n  접속 확인\n\n  접속 로그 확인\n\n  접속된 파드 제거\n    $ k get pod -n development\n$ k delete pod tomcatsessionclustertest-deploy-7d75d5548b-rknfh -n development\n    \n  \n  재접속\n\n\n\n테스트 결과\n\n\n\n\n\n세션 공유가 되지 않는것으로 확인되었다.\n\nkubernetes 환경에서 host 서버간 multicast 통신은 되는 것으로 판단되나 파드간 multicast 통신을 위해서는 multus-cni를 사용하여 NIC로 호스트와 연결해야함\n\n\n  참고: https://stackoverflow.com/a/61234801/15263734\n\n\nCode dependency도 높고 kubernetes 환경에 별도의 CNI 설치를 요구하므로 사용이 어렵다.\n\n\nTomcat session persistence replication 테스트\n\nRedis, dynamoDB, hazelcast 등의 in-memory DB를 이용하여 세션을 저장하고 공유하는 방법 테스트\n\n현재 EPC에서 Redis를 사용하기 때문에 redis를 이용하여 세션 저장하는 방법 테스트\n\n테스트(LOCAL)\n\n맥 로컬 기준 작성\n\n1. Redis 설치\n\nhttps://hub.docker.com/_/redis/\n\n$ docker pull redis\n$ docker run --name redis -p 6379:6379 -d redis\n$ docker ps\nCONTAINER ID   IMAGE     COMMAND                  CREATED         STATUS         PORTS                                       NAMES\n2e96b0e93c0a   redis     &quot;docker-entrypoint.s…&quot;   4 seconds ago   Up 2 seconds   0.0.0.0:6379-&amp;gt;6379/tcp, :::6379-&amp;gt;6379/tcp   redis\n\n\n2. 샘플 프로젝트 작성\n\nbuild.gradle\n\ndependencies {\n\timplementation &#39;org.springframework.boot:spring-boot-starter-web&#39;\n\ttestImplementation &#39;org.springframework.boot:spring-boot-starter-test&#39;\n\timplementation &#39;org.springframework.boot:spring-boot-starter-data-redis&#39;\n\timplementation &#39;org.springframework.session:spring-session-data-redis&#39;\n}\n\n\napplication.yaml\n\nspring:\n  profiles:\n    active: local\n\n---\nspring:\n  config:\n    activate:\n      on-profile: local\n  redis:\n    host: 127.0.0.1\n    port: 6379\nserver:\n  port: 8080\n\n---\nspring:\n  config:\n    activate:\n      on-profile: development\n  redis:\n    host: 172.25.0.163\n    port: 6379\nserver:\n  port: 8080\n\n\nMainController.java\n\n@RestController\npublic class MainController {\n\t\n\t@GetMapping(&quot;/&quot;)\n\tpublic String getSession(HttpServletRequest request) {\n\t\tHttpSession session = request.getSession();\n\t\tString result = &quot;session ID: &quot; + session.getId() + &quot;\\n\\nsession created: &quot; + session.getCreationTime() + &quot;\\n\\nsession accessed: &quot; + session.getLastAccessedTime();\n      \tSystem.out.println(request.getLocalPort());\n\t\treturn result;\n\t}\n\n}\n\n\nSpringBootApplication에 @EnableRedisHttpSession 추가\n@SpringBootApplication\n@EnableRedisHttpSession\npublic class TomcatSessionReplicationTestApplication {\n\tpublic static void main(String[] args) {\n\t\tSpringApplication.run(TomcatSessionReplicationTestApplication.class, args);\n\t}\n}\n\n\nSpring boot Run configuration 설정\n\n\n  \n    tomcat-session-replication-test1\n\n    \n      JVM Arguments: -Djava.net.preferIPv4Stack=true\n      Program Arguments: –server.port=8080\n    \n  \n  \n    tomcat-session-replication-test2\n\n    \n      JVM Arguments: -Djava.net.preferIPv4Stack=true\n      Program Arguments: –server.port=8081\n    \n  \n\n\n테스트(Local)\n\n테스트 순서\n\n\n  tomcat-session-replication-test1 구동\n  localhost 접속\n  session 확인\n  tomcat-session-replication-test2 구동\n  tomcat-session-replication-test1 중지\n  localhost 접속\n  session 확인\n  redis key 확인\n\n\n테스트 진행\n\n  tomcat-session-replication-test1 구동\n\n  localhost 접속\n  session 확인\n\n  tomcat-session-replication-test2 구동\n  tomcat-session-replication-test1 중지\n\n  localhost 접속\n  session 확인\n\n  redis key 확인\n    $ docker run -it redis bash\nroot@c5aeb4bf0493:/data# redis-cli -h 192.168.0.56\t\t# host local ip\n192.168.0.56:6379&amp;gt; keys *\n1) &quot;spring:session:expirations:1622602500000&quot;\n2) &quot;spring:session:sessions:expires:063e4085-0cfc-4c49-b0c9-65a334327e13&quot;\n3) &quot;spring:session:sessions:063e4085-0cfc-4c49-b0c9-65a334327e13&quot;\n    \n  \n\n\nredis에 session 정보가 저장이 되는 것을 확인 가능\n\n\n\n\n\nsession ID를 1번 인스턴스에서 생성했음에도 2번 tomcat으로 접속시 session ID가 서로 동일한 것을 확인할 수 있다.\n\n테스트(Dev EPC)\n\nKubernetes 배포\n\n2번 테스트 항목의 Kubernetes 배포 과정과 똑같음\n\nDockerfile &quot;--spring.profiles.active=development&quot;로 수정\n\n테스트 진행(Pod fail)\n\n  접속 확인\n\n  접속 로그 확인\n\n  접속된 파드 중지\n    $ k get pod -n development\n$ k delete pod tomcatsessionclustertest-deploy-79d5796f9c-dwxv2 -n development\n    \n  \n  재접속\n\n\n\n톰캣 세션이 끊기지 않은 것을 확인할 수 있다.\n\n\n  Redis가 죽으면 세션 접속 시점에 redis에 접속하므로 계속 reconnecting해 서비스가 죽는 문제가 존재하기는 한다.\n\n\n\n\n결론\n\n1. Sticky session\n\n\n\n클라이언트가 세션을 맺은 tomcat에 대해서만 통신을 하여 일반적인 상황에서는 세션이 유지되지만\n\n정상적인 상황에서 로드밸런싱이 전혀 되지 않으며 해당 pod, node가 죽었을 때 세션이 끊기는 문제가 있다.\n\n2. Tomcat session in-memory replication(Multicast)\n\nTomcat에서 기본적으로 가이드하고 있는 Session Replication 방법\n\n\n\n\n\n다른 노드간 세션이 유지되는 것을 확인할 수 있었다.\n\nTomcat 서버 인메모리에 세션 정보를 저장하고 SimpleTCPCluster와 같은 클래스를 이용하여 리플리케이션 한다.\n\nmulticast 기능을 지원해야하지만 multicast는 대부분의 cloud 환경에서는 제공하지 않는다고 한다.\n\n1. 유지보수가 쉬울 것(관리 포인트 적을 것)\n2. 어플리케이션(서비스) 코드에 영향이 없을 것\n3. 구성하는데 추가적인 리소스가 들어가지 않을 것\n\n\n무엇보다도 세션 클러스터링 요구사항의 2, 3 번에 위배되므로 합리적인 방법이 아니다.\n\n3. Tomcat session persistence replication\n\nredis, dynamoDB와 같은 DB에 세션 정보를 저장하고 각 서버로 레플리케이션 하는 방법\n\n\n\n\n\n다른 노드간 세션이 유지되는 것을 확인할 수 있었다.\n\nsession이 in-memory가 아닌 별도의 저장소에 저장되어 사용되는 장점이 있으며 코드 수정이 거의 없다는 장점이 있다.\n\n\n  redis 라이브러리 때문에 의존성이 아예 없을 수는 없다.\n\n\n따라서 가장 합리적인 Session clustering / Session replication이라고 할 수 있다.\n\n\n\nReference\n\n\n  Clustering/Session Replication How-To\n  https://stackoverflow.com/a/61234801/15263734\n\n"
} ,
  
  {
    "title"    : "SonarQube 설정 및 트러블슈팅",
    "category" : "",
    "tags"     : " Sonarqube, Jenkins, Webhook, Ingress, Kubernetes, GitLab",
    "url"      : "/2021/05/10/sonarqube-config.html",
    "date"     : "May 10, 2021",
    "excerpt"  : "SonarQube 설정 및 트러블슈팅\n\n\n\nSonarQube Documentation\n\n본 문서에서는 Sonarqube Jenkins 연동, GitLab 연동, Ingress 인증서 설정, 웹훅 설정 등 내용을 모아 정리하였으며\n\n설정시 오류를 해결했던 내역을(트러블슈팅) 정리하였다.\n\n\n\nSonarQube 설정\n\nSonarQube Ingress SSL 인증서 적용\n\n1. k8s Secret 생성\n\n\n  k8s secret 생성은 Kube...",
  "content"  : "SonarQube 설정 및 트러블슈팅\n\n\n\nSonarQube Documentation\n\n본 문서에서는 Sonarqube Jenkins 연동, GitLab 연동, Ingress 인증서 설정, 웹훅 설정 등 내용을 모아 정리하였으며\n\n설정시 오류를 해결했던 내역을(트러블슈팅) 정리하였다.\n\n\n\nSonarQube 설정\n\nSonarQube Ingress SSL 인증서 적용\n\n1. k8s Secret 생성\n\n\n  k8s secret 생성은 Kubernetes Nginx Ingress 적용 참고\n\n\n2. Ingress 리소스 설정\n\n$ vim sonarqube-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\n metadata:\n   name: sonarqube\n   namespace: default\n   annotations:\n     kubernetes.io/ingress.class: &quot;nginx&quot;\n     nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;\n     nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot;\n     nginx.ingress.kubernetes.io/proxy-body-size: &quot;20M&quot;\t\t// 20M 초과시 HTTP 413 방지하기 위하여\n spec:\n   rules:\n   - host: sonarqube.ccpinfra.xyz # 추가\n     http:\t# -(하이픈) 제거\n       paths:\n       - pathType: Prefix\n         path: &quot;/sonarqube&quot;\n         backend:\n           service:\n             name: sonarqube-sonarqube\n             port:\n               number: 9000\n   tls:\t# tls 이하 부분 모두 추가\n   - hosts:\n     - sonarqube.yourdomain.com # 등록한 도메인명 입력\n     secretName: secret-tls # k8s secret 리소스명 입력\n\n$ kubectl apply -f sonarqube-ingress.yaml\n\n\n\n  Nginx Ingress Controller 사용하였다.\nIngress HTTP, HTTPS(/sonarqube) -&amp;gt; sonarqube 서비스(9000)\n\n  \n    tls 설정에 도메인 입력시 IP 주소로는 Ingress 동작하지 않음\n  \n\n\n3. 인증서 적용 확인\n\n\n\n도메인 접속 및 확인\n\n\n  Jenkins의 SonarQube Server 설정 값에 기존의 IP주소로 되어있는 경우 도메인 주소로 변경 필요(http)\n\n\n\nJenkins Webhook 설정\n\nSonarQube가 검사를 끝낸 뒤 성공/실패 여부를 전달하기 위하여 SonarQube에 Jenkins로의 Webhook이 설정되어 있어야함\n\n\n\n1. SonarQube 대시보드 로그인\n\n2. Administration &amp;gt; Configuration &amp;gt; Webhooks\n\n3. Create\n\n\n  Name: 원하는 이름(jenkins-webhook)\n  URL: jenkins 주소 (http 사용할 것)\n  Secret: (선택)\n\n\n4. 빌드시 웹훅 동작 여부를 이 페이지에서 실시간으로 확인 가능\n\n\nJenkins 파이프라인 - JavaScript, TypeScript 연동\n\nJavaScript는 기본적으로 브라우저 위에서 실행되는 언어이기 때문에 이를 실행할 수 있는 프레임워크인 NodeJS가 젠킨스에 필요하다.\n\n1. NodeJS 플러그인 설치\n\n\n  Jenkins 관리 &amp;gt; 플러그인 관리 &amp;gt; 설치가능 &amp;gt; nodejs 검색\n  재시작 없이 설치하기(업데이트, 디펜던시 추가 등으로 재시작 필요시 재시작 후 설치하기)\n\n\n\n\n2. NodeJS Tools 설정\n\n\n  Jenkins 관리 &amp;gt; Global Tool Configuration\n  NodeJS &amp;gt; NodeJS installations…\n  nodejs installer 정보 입력\n    \n      Name: 원하는 이름(nodejs)\n      Install automatically: 체크\n      Install from nodejs.org\n      Version: NodeJS 14.17.0 (원하는 버전 but latest LTS 버전인 14버전 권장)\n    \n  \n  저장\n\n\n3. 적용할 pipeline에 nodejs tool 환경 추가\n\nnode (&#39;jenkins-slave&#39;){\n    ...\n    env.NODEJS_HOME = &quot;${tool &#39;nodejs&#39;}&quot;\t// 2-3 항목에서 입력한 nodejs installer 이름\n    env.PATH=&quot;${env.NODEJS_HOME}/bin:${env.PATH}&quot;\n    sh &#39;npm --version&#39;\n    ...\n\n위와같이 설정해주면 jenkins-slave에서 node, npm 명령어 사용 가능\n\n4. sonar property 추가\n\nsonarqube {\n       properties {\n                property &quot;sonar.sources&quot;, &quot;src/main&quot;\n                property &quot;sonar.tests&quot;, &quot;src/test&quot;\n       }\n}\n\n\n  위 코드는 gradle 프로젝트 build.gradle 파일 기준\n\n\n소나큐브 검증시 기본적으로 java 경로만 잡는 경우\n\nsonar.sources = src/main, sonar.tests = src/test 등으로 경로를 지정해주어야 JS 코드도 검사 가능\n\n\nSonarQube for Maven 설정\n\nMaven의 경우 설치형이기 때문에 별도로 dependency 설정이 필요하지 않다.\n\nsonar-maven-plugin을 저장소에서 가져와서 바로 실행한다.\n\n    stage(&quot;SonarQube analysis&quot;) {\n        withSonarQubeEnv(&#39;sonarqube server&#39;) {\n            sh &#39;mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.9.0.2155:sonar&#39;\n        }\n    }\n\n    stage(&quot;Quality Gate&quot;){\n        timeout(time: 1, unit: &#39;HOURS&#39;) {\n            def qg = waitForQualityGate()\n            if (qg.status != &#39;OK&#39;) {\n                error &quot;Pipeline aborted due to quality gate failure: ${qg.status}&quot;\n            }\n        }\n    }\n\n\n\nSonarQube for Gradle 설정\n\nGradle 프로젝트의 경우 보통 gradlew의 wrapper 형태로 제공되는 경우가 대부분이다.\n\n따라서 sonarqube 플러그인을 프로젝트의 build.gradle dependency에 추가해주어야한다.\n\n아래 부분 프로젝트의 build.gradle에 추가\n\n# plugin 부분 추가\n\tplugin {\n    \tid &#39;org.sonarqube&#39; version &#39;3.0&#39;\n    }\n# 또는\n    buildscript {\n        repositories {\n            jcenter()\n        }\n        dependencies {\n            classpath(&quot;org.sonarsource.scanner.gradle:sonarqube-gradle-plugin:3.0&quot;)\n        }\n    }\n    \n\tapply plugin: &#39;org.sonarqube&#39;\n\n\n# repository 추가\n\trepositories {   \t\n\t\tjcenter()\n\t}\n    \n# sonarqube 프로퍼티 추가\n    sonarqube {\n\t\tproperties {\n\t\t\tproperty &quot;sonar.sources&quot;, &quot;src/main&quot;\n\t\t}\n\t}\n    \n# multi module project의 경우\n# 아래와 같이 검사할 모듈명 명시 후 안에 적용\nproject(&quot;:makers-web&quot;) {\n\tapply plugin: &#39;org.sonarqube&#39;\n\n\tsonarqube {\n\t\tproperties {\n\t\t\tproperty &quot;sonar.sources&quot;, &quot;src/main&quot;\n\t\t}\n\t}\n}\n# 또는 전체 적용시\nsubprojects {\n\tapply plugin: &#39;org.sonarqube&#39;\n\n\tsonarqube {\n\t\tproperties {\n\t\t\tproperty &quot;sonar.sources&quot;, &quot;src/main&quot;\n\t\t}\n\t}\n}\n\n\n\n빌드시 sonarqube gradle plugin 다운로드 및 검사 확인\n\n\nSonarQube Quality Profiles 적용\n\n기본적으로 각 언어별 Sonar way라는 Default Profile이 적용되어 있음\n\n\n\n\n  sonarqube 대시보드 로그인\n  (선택) Quality Profiles &amp;gt; Create 또는 Java Sonar way 프로필 설정 &amp;gt; Copy\n    \n      Name: 원하는 이름\n      Language: 원하는 언어\n      Parent: none\n    \n  \n  새로 생성한 Profile 설정 &amp;gt; Set as Default\n  설정 &amp;gt; Activate More Rules에서 추가로 적용할 Rule 탐색\n  Activate 클릭하여 적용\n\n\n\nSonarQube GitLab 계정/그룹 연동\n\nSonarQube 로그인시 GitLab OAuth2를 사용하여 gitlab 계정 연동을 할 수 있다.\n\n1. GitLab 사이드\n\n  GitLab OAuth2 Application 등록\n    \n      Admin Area &amp;gt; Applications &amp;gt; New application\n      New application 입력\n        \n          Name: 원하는 어플리케이션 이름 입력(GitLab SonarQube)\n          Redirect URI: ${sonarqube-uri}/oauth2/callback/gitlab 입력 (public URL이어야한다.)\n          Trusted: 현재 등록하는 어플리케이션을 신뢰할 것인지? 체크\n          Confidential: Client Secret을 암호화통신할 것인지? 체크\n          Scopes: api 체크(gitlab oauth2 api 사용권한)\n        \n      \n      Submit 후 앱 정보 확인\n      Application ID, Secret 복사\n    \n  \n\n\n2. SonarQube 사이드\n\n\n\n\n  Server Base URL 설정\n    \n      Administration &amp;gt; Configuration &amp;gt; General Settings\n      Server base URL 입력\n        \n          외부 연동시 기본적으로 public 통신이기 때문에 public url 입력\n            \n              https://sonarqube.yourdomain.com/sonarqube\n            \n          \n        \n      \n      Save\n    \n  \n  ALM Integration 설정\n    \n      Administration &amp;gt; Configuration &amp;gt; ALM Integrations &amp;gt; GitLab\n      GitLab Authentication 항목 입력\n        \n          Enabled: true\n          GitLab URL: public gitlab URL 입력(https://gitlab.yourdomain.com)\n          Application ID, Secret: 1-4에서 복사한 항목 각각 입력\n          Allow users to sign-up: gitlab oauth2로 처음 로그인하는 사용자를 sonarqube에 등록할 것인지? 체크\n          Synchronize user groups: 소나큐브에 gitlab 그룹명과 일치하는 그룹이 생성되어 있다면 유저를 자동으로 등록한다. 체크\n        \n      \n    \n  \n  \n    Java caCerts 인증서 설정(Kubernetes TLS Secret)\n\n    \n      Java에서는 https 통신시 기본적으로 java keystore에 인증서를 요구한다.위에서 public 경로를 http로 설정하였으면 상관 없으나 https로 설정한 경우 진행\n\n      현재 Java 기반 서비스들은 Kubernetes 환경에 떠있으므로 해당 환경으로 진행\n      \n        기존 k8s에 kubernetes.io/tls 타입으로 떠있는 tls 타입 시크릿은 사용불가하므로 opaque 타입(기본타입) secret을 생성한다.\n        sonarqube-secret.yaml 생성\n           \t$ vim sonarqube-secret.yaml\n          \n          apiVersion: v1\nkind: Secret\nmetadata:\n  name: sonarqube-secret\ndata:\n  cert-1.crt: MIIFIzCC... # .crt(X.509 포맷)의 인증서값\n          \n          $ kubectl apply -f sonarqube-secret.yaml\n          \n        \n        helm values.yaml 수정\n          $ vim values.yaml\n          \n          ...\n caCerts:\n   image: adoptopenjdk/openjdk11:alpine\n   enabled: true\n   secret: sonarqube-secret\n...\n          \n          # helm upgrade\n$ helm upgrade sonarqube oteemocharts/sonarqube -f values.yaml\n          \n        \n      \n    \n  \n  \n    테스트\n\n    \n      로그아웃 후 메인페이지에 들어가면 아래 이미지와 같이 Log in with GitLab 버튼 생성 확인\n \n      버튼 누르면 GitLab에 로그인되어 있는 경우 자동으로 소나큐브 사용자 생성 및 연동이 완료된다.\n      gitlab 그룹명과 일치하는 그룹을 미리 생성해 놓으면 로그인시 해당 유저 그룹 자동 연동\n    \n  \n\n\n\nSonarQube 배지\n\n\n\n각 프로젝트 대시보드 &amp;gt; 우상단 Project Information\n\nmarkdown 형태로 실시간 정보 배지를 embed 가능\n\n대신 해당 배지를 사용하려면 SonarQube 프로젝트가 Public으로 설정되어야하며 이는 소스 취약점을 외부에서 누구나 볼 수 있다는 것을 의미한다.\n\n\nTroubleshooting\n\n1. 분석 권한 없음\n\n[ERROR] Error during SonarScanner execution\n[ERROR] You&#39;re not authorized to run analysis. Please contact the project administrator.\n\n\n빌드 후 검증 수행시 위와 같은 오류가 날 때\n\nSonarQube 플러그인 설치 및 실행은 완료되었으나 Jenkins - SonarQube 연동 계정이 Analysis 실행 권한이 없는 경우이다.\n\n\n\n\n  SonarQube 대시보드 로그인(관리자 계정 필요)\n  Administration &amp;gt; Security &amp;gt; Global Permissions\n  연동된 유저가 속한 그룹 혹은 유저 자체에 Execute Analysis 권한 체크\n\n\n2. class 파일 없음\n\n[ERROR] Failed to execute goal org.sonarsource.scanner.maven:sonar-maven-plugin:3.7.0.1746:sonar (default-cli) on project sample-project: Your project contains .java files, please provide compiled classes with sonar.java.binaries property, or exclude them from the analysis with sonar.exclusions property. -&amp;gt; [Help 1]\n\n\npipeline 또는 job 실행 순서에 빌드 이전에 분석을 시도하고 있는지 체크할 것.\n\nJava의 경우 sonarqube는 신뢰도를 높이기 위하여 .java 파일만으로 코드 분석을 하지 않고 .java와 .class 파일을 함께 분석한다고 한다.\n\n\n  https://docs.sonarqube.org/latest/analysis/languages/java/#header-2 참조\n\n\n따라서 컴파일된 .class 파일이 있어야하므로 maven 또는 gradle 플러그인을 사용하여야함.\n\n만약 없을 경우, 수동으로 컴파일하여 .class 파일을 넣어줘야함.\n\n3. 소스 분석시 HTTP 413 REQUEST ENTITY TOO LARGE\n\n소스 분석시 HTTP 413 REQUEST ENTITY TOO LARGE가 나오는 경우\n\n검증할 소스가 sonarqube의 max body size(기본값: 20m)를 초과하여 나오는 경우로\n\n아래와 같이 sonarqube-ingress.yaml에 proxy body size 설정\n\n...\n   apiVersion: networking.k8s.io/v1\n   kind: Ingress\n    metadata:\n      name: sonarqube\n      namespace: default\n      annotations:\n        kubernetes.io/ingress.class: &quot;nginx&quot;\n        nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;\n        nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot;\n        nginx.ingress.kubernetes.io/proxy-body-size: &quot;20M&quot;\t\t// 20M 초과시 HTTP 413 방지하기 위하여\n...\n\n\n\n\nReference\n\n\n  SonarScanner for Jenkins\n  SonarQube Scanner for Jenkins\n  SonarQube GitLab Integration\n\n"
} ,
  
  {
    "title"    : "SonarQube 설치 및 Jenkins pipeline 연동하기",
    "category" : "",
    "tags"     : " Sonarqube, Jenkins, Helm, Docker, Docker-Compose",
    "url"      : "/2021/05/07/sonarqube-jenkins.html",
    "date"     : "May 7, 2021",
    "excerpt"  : "SonarQube 설치 및 Jenkins pipeline 연동하기\n\n\n\nSonarQube Documentation\n\n본 문서에서는 Sonarqube 설치 및 Jenkins 파이프라인 연동한 내용을 정리하였다.\n\n\nSonarQube란\n\n\n\n소스 품질 관리를 위한 자동화된 정적 코드 검증/분석 툴\n\n  버그 탐색\n  취약점 탐색\n  코드 냄새(Code Smell) 탐색\n  보안 핫스팟 탐색\n\n\n안전(Safe) 하고 깨끗한(Clean) 코드를 ...",
  "content"  : "SonarQube 설치 및 Jenkins pipeline 연동하기\n\n\n\nSonarQube Documentation\n\n본 문서에서는 Sonarqube 설치 및 Jenkins 파이프라인 연동한 내용을 정리하였다.\n\n\nSonarQube란\n\n\n\n소스 품질 관리를 위한 자동화된 정적 코드 검증/분석 툴\n\n  버그 탐색\n  취약점 탐색\n  코드 냄새(Code Smell) 탐색\n  보안 핫스팟 탐색\n\n\n안전(Safe) 하고 깨끗한(Clean) 코드를 유지하게끔 도움을 줌(소스코드 품질 관리)\n\n\n  커밋/머지(SCM)\n  체크아웃, 빌드, 테스트(CI/CD)\n  분석/검증(SonarQube)\n\n\n\n  Sonarlint: 코딩/컴파일시 소스 품질 관리를 위한 IDE 플러그인\n\n\n\nSonarQube 설치\n\n두 가지 방법으로 설치해 보았다.\n\n\n  Using Helm Chart\n    \n      k8s 위에서 관리하는 자원의 형태로 사용시\n    \n  \n  Using Docker Compose\n    \n      호스트 위에서 컨테이너형으로 간단하게 사용시\n    \n  \n\n\nUsing Helm Chart\n\n\n  k8s helm을 이용하여 설치하는 방법\n\n\n1. Helm 설치\n\n# Ubuntu\n$ curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -\n$ sudo apt-get install apt-transport-https --yes\n$ echo &quot;deb https://baltocdn.com/helm/stable/debian/ all main&quot; | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list\n$ sudo apt-get update\n$ sudo apt-get install helm\n\n\n2. Helm Chart values.yaml 가져오기\n\n   # helm 저장소 추가\n   $ helm repo add oteemocharts https://oteemo.github.io/charts\n   $ wget https://raw.githubusercontent.com/Oteemo/charts/master/charts/sonarqube/values.yaml\n\n\n\n  현시점 기준 8.5.1-community\n\n\n3. ingress 리소스 설정\n\n   $ vim sonarqube-ingress.yaml\n\n   apiVersion: networking.k8s.io/v1\n   kind: Ingress\n    metadata:\n      name: sonarqube\n      namespace: default\n      annotations:\n        kubernetes.io/ingress.class: &quot;nginx&quot;\n        nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;\n        nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot;\n    spec:\n      rules:\n      - http:\n          paths:\n          - pathType: Prefix\n            path: &quot;/sonarqube&quot;\n            backend:\n              service:\n                name: sonarqube-sonarqube\n                port:\n                  number: 9000\n\n   $ kubectl apply -f sonarqube-ingress.yaml\n\n\n  /sonarqube context로 설정\n\n  \n    Ingress HTTP(/sonarqube) -&amp;gt; sonarqube 서비스(9000)\n  \n\n\n4. PersistenceVolume, PersistenceVolumeClaim 설정\n\n   $ vim sonarqube-storage.yaml\n\n    kind: PersistentVolume\n    apiVersion: v1\n    metadata:\n      name: sonarqube-pv-volume\n    spec:\n      storageClassName: manual\n      capacity:\n        storage: 25Gi\n      accessModes:\n        - ReadWriteMany\n      hostPath:\n        path: &quot;/nfs_nas/volumes/sonarqube/data&quot;\n    ---\n    kind: PersistentVolumeClaim\n    apiVersion: v1\n    metadata:\n      name: sonarqube-pv-claim\n    spec:\n      storageClassName: manual\n      accessModes:\n        - ReadWriteMany\n      resources:\n        requests:\n          storage: 25Gi\n    ---\n    kind: PersistentVolume\n    apiVersion: v1\n    metadata:\n      name: sonarqube-db-pv-volume\n    spec:\n      storageClassName: manual\n      capacity:\n        storage: 25Gi\n      accessModes:\n        - ReadWriteMany\n      hostPath:\n        path: &quot;/nfs_nas/volumes/sonarqube/postgres&quot;\n    ---\n    kind: PersistentVolumeClaim\n    apiVersion: v1\n    metadata:\n      name: sonarqube-db-pv-claim\n    spec:\n      storageClassName: manual\n      accessModes:\n        - ReadWriteMany\n      resources:\n        requests:\n          storage: 25G\n\n   $ kubectl apply -f sonarqube-storage.yaml\n\n\nsonarqube에서 사용할 volume을 지정해주기 위하여 설정\n\n용량과 hostPath는 사용 환경에 맞추어 변경할 것\n\n5. values.yaml 수정\n\n아래 해당하는 부분 모두 수정\n\n    ...\n    readinessProbe:\n      initialDelaySeconds: 60\n      periodSeconds: 30\n      failureThreshold: 6\n      sonarWebContext: /sonarqube/ # 변경(끝에 &#39;/&#39; 포함)\n    livenessProbe:\n      initialDelaySeconds: 60\n      periodSeconds: 30\n      sonarWebContext: /sonarqube/ # 변경(끝에 &#39;/&#39; 포함)\n    ...\n    # 사용할 플러그인 추가(아래 기본 플러그인은 이미 있으므로 충돌남 외부 플러그인만 적용할 것, 아래는 잘못된 예시)\n    plugins:\n      install: [\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-java-plugin/sonar-java-plugin-6.9.0.23563.jar&quot;,\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-javascript-plugin/sonar-javascript-plugin-7.4.3.15529.jar&quot;,\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-xml-plugin/sonar-xml-plugin-2.2.0.2973.jar&quot;,\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-css-plugin/sonar-css-plugin-1.4.2.2002.jar&quot;,\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-html-plugin/sonar-html-plugin-3.4.0.2754.jar&quot;,\n        &quot;https://binaries.sonarsource.com/Distribution/sonar-typescript-plugin/sonar-typescript-plugin-2.1.0.4359.jar&quot;\n        ]\n      lib: []\n    ...\n    env:\n      - name: SONAR_WEB_CONTEXT # 추가\n        value: /sonarqube\n    ...\n    persistence:\n      enabled: true\n      annotations: {}\n      existingClaim: sonarqube-pv-claim # 추가\n    ...\n    postgresql:\n      enabled: true\n      ...\n      persistence:\n        enabled: true\n        existingClaim: sonarqube-db-pv-claim # 추가\n    ...\n\n\n6. helm install\n   # helm v3\n   # helm install [name] [chart] [-f:value]\n   $ helm install sonarqube oteemocharts/sonarqube -f values.yaml\n   \n   # helm v2\n   # helm install [--name:name] [chart] [-f:values]\n   $ helm install --name sonarqube oteemocharts/sonarqube -f values.yaml\n\n\n참고\n\nhelm uninstall\n\n   # helm v3\n   # helm uninstall [name]\n   $ helm uninstall sonarqube\n   \n   # helm v2\n   # helm del [--purge] [name]\n   $ helm del --purge sonarqube\n\n\nhelm upgrade\n\n   # helm upgrade [release] [chart] [-f:values]\n   $ helm upgrade sonarqube oteemocharts/sonarqube -f values.yaml\n\n\n7. kubernetes pods 확인 및 대시보드 접속\n   $ kubectl get pods -A | grep sonarqube\n   $ kubectl logs [pod name]\n   $ curl localhost:9000\n\n\n  http://external-ip:port/sonarqube 접속 확인초기 계정: admin/admin\n\n  접속 후 admin 비밀번호 변경 또는 비활성화 할 것\n\n\nUsing Docker-Compose\n\n1. docker-compose.yaml 작성\n    version: &quot;3.1&quot;\n    services:\n      sonarqube:\n        image: sonarqube:latest\n        restart: always\n        depends_on:\n          - sonarqube-db\n        container_name: sonarqube\n        ports:\n          - &quot;9000:9000&quot;\n        networks:\n          - sonarnet\n        environment:\n          TZ: Asia/Seoul\n          SONAR_HOME: /opt/sonarqube\n          SONAR_JDBC_USERNAME: sonar\n          SONAR_JDBC_PASSWORD: sonar\n          SONAR_JDBC_URL: jdbc:postgresql://sonarqube-db:5432/sonar\n        ulimits:\n          nofile:\n            soft: 65536\n            hard: 65536\n          memlock:\n            soft: -1\n            hard: -1\n        volumes:\n          - /nfs_nas/volumes/sonarqube/data:/opt/sonarqube/data\n          - /nfs_nas/volumes/sonarqube/extensions:/opt/sonarqube/extensions\n          - /nfs_nas/volumes/sonarqube/logs:/opt/sonarqube/logs\n          - /nfs_nas/volumes/sonarqube/temp:/opt/sonarqube/temp\n\n      sonarqube-db:\n        image: postgres\n        container_name: sonarqube-db\n        networks:\n          - sonarnet\n        environment:\n          TZ: Asia/Seoul\n          POSTGRES_USER: sonar\n          POSTGRES_PASSWORD: sonar\n        volumes:\n          - /nfs_nas/volumes/sonarqube/postgres:/var/lib/postgresqli/data\n\n\n2. docker-compose 실행 및 접속 확인\n   $ docker-compose up -d\n   $ docker-compose ps\n   $ curl localhost:9000\n\n\n\n  외부 접속의 경우 포트포워딩 필요\n\n  \n    이 부분은 각 인프라의 환경별로 상이하기 때문에 별도로 기술하지는 않는다.\n  \n\n\n\n\nJenkins 연동\n\n1. SonarQube 사이드\n\n\n  SonarQube Web 접속\n  Administration &amp;gt; Security &amp;gt; Users\n  (선택) admin 계정 deactivate 및 새 관리자 계정 설정\n    \n      admin 계정의 설정 &amp;gt; Deactivate\n      Create User\n      생성된 계정의 Groups 리스트 &amp;gt; Unselected &amp;gt; sonar-administrators 체크\n    \n  \n  위의 관리자 계정에서 Token 메뉴 진입\n  Generate Tokens &amp;gt; Token Name(jenkins-token) 입력 후 Generate\n    \n      SonarQube 입장에서는 Jenkins에서 사용할 토큰이므로 jenkins-token이라고 이름 지음\n    \n  \n\n\n\n\n\n  생성된 토큰 복사\n    \n      토큰값은 생성시에만 볼 수 있으므로 필히 복사 및 다른 곳에 저장할 것\n    \n  \n\n\n2. Jenkins 사이드\n\n\n  SonarQube 플러그인 설치\n    \n      Jenkins 관리 &amp;gt; 플러그인 관리\n      SonarQube 검색 &amp;gt; SonarQube Scanner for Jenkins 설치\n    \n  \n  SonarQube 토큰 등록\n    \n      Jenkins 관리 &amp;gt; Manage Credentials\n      아래 Stores scoped to Jenkins의 Domains 항목 클릭\n      Add Credentials\n        \n          Kind: Secret text\n          Scope: Global\n          Secret: 위 SonarQube에서 생성했던 토큰 붙여넣기\n          ID: 원하는 ID(sonarqube-token) 입력\n        \n      \n    \n  \n  SonarQube 서버 연동\n    \n      Jenkins 관리 &amp;gt; 시스템 설정 &amp;gt; SonarQube servers\n      Environment variables 체크\n        \n          SonarQube servers 항목에 입력하는 config 정보들을 Jenkins에서 환경변수로 사용할 것인지 확인하는 항목\n        \n      \n    \n\n    \n  \n  SonarQube Installations 항목 입력\n    \n      Name: 원하는 서버 이름 입력(ex: sonarqube server)\n      Server URL: SonarQube 서버 주소 입력\n      Server authentication token: 2번 항목에서 생성한 토큰 선택\n    \n  \n\n\n3. Jenkins Pipeline\n\n테스트용 메이븐 프로젝트 기준으로 작성\n\nGradle 프로젝트 또는 다른 설정을 보려면 SonarQube for Gradle을 참고\n\nSonarQube와 연동할 파이프라인 스크립트에 아래와 같이 Stage 추가\n\n    stage(&quot;SonarQube analysis&quot;) {\n        withSonarQubeEnv(&#39;sonarqube server&#39;) {\n            sh &#39;mvn org.sonarsource.scanner.maven:sonar-maven-plugin:3.9.0.2155:sonar&#39;\n        }\n    }\n\n    stage(&quot;Quality Gate&quot;){\n        timeout(time: 1, unit: &#39;HOURS&#39;) {\n            def qg = waitForQualityGate()\n            if (qg.status != &#39;OK&#39;) {\n                error &quot;Pipeline aborted due to quality gate failure: ${qg.status}&quot;\n            }\n        }\n    }\n\n\nStage 설명\n\n  \n    &quot;SonarQube analysis&quot;: mvn 빌드 파일 기준 SonarQube 검사 진행\n  \n  \n    &quot;Quality Gate&quot;: 검사한 코드의 품질을 통해 통과/미통과 판별(Quality Gate 프로파일 설정은 SonarQube에서 진행)\n  \n\n\n\n  withSonarEnv()에는 위의 설정에서 Jenkins에서 설정한 SonarQube Server 이름으로 설정해야함Qulaity Gate의 경우 SonarQube에서 Jenkins로 Webhook이 설정되어 있어야함 (Webhook 설정 참고)\n\n\n\n\nJenkins 빌드시 각 Stage 정상 통과 및 SonarQube Webhook 동작 여부 확인\n\n\n  연동 성공시 Build History에 SonarQube 대시보드로 이동할 수 있는 아이콘이 나옴\n\n\n\n\nReference\n\n\n  SonarScanner for Jenkins\n  SonarQube Scanner for Jenkins\n\n"
} ,
  
  {
    "title"    : "Kubernetes Nginx Ingress 적용기",
    "category" : "",
    "tags"     : " Kubernetes, Ingress, Ingress Controller, Certificate, TLS, SSL, Nginx, Security",
    "url"      : "/2021/04/03/kubernetes-ingress.html",
    "date"     : "April 3, 2021",
    "excerpt"  : "Kubernetes Nginx Ingress 적용기\n\n\n\nKubernetes Ingress\n\n본 문서는 온프레미스 환경에서의 Ingress, SSL 인증서 적용 내용을 정리하였다.\n\nnginx ingress controller를 이용하여 jenkins 서비스에 연결하는 과정을 정리하였다.\n\n\n선행 사항\n\n  Let’s Encrypt 인증서 발급\n  Kubernetes 환경 구축\n  Ingress controller 설치\n\n\n\nIngress...",
  "content"  : "Kubernetes Nginx Ingress 적용기\n\n\n\nKubernetes Ingress\n\n본 문서는 온프레미스 환경에서의 Ingress, SSL 인증서 적용 내용을 정리하였다.\n\nnginx ingress controller를 이용하여 jenkins 서비스에 연결하는 과정을 정리하였다.\n\n\n선행 사항\n\n  Let’s Encrypt 인증서 발급\n  Kubernetes 환경 구축\n  Ingress controller 설치\n\n\n\nIngress 개요\n\nflowchart LR\n  A[User]\n  subgraph Kubernetes Cluster\n  subgraph Ingress\n  B[Service: Ingress Contoller]\n  J[Pod: Ingress Controller]\n  F[Resource: Ingress]\n  end\n  subgraph Deployment1\n  C[Service1]\n  G[Pod1]\n  end\n  subgraph Deployment2\n  D[Service2]\n  H[Pod2]\n  end\n  subgraph Deployment3\n  E[Service3]\n  I[Pod3]\n  end\n  end\n  A -.Ingress.-&amp;gt; B\n  B---J\n  C---G\n  D---H\n  E---I\n  B---F\n  B --&amp;gt; C &amp;amp; D &amp;amp; E\n\n\n외부에서 클러스터에 접근할 때 요청받는 것이 Ingress이다. 쉽게 얘기하면 일반적인 proxy, gateway라고 보면 된다.\n\n동일하게 로드밸런서, 서비스 메쉬의 역할도 수행하며 경우에 따라서는 Blue Green 배포나 Canary 배포도 가능하다.\n\n외부 → Ingress → Service → Pod\n\n내부 서비스에 SSL 인증서를 적용하는 것이 아닌 Ingress에 인증서를 적용한다.\n\n이와 같이 적용하면 서비스 종속성 없이 앞단에서 인증서 적용이 가능하므로 뒷단의 서비스가 추가되어도 쉽게 적용이 가능하다.\n\nIngress Controller와 Ingress Resource의 차이\n\n\n\nIngress Controller\n\n\n\n\n  이미지: Certified Kubernetes Administrator(CKA) with Practice Tests 발췌\n\n\n\n  80:31280/TCP, 443:32443/TCP등의 서비스와 함께 NodePort 형식으로 외부와의 연결을 담당\n  보통 nginx, haproxy 등 proxy 서버로 구현되어 있다.\n  Ingress Resource의 설정값(Ingress Rule)에 따라 클러스터 내의 Service로 연결한다.\n\n\nIngress Resource\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n...\nspec:\n  rules:\n  - host: service.yourdomain.com # 이 부분 추가 (하이픈은 맨 위에만 있어야함)\n    http:\n      paths:\n      - pathType: Prefix\n        path: &quot;/service&quot;\n        backend:\n          service:\n            name: service-name\n            port:\n              number: 8080\n\n\n\n  Ingress Controller에서 어떤 서비스로 라우팅할 것인지 규칙을 명세한 Resource이다.\n  path등의 값으로 어떤 service에 연결할 것인지에 대한 명세이다.\n  Kubernetes secret 리소스를 참조하여 tls 연결을 설정할 수 있다.\n\n\n\nIngress 적용\n\n인증서 파일 구성\n\nPEM 포맷의 RSA crt, key가 필요하다.\n\n본 문서에서는 Let’s Encrypt의 DNS 포맷으로 발급된 *.yourdomain.com 도메인 인증서를 사용한다.\n\n# yourdomain.com.crt\n-----BEGIN CERTIFICATE-----\nMII...\n-----END CERTIFICATE-----\n\n# yourdomain.com.crt\nPrivate-Key: (2048 bit)\nmodulus:\n...\n-----BEGIN RSA PRIVATE KEY-----\nMII...\n-----END RSA PRIVATE KEY-----\n\n\nKubernetes secret 생성\n\nkubernetes에서는 여러가지 시크릿 타입을 제공하는데\n\n그 중 SSL 인증서에 대한 정보를 담고 있는 Secret 타입은 kubernetes.io/tls 이다.\n\nsecret 생성하는 두 가지 방법이 존재\n\n1. secret yaml 파일 생성\n\n$ vim service-secret.yaml\n\napiVersion: v1\nkind: Secret\nmetadata:\n  name: secret-tls\ntype: kubernetes.io/tls\ndata:\n  # the data is abbreviated in this example\n  tls.crt: |\n        MIIC2DCCAcCgAwIBAgIBATANBgkqh ...\n  tls.key: |\n        MIIEpgIBAAKCAQEA7yn3bRHQ5FHMQ ...\n\n\n이 경우 인증서 값은 —–BEGIN CERTIFICATE—–와 —–END CERTIFICATE—– 사이에 있는 인코딩값을 넣어야한다고 한다.\n\n\n  확인결과 .crt 안에 있는 인증서 값을 모두 포함하여야함\n\n\n$ kubectl create -f service-secret.yaml\n\n\nkubernetes secret 생성\n\n2. 직접 생성(권장)\n\n# kubectl create secret {type} {name} --cert {certPath} --key {keyPath}\n$ kubectl create secret tls secret-tls --cert ssl/yourdomain.com.crt --key ssl/yourdomain.com.key\n\n\nImperative 방식으로 생성\n\n인증서 파일을 설정값으로 물고갈 수 있어서 권장\n\n# secret 생성 확인\n$ kubectl get secret\n$ kubectl describe secret secret-tls\n\n\nIngress TLS 적용\n\n$ vim jenkins-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n...\nspec:\n  rules:\n  - host: jenkins.yourdomain.com # 이 부분 추가 (하이픈은 맨 위에만 있어야함)\n    http: \n      paths:\n      - pathType: Prefix\n        path: &quot;/jenkins&quot;\n        backend:\n          service:\n            name: jenkins\n            port:\n              number: 8080\n  # 아래 부분 추가\n  tls:\n  - hosts:\n    - jenkins.yourdomain.com\n    secretName: secret-tls\n\n\n# 적용\n$ kubectl apply -f jenkins-ingress.yaml\n\n위에서 생성한 Secret을 이용하면 Ingress에 인증서를 적용할 수 있다.\n\n인증서 적용 확인\n\n1. Ingress Controller 접속 확인\n\n# ingress-nginx 서비스 포트매핑 확인\n$ kubectl get svc -A\n\nNAMESPACE       NAME                                 TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)                      AGE\ningress-nginx   ingress-nginx-controller             NodePort    10.104.33.237    &amp;lt;none&amp;gt;        80:31623/TCP,443:32452/TCP   29h\n\n$ curl -k -v https://10.104.33.237:443\n...\n* Connected to 10.104.33.237 (10.104.33.237) port 443 (#0)\n...\n\n\n2. 외부 도메인 접속 확인\n\n$ curl -k -v https://jenkins.yourdomain.com:32452\n...\n* Connected to jenkins.yourdomain.com (...) port 32452 (#0)\n...\n\n\n3. 인증서 검증\n\n$ openssl s_client -debug -connect https://jenkins.yourdomain.com:32452\n\n...\nsubject=CN = *.yourdomain.com\n\nissuer=C = US, O = Let&#39;s Encrypt, CN = R3\n...\n---\n\n\n4. 웹 브라우저로 접속하여 인증서 확인\n\n\n\n\nSSL Redirect 끄기\n\n위처럼 Ingress 설정에 tls를 적용하면 https 접속이 강제되므로 기존에 http에 운영중인 서비스에 영향이 있다.\n\n따라서 기존의 http 서비스는 https로 redirect 되지 않도록 설정하여야 한다.\n\n1. Ingress rule 변경\n\n$ vim jenkins-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  ...\n  namespace: default\n  # 아래 annotations 추가\n  annotations:\n    kubernetes.io/ingress.class: &quot;nginx&quot;\n    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;\n    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot;\n\n\n\n  Ingress rule의 ssl-redirect 설정은 host 주소와 port는 그대로 둔채 http 요청을 https로 리다이렉트한다.(서버에서 https redirect 응답)\n\n  예시) http://jenkins.yourdomain.com:31623/jenkins → https://jenkins.yourdomain.com:31623/jenkins\n\n\n2. ConfigMap 설정 변경\n\n# vim ~/ingress-controller/ingress-nginx/deploy/static/provider/baremetal/deploy.yaml\n\n# Source: ingress-nginx/templates/controller-configmap.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-3.23.0\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: ingress-nginx\n    app.kubernetes.io/version: 0.44.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: ingress-nginx-controller\n  namespace: ingress-nginx\n# 아래 부분 추가\ndata:\n  hsts: &quot;false&quot;\n  ssl-redirect: &quot;false&quot;\n\n\n\n  Ingress controller의 ssl-redirect 설정은 host 주소는 그대로지만 https의 default port인 443으로 redirect한다.\n\n  예시) http://jenkins.yourdomain.com:31623/jenkins → https://jenkins.yourdomain.com/jenkins\n\n\ningress-controller의 baremetal template에 있는 ConfigMap으로 띄워져 있으므로 해당 부분 수정\n\n만약 다른 환경으로 인하여 ConfigMap이 없는 경우 새로 생성하여 적용할 것\n\n$ kubectl apply -f deploy.yaml\n\n설정 변경 적용\n\n추가로 적용기간에는 http와 https의 포트를 서로 다르게하여 둘 다 접속 가능하도록 만드는 것이 옳다.\n\n현재 http와 https를 동시 서비스 중이기 때문에 서로 다른 포트에서 접근을 한다.\n\n따라서 수동으로 Host 주소와 port를 변경해주어야 한다.\n\n3. Ingress rule 변경\n\n$ vim jenkins-ingress.yaml\n\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  ...\n  namespace: default\n  # 아래 annotations 추가\n  annotations:\n  \tkubernetes.io/ingress.class: &quot;nginx&quot;\n    nginx.ingress.kubernetes.io/proxy-redirect-from: http://jenkins.yourdomain.com:31623\n    nginx.ingress.kubernetes.io/proxy-redirect-to: https://jenkins.yourdomain.com:32452\n    nginx.ingress.kubernetes.io/ssl-redirect: &quot;false&quot;\n    nginx.ingress.kubernetes.io/force-ssl-redirect: &quot;false&quot;\n\n\n$ curl -v http://jenkins.jenkins.yourdomain.com:32452\n* About to connect() to jenkins.yourdomain.com port 32452 (#0)\n*   Trying ...\n* Connected to jenkins.yourdomain.com (...) port 32425 (#0)\n&amp;gt; GET /jenkins HTTP/1.1\n&amp;gt; User-Agent: curl/7.29.0\n&amp;gt; Host: jenkins.yourdomain.com:32452\n&amp;gt; Accept: */*\n&amp;gt;\n&amp;lt; HTTP/1.1 302 Found\n&amp;lt; Date: Fri, 05 Mar 2021 04:51:59 GMT\n&amp;lt; Content-Length: 0\n&amp;lt; Connection: keep-alive\n&amp;lt; Location: https://jenkins.yourdomain.com:32452/\n\n테스트해보면 redirect가 정상적으로 되는 것을 확인할 수 있다.\n\n크롬의 경우 HSTS 기능 끄기\n\n위와 같이 설정해도 크롬과 같은 웹브라우저의 경우 HSTS 기능때문에 웹브라우저 레벨에서\n\n위에서 설정한 proxy가 아니라 호스트와 포트는 그대로인 상태로 http → https 리다이렉션이 강제된다.\n\n\n  http 접속을 한 후에 서버에서 https 응답을 주는데 http 접속 이전에 브라우저에서 https로 변경하기 때문이다.\n\n\n\n시크릿 모드로 들어가서 테스트하거나 이미 크롬에 설정된 HSTS 설정을 제거해야한다.\n\n\n  chrome://net-internals/#hsts 접속\n  Query HSTS/PKP domain에서 HSTS 설정되어 있는지 검색\n\n  Delete domain security policies에서 해당 도메인 HSTS 설정 지우기\n\n\n이후 크롬을 통해 재접속하면 https redirect가 성공적으로 되는 것을 확인할 수 있다.\n\n\n참고사항\n\n  Ingress controller 기본 인증서를 가져간다?\n\n\n* successfully set certificate verify locations:\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\n\n\n위처럼 Ingress Controller에서 기본 설정 SSL Cert를 가져간다고 나오는데 Ingress Controller에서 SSL을 설정하는 것이 아닌 Ingress의 Secret 설정에 있는 인증서 파일을 적용하는 것이기 때문에 무시해도 된다.\n\n\nTroubleshooting\n\nIngress controller 기본 인증서를 가져간다?\n\n* successfully set certificate verify locations:\n*   CAfile: /etc/ssl/certs/ca-certificates.crt\n  CApath: /etc/ssl/certs\n\n\n위처럼 Ingress Controller에서 기본 설정 SSL Cert를 가져간다고 나오는데 Ingress Controller에서 SSL을 설정하는 것이 아닌 Ingress의 Secret 설정에 있는 인증서 파일을 적용하는 것이기 때문에 무시해도 된다.\n\nKubernetes Ingress Controller Fake Certificate 인증서가 적용될 때\n\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            0a:2a:7b:52:02:51:fe:7d:ff:ad:65:ea:41:8a:95:44\n        Signature Algorithm: sha256WithRSAEncryption\n        Issuer: O = Acme Co, CN = Kubernetes Ingress Controller Fake Certificate\n\n\nIngress에 tls 설정을 추가할 경우 기본적으로 해당 서비스는 https를 사용하게 되며 인증서 설정을 별도로 해주지 않은 경우 Ingress Controller fake certificate를 기본적으로 사용하게 된다.\n\n이 경우는 위에서 설정한 인증서 적용이 안 된 경우이다.\n\n1. kubernetes secret에서 cert와 key를 잘 물고 갔는지 확인\n\n2. Ingress yaml 파일에 tls 적용이 제대로 되었는지 확인\n\n\n  ssl을 적용할 경우 연동되는 서비스와 tls 설정에 host 이름을 꼭 추가해주어야한다.\n\n\n\n\nReference\n\n\n  Ingress\n  Ingress Controllers\n  Secret\n  Certified Kubernetes Administrator(CKA) with Practice Tests\n\n"
} ,
  
  {
    "title"    : "Let&#39;s Encrypt 인증서 발급 및 갱신",
    "category" : "",
    "tags"     : " Certificate, Certbot, Domain, DNS",
    "url"      : "/2021/02/25/letsencrypt-renew.html",
    "date"     : "February 25, 2021",
    "excerpt"  : "Let’s Encrypt 인증서 발급 및 갱신\n\n\n\nLet’s Encrypt Docs\n\n본 문서는 무료 SSL 인증서를 제공하는 Let’s Encrypt를 사용하여 인증서 발급 및 갱신을 하는 방법을 정리하였다.\n\n\n사전 준비 사항\n\n\n  SSL 인증서 적용을 위해서는 도메인 구매 필수\n  구입한 도메인과 웹서버 연결: 가비아와 같은 도메인 구매 기관에서 설정 필요\n  root 유저 상태로 가정하고 진행\n\n\n인증서 발급 방법은 수동으로 c...",
  "content"  : "Let’s Encrypt 인증서 발급 및 갱신\n\n\n\nLet’s Encrypt Docs\n\n본 문서는 무료 SSL 인증서를 제공하는 Let’s Encrypt를 사용하여 인증서 발급 및 갱신을 하는 방법을 정리하였다.\n\n\n사전 준비 사항\n\n\n  SSL 인증서 적용을 위해서는 도메인 구매 필수\n  구입한 도메인과 웹서버 연결: 가비아와 같은 도메인 구매 기관에서 설정 필요\n  root 유저 상태로 가정하고 진행\n\n\n인증서 발급 방법은 수동으로 certbot 설치 후 발급 진행 하는 방법과 docker image로 발급하는 방법이 있다.\n\n\ndocker image로 인증서 발급 방법\ndocker volume path의 mount 지점에 인증서가 생성된다.\n$ docker run -it --rm --name certbot -v &#39;/etc/letsencrypt:/etc/letsencrypt&#39; -v &#39;/var/lib/letsencrypt:/var/lib/letsencrypt&#39; certbot/certbot certonly -d *.yourdomain.com --manual --preferred-challenges dns --server https://acme-v02.api.letsencrypt.org/directory\n\n\ncertbot 설치 후 발급 방법\n\n\n  Ubuntu 20.04 LTS 대상 certbot 설치 시 ppa repo는 deprecated 됨.\n\n  \n    nginx + ubuntu 20.04 환경 시 certbot 설치 매뉴얼 참조하여 설치 진행 참고: https://certbot.eff.org/lets-encrypt/ubuntufocal-nginx\n  \n\n\n\nUbuntu 20.04 환경에서 certbot 설치\n# 필요 시 snapd package 설치. \n# ubuntu 16.04 부터 기본 설치 되어 있음.(https://snapcraft.io/docs/installing-snap-on-ubuntu)\n\n# snap 최신으로 업데이트\n$ sudo snap install core; sudo snap refresh core\n\n# 기존 certbot 설치되어 있으면 삭제\n$ sudo apt-get remove certbot\n\n# certbot 설치\n$ sudo snap install --classic certbot\n\n# certbot 실행파일 link 설정\n$ sudo ln -s /snap/bin/certbot /usr/bin/certbot\n\n# certbot 버전 확인\n$ certbot --version\n\n\nUbuntu\n$ apt-get update\n$ apt-get install software-properties-common\n$ add-apt-repository ppa:certbot/certbot\n$ apt-get update\n$ apt-get install certbot\n\n\nCentOS\n$ sudo yum install epel-release -y\n$ sudo yum install certbot -y\n\n\nOption) certbot 웹서비스 플러그인 설치\n\n# apach 인 경우\n$ apt-get install python-certbot-apach\n\n# nginx 인 경우\n$ apt-get install python-certbot-nginx\n\n\n\n인증서 발급\n\n\n  webroot, standalone, dns의 3가지 방식 중 dns 방식으로 설치 진행\n  DNS 방식 인증서는 와일드카드 url 지원하므로 *.yourdomain.com 형식으로 발급 요청\n  certonly 옵션으로 인증서만 발급 → 웹 서비스의 설정 파일 자동으로 변경하지 않음\n  standalone 방식으로 발급 시 web serivce를 잠시 중단해야 함\n  하루 동안 3번만 발급 가능하므로 발급 시 주의 필요\n\n\nmanual 옵션으로 dns 방식 지정\n\n# dns 방식 인증서 발급 \n$ certbot certonly --manual --preferred-challenges dns-01 --server https://acme-v02.api.letsencrypt.org/directory --agree-tos -m csupreme19@gmail.com -d *.yourdomain.com\n\n\nDNS TXT 값 표시되는 화면에서 엔터 눌러 진행하지 않고 대기\n\ndns 방식 인증서 발급 command 입력 후 더 이상 진행하지 않고 대기한 후 아래 도메인 설정 등을 진행한다.\n\n\n\nDNS TXT record 값 설정\n\n도메인 구입한 곳(가비아 등)의 DNS 관리메뉴로 진입하여 dns txt record 값을 설정.\n\n예를 들어 가비아의 경우 아래 그림과 같이 ‘DNS 레코드 추가’ 메뉴에서 TXT 레코드를 추가 가능\n\n\n\n호스트 컬럼에는 _acme-challenge 만 입력\n\n가비아의 경우 설정한 값 예시는 아래 이미지 참조. record 값 앞/뒤로 쌍따옴표 추가.\n\n\n\n해당 record 설정 후 저장하여 반영. 아래 command로 도메인 서버에 레코드가 반영 되었는지 확인 가능.\n\n도메인 서버 레코드 확인\n\n$ nslookup -type=txt _acme-challenge.yourdomain.com\n# 또는\n$ dig +noall +answer _acme-challenge.yourdomain.com txt\n\n\n정상 반영 시 아래 이미지와 같이 확인 가능.\n\n\n\n이후 앞 과정의 dns 방식 인증서 발급 대기 화면에서 enter를 눌러 진행\n\n\n\n인증서 저장 위치\n\n인증서 생성 후 저장되는 위치:\n\n\n  Certificate is saved at: /etc/letsencrypt/live/{domain name}/fullchain.pem\n  Key is saved at:         /etc/letsencrypt/live/{domain name}/privkey.pem\n\n\n인증서 확인\n\n$ certbot-auto certificates\n\n\n인증서 삭제\n\n$ certbot-auto delete\n\n\n\n인증서 갱신\n\nLet’s Encrypt의 경우 무료이지만 인증서 유효 기간이 3개월로 짧다.\n\n인증서 만료 전 반드시 갱신을 해야만 site에 접속이 안되는 현상을 방지할 수 있다.\n\ngoogle calendar에 3개월 주기로 알람을 등록하여 인증서 갱신 관리는 하는 것을 추천.\n\n또한 인증서를 manual 로 생성했을 경우 certbot 자동 갱신 명령어로는 갱신 되지 않는다.\n\n이 경우 아래 명령어로 갱신가능\n\n$ certbot --server https://acme-v02.api.letsencrypt.org/directory -d &quot;*.ccpinfra.xyz&quot; --manual --preferred-challenges dns-01 certonly\n\n\n명령어 입력 후 나오는 DNS TXT record 값을 도메인 구입처(예를 들어 가비아 등)에서 DNS 설정에 들어가 업데이트 후 진행한다.\n\n\n  인증서를 갱신하여 새로운 인증서를 받아도 기존 인증서는 만료되지 않는다.\n\n\n\n인증서 갱신 후 서비스별 적용 가이드\n\n기본적으로 NAS에 인증서 정보를 저장하고 각 서비스에 가져다 사용한다.\n\n본 예제에서는 인증서를 /nas/volumes/ssl에서 관리\n\n\n  k8s의 경우 kubernetes secret 형태로 떠져 있음\n  그 외 docker로 직접 올라가있는 경우 각 서비스별 인증서 정보 적용\n\n\n위에서 발급받은 인증서파일을 관리하는 인증서 폴더로 옮긴다.\n\n# 인증서 복사\n$ cp /etc/letsencrypt/live/yourdomain.com-0001/fullchain.pem /nas/volumes/ssl\n$ cp /etc/letsencrypt/live/yourdomain.com-0001/privkey.pem\n\n# 인증서 포맷 변환\n$ openssl x509 -inform PEM -in fullchain.pem -out yourdomain.com.crt\n$ openssl rsa -in privkey.pem -text -text &amp;gt; yourdomain.com.key\n\n\n\n  기존의 인증서는 백업할 것\n\n\n\nTroubleshooting\n\n1. NET::ERR_CERT_COMMON_NAME_INVALID\n\n인증서 갱신 및 발급 후에 인증서 적용이 성공적으로 되었으나 NET::ERR_CERT_COMMON_NAME_INVALID로 인증서가 유효하지 않음\n\n인증서의 주소와 인증서가 적용된 도메인의 주소가 일치하지 않을 때 발생하는 오류\n\n\n  인증서의 도메인 주소가 제대로 적용되었는지 확인한다.\n  * wildcard가 포함되어 있는지 확인한다.\n  도메인 주소가 정상인 경우 전세계의 DNS 서버에 적용이 바로 되지 않고 시간이 어느정도 소요되어서 발생하는 문제로 기다리면 해결된다.\n\n\n도메인 서버 레코드 확인\n\n$ nslookup -type=txt _acme-challenge.subdomain.yourdomain.com\n# 또는\n$ dig +noall +answer _acme-challenge.sbubdomain.yourdomain.com txt\n\n\nAuthoritative answers can be found from:에 리스트로 도메인 서버 정보가 나오면 등록이 완료된 것\n\n\n\nReference\n\n\n  Let’s Encrypt Docs\n  Certbot\n  Secret\n  Certified Kubernetes Administrator(CKA) with Practice Tests\n\n"
} ,
  
  {
    "title"    : "CentOS 파티션 생성 및 마운트",
    "category" : "",
    "tags"     : " Centos, RHEL, Linux, Partition, Mount",
    "url"      : "/2021/02/25/centos-partition-mount.html",
    "date"     : "February 25, 2021",
    "excerpt"  : "CentOS 파티션 생성 및 마운트\n\n1. 디스크 정보 확인\n\nfdisk -l\n\n\n\n\n2. 파티션 구성하기\n\nfdisk /dev/sda\n\n\n\n\n별도의 파티션을 구성하는 것이 아니라면 기본값 사용\n\nPartition type: p\n\nPartition number: 1 (시스템에 따라 다를 수 있음)\n\nFirst sector: 2048 (시스템에 따라 다를 수 있음)\n\nLast sector: 4294967294 (시스템에 따라 다를 수 있음)...",
  "content"  : "CentOS 파티션 생성 및 마운트\n\n1. 디스크 정보 확인\n\nfdisk -l\n\n\n\n\n2. 파티션 구성하기\n\nfdisk /dev/sda\n\n\n\n\n별도의 파티션을 구성하는 것이 아니라면 기본값 사용\n\nPartition type: p\n\nPartition number: 1 (시스템에 따라 다를 수 있음)\n\nFirst sector: 2048 (시스템에 따라 다를 수 있음)\n\nLast sector: 4294967294 (시스템에 따라 다를 수 있음)\n\np 명령어로 파티션 설정 확인\n\nw 명령어로 저장하고 fdisk 나오기\n\n3. 파티션 포맷하기\n\nmkfs.ext4 /dev/sda\n\n\n\n\n위에서 구성한 파티션을 포맷한다\n\next4 포맷 사용\n\n4. 파티션 마운트하기\n\nmount /dev/sda /data\n\n\n/dev/sda 파티션을 /data에 마운트\n\n5. 마운트 확인\n\ndf -h\nmount -a\n\n\n\n\n\n\n마운트 디렉토리에 lost+found 디렉토리 확인\n\n6. 재부팅시 자동 마운트를 위한 설정\n\nblkid\n\n\n\n\n하드디스크 UUID 확인\n\nvim /etc/fstab\n\n\n\n\nUUID={UUID} {마운트 디렉토리} {파티션 포맷} defaults 0 0 추가\n\n\n"
} 
  
  ,
  
  {
  
  "title"    : "Ninja",
  "category" : "",
  "tags"     : " Lorem",
  "url"      : "/portfolio/ninja",
  "date"     : "April 8, 2014",
  "excerpt"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem ...",
  "content"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, \nadipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\n"
  
} ,
  
  {
  
  "title"    : "Creative",
  "category" : "",
  "tags"     : " Ipsum",
  "url"      : "/portfolio/safe",
  "date"     : "August 16, 2014",
  "excerpt"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem qu...",
  "content"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, \nsed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\nUse this area of the page to describe your project. \nThe icon above is part of a free icon set by Flat Icons.\n"
  
} ,
  
  {
  
  "title"    : "Circus",
  "category" : "",
  "tags"     : " Ipsum",
  "url"      : "/portfolio/circus",
  "date"     : "September 1, 2014",
  "excerpt"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem ...",
  "content"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, \nadipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\n"
  
} ,
  
  {
  
  "title"    : "Tower of Hanoi",
  "category" : "",
  "tags"     : " ",
  "url"      : "/portfolio/hanoi",
  "date"     : "September 1, 2014",
  "excerpt"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem ...",
  "content"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, \nadipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\n"
  
} ,
  
  {
  
  "title"    : "Tic tac toe",
  "category" : "",
  "tags"     : " ",
  "url"      : "/portfolio/tictactoe",
  "date"     : "September 1, 2014",
  "excerpt"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem qu...",
  "content"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, \nsed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\nUse this area of the page to describe your project. \nThe icon above is part of a free icon set by Flat Icons.\n\n"
  
} ,
  
  {
  
  "title"    : "Cake",
  "category" : "",
  "tags"     : " Lorem, Ipsum",
  "url"      : "/portfolio/cake",
  "date"     : "September 27, 2015",
  "excerpt"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem qu...",
  "content"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, \nsed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\nUse this area of the page to describe your project. \nThe icon above is part of a free icon set by Flat Icons.\n"
  
} ,
  
  {
  
  "title"    : "Jekyll",
  "category" : "",
  "tags"     : " ",
  "url"      : "/portfolio/jekyllblog",
  "date"     : "May 26, 2017",
  "excerpt"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem qu...",
  "content"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, \nadipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\n"
  
} ,
  
  {
  
  "title"    : "Lorem Ipsum",
  "category" : "",
  "tags"     : " ",
  "url"      : "/portfolio/submarine",
  "date"     : "September 3, 2017",
  "excerpt"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem qu...",
  "content"  : "Sed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, \ntotam rem aperiam, eaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit, \nsed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\nUse this area of the page to describe your project. \nThe icon above is part of a free icon set by Flat Icons.\n"
  
} ,
  
  {
  
  "title"    : "Github",
  "category" : "",
  "tags"     : " Lorem",
  "url"      : "/portfolio/gitlecture",
  "date"     : "October 20, 2017",
  "excerpt"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem ...",
  "content"  : "\n\nSed ut perspiciatis unde omnis iste natus error sit voluptatem accusantium doloremque laudantium, totam rem aperiam, \neaque ipsa quae ab illo inventore veritatis et quasi architecto beatae vitae dicta sunt explicabo. \nNemo enim ipsam voluptatem quia voluptas sit aspernatur aut odit aut fugit, \nsed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt. \nNeque porro quisquam est, qui dolorem ipsum quia dolor sit amet, consectetur, \nadipisci velit, sed quia non numquam eius modi tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. \nUt enim ad minima veniam, quis nostrum exercitationem ullam corporis suscipit laboriosam, nisi ut aliquid ex ea commodi consequatur? \nQuis autem vel eum iure reprehenderit qui in ea voluptate velit esse quam nihil molestiae consequatur, \nvel illum qui dolorem eum fugiat quo voluptas nulla pariatur?\n\n"
  
} 
  
]
